title|description|participants|submissions|year|phases|reward|reward_USD|duration_day|challenge|website|papers|winner's code|dataset|field of application|ML domain|real task|ML task|dataset size|dataset details|score|metrics|multi-score|multi-task|results submission|code submission|remark|utility
The Third Evaluation Workshop on Chinese Machine Reading Comprehension|It is the sentence cloze-style machine reading comprehension (SC-MRC). Given a narrative passage and some sentences extract from the passage, the participants should build models to complete the passage by putting the sentences back into the passages on the right places. This task requires the machine to infer the missing segment by reasoning from the context, increasing the difficulty of machine reading comprehension. |46|248.0|2019|3.0|6500 USD|6500|87.0|https://competitions.codalab.org/competitions/20131|https://hfl-rc.com/cmrc2019/; https://ymcui.com/cmrc2019/|https://arxiv.org/pdf/2004.03116.pdf|None|https://github.com/ymcui/cmrc2019|linguistic, philology, litterature|NLP|sentence cloze-style machine reading comprehension (SC-MRC)|machine reading comprehension|10000||QAC|Question Accuracy (QAC),Passage Accuracy (PAC), Final ranking will be judged on the test set, according to QAC (PAC as secondary metric when there is a tie).|True|False|True|False|https://github.com/ymcui/cmrc2019/tree/master/baseline|[1.1760304486375888]
Pose-guided Human Rendering Benchmark Challenge|HUMBI is an ideal dataset to evaluate the ability of modeling human appearance. To measure such ability, we formulate a novel benchmark challenge on a pose-guided appearance rendering task: given a single view image of a person, render the person appearance from other viewsand poses. HUMBI offers the ground truth of this challenging task where the performance of the approaches can be precisely characterized. We validate the feasibility of thebenchmark challenge using the state-of-the-art rendering methods.|12|65.0|2013|1.0|0|0||https://competitions.codalab.org/competitions/35349|https://www.humbi-data.net/|https://arxiv.org/pdf/2110.00119.pdf; https://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_HUMBI_A_Large_Multiview_Dataset_of_Human_Body_Expressions_CVPR_2020_paper.pdf|None|https://competitions.codalab.org/competitions/35349#learn_the_details||computer vision|posture generation|image generation|374352|366300 train images, 7674 validation images, 378 test images|Mrmse|RMSE, MRMSE, SSIM, MSSIM, ranking is based on the MSSIM(rmse according to leaderboard???)|False|False|True|False|https://github.com/Gorokke/humanrender_class|[1.192140697304481]
MICCAI Multimodal Brain Tumor Segmentation (BRaTS) Challenge|The BRaTS challenge is designed to gauge the current state-of-the-art in automated brain tumor segmentation and to compare between different methods. It is organized in conjuction with the MICCAI conference.|141|50.0|2013|2.0|0|0|22.0|https://competitions.codalab.org/competitions/1|http://braintumorsegmentation.org/|https://hal.inria.fr/hal-00912934/document; https://www.cbica.upenn.edu/sbia/Spyridon.Bakas/MICCAI_BraTS/MICCAI_BraTS_2013_proceedings.pdf|None||medicine|computer vision|image recognition|image recognition||772 distinctive subjects|Overall Rank|Dice score, Hausdorff distance, Sensitivity, Specificity, Kappa|True|False|True|False||[0.0]
ChaLearn LAP 2014 - Track 1: Human Pose Recovery|More than 8,000 frames of continuous RGB sequences are recorded and labeled with the objective of performing human pose recovery by means of recognizing more than 120,000 human limbs of different people.|100|25.0|2014|2.0|1000 USD|1000|108.0|https://competitions.codalab.org/competitions/971|https://gesture.chalearn.org/2014-looking-at-people-challenge|https://www.researchgate.net/publication/289842286_ChaLearn_Looking_at_People_Challenge_2014_Dataset_and_Results|None|http://sunai.uoc.edu/chalearnLAP/||computer vision|human pose recovery from RGB data|image recognition|8234|4,000 dev labelled frames, 2,000 validation labelled frames, 2,234 test frames; 120K+ manually annotated limbs for 8K+ frames showing actors performing natural motion|Overlap|Jaccard Index (overlapping)|False|False|True|True||[1.8293515358361774]
ChaLearn LAP 2014 - Track 2: Action Recognition|Recognizing actions/interactions using 235 performances of 11 action/interaction categories recorded and manually labeled in continuous RGB sequences of different people performing natural isolated and collaborative behaviors.|108|36.0|2014|2.0|1000 USD|1000|108.0|https://competitions.codalab.org/competitions/981|||None|http://sunai.uoc.edu/chalearnLAP/||computer vision|action and interaction recognition from RGB data sequences|image recognition||235 action samples performed by 17 actors|Overlap|Jaccard Index (overlapping)|False|False|True|True||[2.079578859873935]
ChaLearn LAP 2014 - Track 3: Gesture Recognition|Recognizing gestures drawn from a vocabulary of Italian sign gesture categories. The emphasis of this track is on multi-modal automatic learning of a set of 20 gestures performed by several different users, with the aim of performing user independent continuous gesture spotting.|299|373.0|2014|2.0|1000 USD|1000|108.0|https://competitions.codalab.org/competitions/991|||None|http://sunai.uoc.edu/chalearnLAP/||computer vision|multi-modal gesture recognition from RGB-Depth sequences|image recognition||14K manually labeled (beginning and ending frame) gesture performances in continuous video sequences, with a vocabulary of 20 Italian gesture categories; 393 train (7,754 gestures), 287 validation (3,362 gestures), 276 test (2,742 gestures) |Overlap|Jaccard Index (overlapping)|False|False|True|True||[1.360556174784397]
ChaLearn Fast Causation Coefficient Challenge|"Score pairs of variables {A, B} with a positive coefficient if A causes B, a negative one if B causes A, and zero otherwise. Consider for instance a target variable B, like occurrence of ""lung cancer"" in patients. The goal would be to find whether a factor A, like ""smoking"", might cause B. The objective of the challenge is to rank pairs of variables {A, B} to prioritize experimental verifications of the conjecture that A causes B. As is known, ""correlation does not mean causation"". More generally, observing a statistical dependency between A and B does not imply that A causes B or that B causes A; A and B could be consequences of a common cause. But, is it possible to determine from the joint observation of samples of two variables A and B that A should be a cause of B or vice versa?"|98|115.0|2014|2.0|3250 USD|3250|70.0|https://competitions.codalab.org/competitions/1381|http://www.causality.inf.ethz.ch/cause-effect.php?page=help; http://clopinet.com/isabelle/Projects/NIPS2013/|http://www.causality.inf.ethz.ch/cause-effect.php?page=help; https://sites.google.com/a/chalearn.org/causality/experimental-design|http://clopinet.com/isabelle/Projects/NIPS2013/; https://github.com/jarfo/cause-effect; https://github.com/waynezhanghk/FastCausation; https://bitbucket.org/lopezpaz/causality_challenge/src/master/|http://www.causality.inf.ethz.ch/cause-effect.php?page=data; http://www.causality.inf.ethz.ch/CEdata/||NLP|Score pairs of variables {A, B} with a positive coefficient if A causes B, a negative one if B causes A, and zero otherwise.|multi-class classification|12150||Bidirectional AUC|AUC|False|False|True|True|Kaggle?? https://www.kaggle.com/c/cause-effect-pairs/data|[0.0]
The MICCAI 2014 Machine Learning Challenge|Predicting Binary and Continuous Phenotypes from Structural Brain MRI Data. The MICCAI 2014 Machine Learning Challenge (MLC) will take a significant step in this direction, where we will employ four separate, carefully compiled, and curated large-scale (each N > 70) structural brain MRI datasets with accompanying clinically relevant phenotypes. Our goal is to provide a snapshot of the current state of the art in the field of neuroimage-based prediction, and attract machine-learning practitioners to the MICCAI community and the field of medical image computing in general|92|142.0|2014|1.0|0|0|61.0|https://competitions.codalab.org/competitions/1471|||None||medicine|computer vision||binary classification, continuous regression|||Mean Accuracy, Mean RMS|Accuracy Estimate, AUC estimate; RMSE estimate, Pearson Correlation Coefficient Estimate|True|True|True|False||[-0.09887446819968766]
ChaLearn Automatic Machine Learning Challenge (AutoML)|Create a fully Automatic Machine Learning solution, capable of building models without ANY human intervention.|717|5097.0|2014|5.0|30000 USD|30000|565.0|https://competitions.codalab.org/competitions/2321||http://www.causality.inf.ethz.ch/AutoML/automl_ijcnn15.pdf; https://www.microsoft.com/en-us/research/blog/automl-challenge-leap-forward-machine-learning-competitions/|None|5 datasets||AutoML||regression, binary classification, multi-class classification, multi-label classification|||<Rank>|R2, ABS, BAC, AUC, F1, PAC|True|True|True|True||[0.0]
COCO Detection Challenge|The COCO Object Detection Task is designed to push the state of the art in object detection forward. COCO features two object detection tasks: using either bounding box output or object segmentation output (the latter is also known as instance segmentation).|1214|11078.0|2015|3.0|0|0|1325.0|https://competitions.codalab.org/competitions/5181|https://cocodataset.org/#detection-2019; https://cocodataset.org/workshop/coco-lvis-eccv-2020.html|https://arxiv.org/abs/1405.0312; https://cocodataset.org/#detection-leaderboard|None|https://cocodataset.org/#download||computer vision|object recognition|object recognition|200000|200,000 images and 80 object categories|AP|Average Precision (AP),AP Across Scales, Average Recall (AR), AR Across Scales|True|False|True|False|https://github.com/cocodataset/cocoapi|[1.501514131897712]
COCO Detection Challenge (Segmentation Mask)|The COCO Object Detection Task is designed to push the state of the art in object detection forward. COCO features two object detection tasks: using either bounding box output or object segmentation output (the latter is also known as instance segmentation).|996|5274.0|2019|3.0|0|0|363.0|https://competitions.codalab.org/competitions/20796|||None|https://cocodataset.org/#download||computer vision|object recognition|object detection using object segmentation output|||AP|Average Precision (AP),AP Across Scales, Average Recall (AR), AR Across Scales|True|False|True|False||[1.3968992248062018]
COCO Detection Challenge (Bounding Box)|The COCO Object Detection Task is designed to push the state of the art in object detection forward. COCO features two object detection tasks: using either bounding box output or object segmentation output (the latter is also known as instance segmentation).|2017|21677.0|2019|3.0|0|0|363.0|https://competitions.codalab.org/competitions/20794|||None|https://cocodataset.org/#download||computer vision|object recognition|object detection using bounding box output|||AP|Average Precision (AP),AP Across Scales, Average Recall (AR), AR Across Scales|True|False|True|False||[1.5308679474523499]
Feature Selection Challenge|post-challenge submissions on test data to benchmark new methods. The aim of the challenge in feature selection is to find feature selection algorithms that significantly outperform methods using all features, on ALL five benchmark datasets.|70|1972.0|2015|1.0|0|0||https://competitions.codalab.org/competitions/3931|http://web.archive.org/web/20130512034606/http://www.nipsfsc.ecs.soton.ac.uk/datasets||None|http://web.archive.org/web/20140602202553/http://www.nipsfsc.ecs.soton.ac.uk/papers/NIPS2003-Datasets.pdf|medicine|NLP|find feature selection algorithms that significantly outperform methods using all features, on ALL five benchmark datasets|binary classification, feature selecion||5 datasets: 9200 train examples, 2350 validation, 11800 test|< Rank >|BER, AUC, FP, FF|True|False|True|False||[0.0]
Microsoft COCO Image Captioning Challenge|The automatic generation of captions for images is a long-standing and challenging problem in artificial intelligence. To promote and measure the progress in this area, we carefully created the Microsoft Common objects in COntext (MS COCO) dataset to provide resources for training, validation, and testing of automatic image caption generation. Currently, the MS COCO 2014 dataset contains one million captions and over 160,000 images.|2081|4700.0|2015|1.0|0|0||https://competitions.codalab.org/competitions/3221|unaccessible website http://mscoco.org/|https://arxiv.org/abs/1504.00325|None|||computer vision|object recognition, annotation generation|automatic generation of captions for images|<1000000|one million captions and over 160,000 images (164K images split into training (83K), validation (41K) and test (41K) sets)|CIDEr-D c40|BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L, METEOR and CIDEr-D|True|False|True|False|https://github.com/tylin/coco-caption|[0.0]
Dialog State Tracking Challenge 4 (DSTC4)|Dialog state tracking is one of the key sub-tasks of dialog management, which defines the representation of dialog states and updates them at each moment on a given on-going conversation. In this fourth edition of the Dialog State Tracking Challenge, we will focus on a dialog state tracking task on human-human dialogs. In addition to this main task, we also propose a series of pilot tracks for the core components in developing end-to-end dialog systems based on the same dataset. We expect these shared efforts on human dialogs will contribute to progress in developing much more human-like systems.|25|51.0|2015|2.0|0|0|17.0|https://competitions.codalab.org/competitions/4971|https://colips.org/workshop/dstc4/index.html; https://www.microsoft.com/en-us/research/event/dialog-state-tracking-challenge/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fevents%2Fdstc%2F|https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/dstc4_final-1.pdf|None||linguistics|NLP|dialog state tracking|dialogue generation||35 dialogs, 31034 utterances, 273580 words|Schedule 2 F-measure|Accuracy, Precision, Recall, F-measure|True|False|True|False||[0.0]
VQA Image Challenge|Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. To promote and measure progress in this area, we have carefully created the VQA dataset of questions and answers about real images and abstract scenes. We also offer two versions of the task: open-ended and multiple-choice. Open-ended requires a system to produce a natural language answer, while multiple-choice only requires a system to pick an option out of the provided answers. This dataset partially builds on top of the recent Microsoft Common Objects in COntext (MSCOCO) dataset by using its images as part of the VQA dataset. In addition, it also builds a VQA dataset on top of a new collection of 50,000 abstract scenes (see the paper for more details).|530||2015|3.0|0|0|229.0|https://competitions.codalab.org/competitions/6961; https://competitions.codalab.org/competitions/6971; https://competitions.codalab.org/competitions/6991; https://competitions.codalab.org/competitions/6981|https://visualqa.org/|https://arxiv.org/pdf/1505.00468; https://visualqa.org/|None|||computer vision, NLP|open-ended questions about images (VQA: Visual Question Answering)|QA|204721 images, 50000 scenes|50,000 abstract scenes (20K trtain, 10K validation, 20K test set; ∼0.25M images, ∼0.76M questions, and ∼10M answers)|Overall|new evaluation metric which is robust to inter-human variability in phrasing the answers|False|True|True|False||[0.0, 0.0, 0.0, 0.0]
SemEval-2017 Task 2 Subtask 1, Multilingual Word Similarity|This is a competition on monolingual word similarity. Data available for English, Farsi, German, Italian and Spanish. The subtask is intended to test not only monolingual approaches but also multilingual and language-independent techniques. While monolingual approaches will be evaluated in their corresponding language datasets, multilingual and language-independent techniques will also be given a global score (see Evaluation for more details). We included Farsi as an under-resourced language from a different family in order to provide a framework for models that do not rely on many external tools and can be effectively applied to less-resourced languages.|35|44.0|2016|1.0|0|0|388.0|https://competitions.codalab.org/competitions/15961|https://alt.qcri.org/semeval2017/task2/; https://semeval.github.io/|https://aclanthology.org/S17-2002.pdf|None|https://alt.qcri.org/semeval2017/task2/index.php?id=data-and-tools|linguistics|NLP|word similarities|word embeddings||Five monolingual word similarity datasets of 500 word pairs each||Harmonic average of Pearson and Spearman correlation.|False|True|True|False||[0.0]
SemEval-2017 Task 2 Subtask 2, Cross-lingual Word Similarity|This is a competition on cross-lingual word similarity. Data available for ten language pairs, including all pairwise combinations of English, Farsi, German, Italian and Spanish. In the cross-lingual word similarity subtask each word pair is composed by words in different languages (e.g. building-habitación). This subtask is composed of ten cross-lingual word similarity datasets: EN-DE, EN-ES, EN-FA, EN-IT, DE-ES, DE-FA, DE-IT, ES-FA, ES-IT, and FA-IT. The subtask is intended to test bilingual and multilingual semantic representation techniques.|18|19.0|2016|1.0|0|0|388.0|https://competitions.codalab.org/competitions/15962|||None|https://alt.qcri.org/semeval2017/task2/index.php?id=data-and-tools|linguistics|NLP|word similarities|word embeddings||Ten cross-lingual word similarity datasets in the range of 750-1000 word pairs each.||Harmonic average of Pearson and Spearman correlation.|False|True|True|False||[0.0]
Word Sense Disambiguation, Unified Evaluation Framework|Word Sense Disambiguation is a longstanding task in Natural Language Processing, lying at the core of human language understanding. The task of Word Sense Disambiguation consists of associating words in context with the most suitable entry in a pre-defined sense inventory. Depending on their nature, WSD systems are divided into two main groups: supervised and knowledgebased. However, the evaluation of automatic systems has been problematic, mainly due to the lack of a reliable evaluation framework. In this paper we develop a unified evaluation framework and analyze the performance of various Word Sense Disambiguation systems in a fair setup. The results show that supervised systems clearly outperform knowledge-based models. Among the supervised systems, a linear classifier trained on conventional local features still proves to be a hard baseline to beat. Nonetheless, recent approaches exploiting neural networks on unlabeled corpora achieve promising results, surpassing this hard baseline in most test sets.|32|13.0|2016|1.0|0|0||https://competitions.codalab.org/competitions/15984|http://lcl.uniroma1.it/wsdeval/|http://lcl.uniroma1.it/wsdeval/data/EACL17_WSD_EvaluationFramework.pdf|None|http://lcl.uniroma1.it/wsdeval/|linguistics|NLP|word sense disambiguation|word sense disambiguation||five standard all-words Word Sense Disambiguation datasets|ALL|F-Measure (harmonic mean between precision and recall)|False|False|True|False||[0.0]
ChaLearn LAP 2016 - Track 1: Age Estimation|An extended version of the previous ICCV2015 challenge dataset. It contains 8,000 images each displaying a single individual, labeled with the apparent age. Each image has been labeled by multiple individuals, using a collaborative Facebook implementation and Amazon Mechanical Turk. The votes variance is used as a measure of the error for the predictions. This is the first state of the art database for Apparent Age Recognition rather than Real Age recognition.|231|308.0|2016|2.0|3000 USD|3000|50.0|https://competitions.codalab.org/competitions/7511|https://chalearnlap.cvc.uab.cat/challenge/13/track/13/description/||None|https://chalearnlap.cvc.uab.cat/login/?next=/dataset/19/data/29/files/||computer vision|age estimation/recognition|classification or regression problem|8000 images||Error|ERROR: http://sunai.uoc.edu/chalearnLAP/2015/EvalFormula.png|False|False|True|False||[0.0]
Lives : Learning with Multiple Views|This is a hackathon on multiview learning - part of the DAMVL workshop @ ECML 2019 - with a dataset from the field of developmental biology. Developmental biology is concerned with the study of how an embryo develops from a single fertilized cell into a complex and organized multicellular system. This process involves dynamics at multiple scales which are recorded using numerous acquisition techniques, from live movies using fluorescent reporter proteins to fixed samples in in situ hybridization and immunocytochemistry techniques. To study how cell fates are established by gene regulatory networks in Drosophila melanogaster embryogenesis, it has recently been proposed that a first necessary step is to integrate multiple views from eterogeneous image datasets. We focus on the dorso-ventral patterning in Drosophila melanogaster early development. We provide a dataset that gather together multiple snapshots (images of 128x128 pixels in single channel) acquired during the development of few subjects. Nuclei is referred to as view 0, it corresponds to the morphology ; protein expression of doubly phosophorylated ERK (dpERK is view 1) ; Twist (view 2) and Dorsal (view 4) ; and mRNA expression of ind (view 3) and rho (view 5). In total, the dataset consists in 255 instances of drosophila embryo during its development, featured into 6 views. There are missing data such that none of the instances has the entire 6 views. Examples of data are shown below. Once completing missing views, combining all together results in a colored sequence showing the five molecular components (views 1 to 5) during embryo development. One goal is to develop methods to predict such missing data, a first step in this direction is to predict one view from the others. We then formalize the given task as predicting view 1 given views 0, 2, 3, 4 and 5.|11|41.0|2016|1.0|0|0||https://competitions.codalab.org/competitions/21342|https://damvl.lis-lab.fr/||None|https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005742|biology|computer vision||multiview regression||255 instances of drosophila embryo during its development, featured into 6 views|MSE|MSE|False|False|True|False||[0.0]
NEWS 2016 (Standard Submissions)|Transliteration is defined as phonetic translation of names across languages. Transliteration of Named Entities (NEs) is necessary in many applications, such as machine translation, corpus alignment, cross-language IR, information extraction and automatic lexicon acquisition. The tasks are to transliterate personal names or place names from a source to a target language. All such systems call for high-performance transliteration, which is the focus of shared task. The objective of the shared task is to promote machine transliteration research by providing a common benchmarking platform for the community to evaluate the state-of-the-art technologies.|25|255.0|2016|1.0|0|0|9.0|https://competitions.codalab.org/competitions/8991|http://workshop.colips.org/news2016/index.html|http://workshop.colips.org/news2016/documents/news2016whitepaper.pdf; https://aclanthology.org/W18-2413.pdf; https://aclanthology.org/C18-1053.pdf|None|http://workshop.colips.org/news2016/dataset.html; http://workshop.colips.org/news2016/documents/keynote_abstract.pdf|linguistics|NLP|neural machine translation, transliteration of named entities|machine translation||Five different datasets comprising a total of 14 different transliteration tasks|Accuracy|Word Accuracy in Top-1 (ACC), Fuzziness in Top-1 (Mean F-score), Mean Reciprocal Rank (MRR), Mean Average Precision (MAP)|True|True|True|False||[1.0]
ChaLearn LAP. Apparent Personality Analysis: First Impressions (first round)|As part of the speeds interviews project, we are organizing a challenge on “first impressions”, in which participants will develop solutions for recognizing personality traits of users in short video sequences. We are making available a large newly collected data set sponsored by Microsoft of 10,000 videos of about 15-seconds each collected from YouTube, annotated with personality traits by AMT workers. For each video sample, RGB and audio information are provided, as well as continuous ground-truth values for each of the 5 Big Five Traits.|294|425.0|2016|2.0|3000 USD|3000|59.0|https://competitions.codalab.org/competitions/9181|https://gesture.chalearn.org/2016-looking-at-people-eccv-workshop-challenge|https://www.researchgate.net/publication/309741759_ChaLearn_LAP_2016_First_Round_Challenge_on_First_Impressions_-Dataset_and_Results |https://tzzcl.github.io/papers/eccvw16_APA.pdf|||computer vision||classification|10000 videos|10,000 videos of about 15-seconds each|Accuracy|mean accuracy|False|False|True|False||[1.0462216133216125]
3D Face Alignment in the Wild Challenge|Face alignment – the problem of automatically locating detailed facial landmarks across different subjects, illuminations, and viewpoints – is critical to all face analysis applications, such as identification, facial expression and action unit analysis, and in many human computer interaction and multimedia applications. The most common approach is 2D alignment, which treats the face as a 2D object. This assumption holds as long as the face is frontal and planar. As face orientation varies from frontal, however, this assumption breaks down: 2D annotated points lose correspondence. Pose variation results in self occlusion that confounds landmark annotation. To enable alignment that is robust to head rotation and depth variation, 3D imaging and alignment has been explored. 3D alignment, however, requires special sensors for imaging or multiple images and controlled illumination. When these assumptions cannot be met, which is common, 3D alignment from 2D video or images has been proposed as a potential solution. This challenge addresses the increasing interest in 3D alignment from 2D images. 3DFAW Challenge evaluates 3D face alignment methods on a large, diverse corpora of multi-view face images annotated with 3D information.|106|200.0|2016|3.0|0|0|142.0|https://competitions.codalab.org/competitions/10261||https://www.laszlojeni.com/pub/articles/Jeni16_ECCV_3DFAW.pdf; participants: de B. Zavan, F.H., Nascimento, A.C.P., e Silva, L.P., Bellon, O.R.P., Silva, L.: 3d face alignment in the wild: A landmark-free, nose-based approach. In: 2016 European Conference on Computer Vision Workshops (ECCVW) (2016); [1] Gross, R., Matthews, I., Cohn, J., Kanade, T., and Baker, S. (2010). Multi-pie. Image and Vision Computing, 28(5), 807-813. [2] Zhang, X., Yin, L., Cohn, J. F., Canavan, S., Reale, M., Horowitz, A., Liu, P., and Girard, J. M. (2014). BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial expression database. Image and Vision Computing, 32(10), 692-706. [3] Jeni, L. A., Cohn, J. F., and Kanade, T. (2015). Dense 3D face alignment from 2D videos in real-time. In Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on (Vol. 1, pp. 1-8). IEEE.|None|||computer vision|3D alignment from 2D images|image tranformation|23000 images|23,000 multi-view images from four sources (db: BU-4DFE [25], BP4D-Spontaneous [26], MultiPIE [11], and time-sliced videos from the internet)  together with 3D annotation|Prediction consistency (CVGTCE), %|Ground Truth Error - GTE, Cross View Ground Truth Consistency Error - CVGTCE|True|False|True|False||[0.0]
CIKM Cup 2016 Track 1: Cross-Device Entity Linking Challenge|building accurate user identity becomes a very difficult and important problem for advertising companies. The crucial task in this process is finding the same user across multiple devices and integrating her/his digital traces together to perform more accurate profiling.|185|1734.0|2016|2.0|5000 USD|5000|1522.0|https://competitions.codalab.org/competitions/11171||https://competitions.codalab.org/competitions/11171#learn_the_details-related_work; https://github.com/aduispace/CIKM-Cup-2016-Track-1-Cross-Device-Entity-Linking-Challenge/blob/master/CS249Report.pdf|https://github.com/aduispace/CIKM-Cup-2016-Track-1-Cross-Device-Entity-Linking-Challenge|https://drive.google.com/drive/folders/0B7XZSACQf0KdNXVIUXEyVGlBZnc?resourcekey=0-7ITozWjtDNvBHfTROIfxLg|online advertising|NLP|The participants have to predict new edges (identify the same user across multiple devices). finding the same user across multiple devices and integrating her/his digital traces together to perform more accurate profiling||721443 ground-truth pairs||Precision|F1 score|False|False|True|False||[1.9930425199210162]
CIKM Cup 2016 Track 2: Personalized E-Commerce Search Challenge|"The Personalized E-commerce Search Challenge provides a unique opportunity for academia and industry researchers to test new ideas for personalized e-commerce search and consolidate the approaches already published and described in existing work. The successful participation in the challenge implies solid knowledge of learning to rank, log mining, and search personalization algorithms, to name just a few. For the model development, we release a new dataset provided by DIGINETICA and its partners containing anonymized search and browsing logs, product data, anonymized transactions, and a large data set of product images. The participants have to predict search relevance of products according to the personal shopping, search, and browsing preferences of the users. Both ""query-less"" and ""query-full"" sessions are possible. The evaluation is based on click and transaction data."|205|800.0|2016|2.0|5000 USD|5000|1522.0|https://competitions.codalab.org/competitions/11161||||https://drive.google.com/drive/folders/0B7XZSACQf0KdXzZFS21DblRxQ3c?resourcekey=0-3k4O5YlwnZf0cNeTZ5Y_Uw|ecommerce|computer vision, NLP|predict relevance labels and re-rank products returned by an e-commerce search engine on the search engine result page (SERP) using (1) search, browsing, and transaction histories for all users and specifically the user interacting with the search engine in the current session; (2) product meta-data; (3) product images|ordinal regression|||SearchNDCG (query-full; textual queries) |NDCG (Normalized Discounted Cumulative Gain)|False|False|True|False||[1.5603975178463116]
The Large Scale Movie Description Challenge (LSMDC) 2017 : Movie Multiple-Choice Test|https://sites.google.com/site/describingmovies/previous-years/lsmdc-2016/moviedescription|29|42.0|2017|1.0|0|0||https://competitions.codalab.org/competitions/11491|https://sites.google.com/site/describingmovies/home|https://arxiv.org/abs/1809.07257|https://github.com/OSUPCVLab/VideoToTextDNN|https://sites.google.com/site/describingmovies/download|movies|computer vision|Given a video query and 5 captions, find the correct caption for the video among 5 possible choices|video captioning||1,970 open-domain video clips and over 85k English description sentences +  10k open-domain video clips that are described by 200k crowdsourced sentences + 1,880 videos with 3,760 sentences|ACCURACY |accuracy|False|False|True|False||[1.4017556442559558]
The Large Scale Movie Description Challenge (LSMDC) 2017 : Movie Retrieval|https://sites.google.com/site/describingmovies/previous-years/lsmdc-2016/movieretrieval|27|50.0|2017|1.0|0|0||https://competitions.codalab.org/competitions/11531||https://drive.google.com/file/d/1IrETp3nOzeOeCxXvAq81E7W-Ih1_21yo/view; https://drive.google.com/open?id=0B9nOObAFqKC9WmNDWWQ2NFEyWmM|None||movies|computer vision||movie description|||Recall@10|Recall@1, Recall@5, Recall@10, and Median Rank for video retrieval (given caption rank videos).|True|False|True|False||[1.6721491228070173]
The Large Scale Movie Description Challenge (LSMDC) 2017 : Movie Fill-in-the-Blank|https://sites.google.com/site/describingmovies/previous-years/lsmdc-2016/movie-fill-in-the-blank|16|52.0|2017|1.0|0|0||https://competitions.codalab.org/competitions/11691||https://arxiv.org/abs/1808.02559|None||movies|computer vision, NLP|Movie Fill-in-the-Blank||||Accuracy|accuracy |False|False|True|False||[1.8029993183367419]
SemEval 2017 Task 10 ScienceIE|Extracting Keyphrases and Relations from Scientific Publications|75|228.0|2016|2.0|None|None|152.0|https://competitions.codalab.org/competitions/15898|https://scienceie.github.io/||None|https://scienceie.github.io/resources.html|linguistics|NLP|Identification of keyphrases, Classification of identified keyphrases, Extraction of relationships between two identified keyphrases||500|500 journal articles|F1|precision, recall and F1-score|True|True|True|False||[0.0]
The Story Cloze Test|A challenge for evaluating a system's natural language and story understanding.|137|281.0|2016|3.0|None|None||https://competitions.codalab.org/competitions/15333|https://www.cs.rochester.edu/nlp/rocstories/|https://aclanthology.org/P18-2119/; https://arxiv.org/pdf/1604.01696v1.pdf|None|https://www.cs.rochester.edu/nlp/rocstories/|linguistics|NLP|sentence cloze|binary classification|||PercentageScore|accuracy|False|False|True|False||[1.2346496869989714]
Head pose challenge|The challenge is to build a system that would be able to estimate the head-pose using only depth information from a single Kinect 2 frame of a person sitting in front of a camera. Figure 3 shows an example of the database, where we show high variability in pitch, yaw, and roll values.|46|23.0|2017|2.0|None|None|58.0|https://competitions.codalab.org/competitions/15972|http://icv.tuit.ut.ee/fc2017||None|||computer vision|estimate the head-pose||||Difference|sum of average estimation errors for each angle picth, yaw and roll|False|False|True|False||[0.0]
ChaLearn LAP - Job Candidate Screening Coopetition|We propose a challenge on explainable computer vision and pattern recognition in first impressions, were the goal is to devise automated methods for deciding whether a job candidate has to be interviewed or not, starting from short video clips (see data description). In addition, solutions will have to “explain” why a given decision (either inviting or not) was taken.|88|231.0|2017|4.0|None|None|90.0|https://competitions.codalab.org/competitions/15975|https://gesture.chalearn.org/speed-interviews; https://chalearnlap.cvc.uab.cat/challenge/23/description/; https://gesture.chalearn.org/speed-interviews||None||HR|computer vision|automatic recommendations based on multi-media CVs (videos): predicting two types of variables (1) personality traits (context variables) and (2) whether a person should be invited to a job interview (decision variable)|regression|||interview|accuracy: mean accuracy along each dimension|True|True|True|False||[1.0171704310191718]
Syngenta AI Challenge - Harness data to help feed our rising population|The AI for Good Foundation, with support from Syngenta, is proud to present the Syngenta AI Challenge, a unique competition focused on bringing Artificial Intelligence (AI) tools to agriculture.|103|153.0|2017|4.0|13500 USD|13500|125.0|https://competitions.codalab.org/competitions/16194|https://www.ideaconnection.com/Syngenta-AI-Challenge/challenge.php||None|https://www.ideaconnection.com/Syngenta-AI-Challenge/datasets.php|agriculture|NLP|predict seed variety performance for the class of 2014|regression|||FMEASURE|F1-score, ACCURACY, MATHEWSCC, R2, VARSCORE|False|False|True|True||[2.164556962025316]
WASSA-2017 Shared Task on Emotion Intensity (EmoInt)|Given a tweet and an emotion X, determine the intensity or degree of emotion X felt by the speaker.|141|524.0|2017|2.0|None|None|71.0|https://competitions.codalab.org/competitions/16380|http://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.html|https://arxiv.org/abs/1708.03696|None|https://competitions.codalab.org/competitions/16380#learn_the_details-datasets|linguistics|NLP|prediction of intensity of emotions in tweets|sentimel analysis|||avg_pearson|average of Pearson Correlation Coefficients|False|False|True|False||[0.0]
NTIRE 2017 Super-Resolution Challenge|To gauge the state-of-the-art, compare and promote different solutions in example-based single-image super-resolution we are organizing this challenge and propose a large DIVerse 2K resolution images dataset.|1301|723.0|2017|2.0|10000 USD|10000|47.0|https://competitions.codalab.org/competitions/16305; https://competitions.codalab.org/competitions/16304; https://competitions.codalab.org/competitions/16303; https://competitions.codalab.org/competitions/16307; https://competitions.codalab.org/competitions/16308; https://competitions.codalab.org/competitions/16306|||None|||computer vision|Image restoration and image enhancement||||PSNR|Peak Signal To Noise Ratio (PSNR) and, complementary, the Structural Similarity (SSIM) index|True|True|True|False||[1.0460091456233402, 1.0550411233249701, 1.0700547855551918, 1.0779348126053234, 1.1261152711041733, 1.1626692425107994]
CIKM AnalytiCup 2017: Lazada Product Title Quality Challenge|Free Lazada from poor quality titles and return us to a state of clean and relevant product titles.|519|6282.0|2017|2.0|9000 SGD|6300|98.0|https://competitions.codalab.org/competitions/16652|||None||ecommerce|NLP|build product title quality models that can automatically grade product titles based on the provided features|regression|||Overall|RMSE|False|False|True|False||[0.0]
LiTS - Liver Tumor Segmentation Challenge|The liver is a common site of primary (i.e. originating in the liver like hepatocellular carcinoma, HCC) or secondary (i.e. spreading to the liver like colorectal cancer) tumor development. Due to their heterogeneous and diffusive shape, automatic segmentation of tumor lesions is very challenging. Until now, only interactive methods achieve acceptable results on segmenting liver lesions.|4894|6224.0|2017|2.0|None|None|36.0|https://competitions.codalab.org/competitions/17094|||None||medicine|computer vision|segmentation of tumor lesions|image segmentation|200|training data set contains 130 CT scans and the test data set 70 CT scans|Dice per case|Dice per case, Dice global, VOE, RVD, ASSD, MSD, RMSD, Precision at 50% overlap, Recall at 50% overlap, Precision at >0% overlap;Recall at >0% overlap|True|False|True|False|https://github.com/PatrickChrist/LITS-CHALLENGE/blob/master/submission-guide.md|[0.0]
AI4Industry Dassault-Aviation Challenge||10|1050.0|2021||500000 EUR|540000||None|None|https://hal.inria.fr/hal-03463307v3/document|None||aeronautics, physics|predictive maintenance||multivariate regression|||MAE|MAE|False|False|False|True||[]
Word Sense Induction and Disambiguation for the Russian Language: wiki-wiki dataset|A Shared Task on Word Sense Induction and Disambiguation for the Russian Language. Track for the wiki-wiki dataset.|137|1342.0|2017|2.0|None|None|65.0|https://competitions.codalab.org/competitions/17810; https://competitions.codalab.org/competitions/17809; https://competitions.codalab.org/competitions/27331; https://competitions.codalab.org/competitions/17806|https://russe.nlpub.org/2018/wsi/|https://www.dialog-21.ru/media/4539/panchenkoaplusetal.pdf|None|||NLP|cluster these contexts in the (unknown in advance) number of clusters which correspond to various senses of the word|clusterisation|||ARI (private)|Adjusted Rand Index (ARI)|False|True|True|False||[2.563624498927939, 1.9900337367297036, 2.6525336091003107, 1.9287324289988115]
NTIRE 2018 Super-Resolution Challenge|To gauge the state-of-the-art, compare and promote different solutions in example-based single-image super-resolution we are organizing this challenge and propose a large DIVerse 2K resolution images dataset.|732|729.0|2018|2.0|16000 USD|16000|71.0|https://competitions.codalab.org/competitions/18025; https://competitions.codalab.org/competitions/18015; https://competitions.codalab.org/competitions/18033; https://competitions.codalab.org/competitions/18034; https://competitions.codalab.org/competitions/18026; https://competitions.codalab.org/competitions/18024; https://competitions.codalab.org/competitions/18047; https://competitions.codalab.org/competitions/18046|https://data.vision.ee.ethz.ch/cvl/ntire18//|||||computer vision|Image restoration and image enhancement||||PSNR|Peak Signal To Noise Ratio (PSNR) and, complementary, the Structural Similarity (SSIM) index|True|True|True|False||[nan, 1.034430862360161, 0.0, 0.0, nan, nan, nan, nan]
Metaphor Shared Task|Shared Task in Workshop on Figurative Language Processing|70|137.0|2018|2.0|None|None|24.0|https://competitions.codalab.org/competitions/17805|https://sites.google.com/site/figlangworkshop/|||http://ota.ahds.ac.uk/headers/2541.xml||NLP|detect metaphor at word level|classification||||precision, recall, and F|True|False|True|False||[0.0]
HWxPI Handwritten texts for Personality Identification|Predict a discretized variable associated to personality traits from handwritten text, including transcripts and raw images|66|79.0|2018|2.0|None|None|141.0|https://competitions.codalab.org/competitions/18362|https://chalearnlap.cvc.uab.cat/challenge/27/track/29/description/||||psychology|computer vision, NLP|Each subject has an associated class 1 and 0, corresponding to the presence of a high pole or a low pole of a specific personality trait. The traits correspond to the Big Five personality model used in psychology: Extraversion, Agreeableness, Conscientiousness, Emotional stability, and Openness to experience. Thus, participants will have to develop a classifier to predict the pole of each trait, this classifier should be able to use the information from both modalities (i.e. textual and visual). |multi-label classification|||AUC performance|AUC|False|True|True|False||[nan]
CTW dataset classification|CTW dataset classification|95|625.0|2018|1.0|None|None||https://competitions.codalab.org/competitions/18634|https://ctwdataset.github.io/|https://jcst.ict.ac.cn/EN/10.1007/s11390-019-1923-y||||computer vision, NLP|detection and recognition of text in natural images: character recognition (top-1 accuracy of 80.5%), character detection (AP of 70.9%), and text line detection (AED of 22.1)|OCR|||all|top-1 accuracy of 80.5%, AP of 70.9%, AED of 22.1|True|True|True|False||[0.0]
Animal Behavior Challenge (ABC2018) for understanding animal behavior|Predicting gender of bird from its GPS trajectory|85|919.0|2018|2.0|0|0|153.0|https://competitions.codalab.org/competitions/16283|||||biology, ethnology|signal processing|birds gender identification from GPS trajectories|binary classification|||Prediction score|accuracy|False|False|True|False||[1.164714170167245]
DeepGlobe Road Extraction Challenge|DeepGlobe - Road Challenge|1792|5853.0|2018|4.0|None|None|75.0|https://competitions.codalab.org/competitions/18467|http://deepglobe.org/|https://arxiv.org/pdf/1805.06561.pdf|||urbanisme|computer vision|road extraction from satellite images|multi-label binary classification|||Prediction score|IoU (Intersection over Union)|False|False|True|False||[1.2553777279063445]
DeepGlobe Building Extraction Challenge|DeepGlobe - Building Extraction Challenge|1043|817.0|2018|2.0|None|None|75.0|https://competitions.codalab.org/competitions/18544|http://deepglobe.org/|https://arxiv.org/pdf/1805.06561.pdf|||urbanisme|computer vision|building extraction from satellite images|object segmentation|||Prediction score|F1-score|False|False|True|False||[0.0]
DeepGlobe Land Cover Classification Challenge|DeepGlobe - Land Cover Classification|1358|2267.0|2018|2.0|None|None|75.0|https://competitions.codalab.org/competitions/18468|http://deepglobe.org/|https://arxiv.org/pdf/1805.06561.pdf|||urbanisme|computer vision|land cover classification from satellite images|multi-class classification|||Prediction score|mean Intersection over Union (mIoU)|False|False|True|False||[nan]
2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM)|A task to model the acquisition of a second language using millions of exercises completed by thousands of students over their first 30 days of learning on Duolingo|41|96.0|2018|2.0|None|None|14.0|https://competitions.codalab.org/competitions/18491|http://sharedtask.duolingo.com/|||https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/8SWHNO|linguistics|NLP|English prompts and generate high-coverage sets of plausible translations in five other languages|machine translation|||AUROC|ROC AUC,  F1 |True|True|True|False||[0.0]
WASSA 2018 Implicit Emotion Shared Task|Shared Task on Implicit Emotion Recognition, organized as part of WASSA 2018|75|256.0|2018|3.0|None|None|232.0|https://competitions.codalab.org/competitions/19214|https://wt-public.emm4u.eu/wassa2018/||||linguistics|NLP|emotion recognition|multi-class classification|||Macro F1-Score |macro-averaged F1-score|True|False|True|False||[1.1281132996907048]
WebVision Challenge 2018|The recent success of deep learning has shown that a deep architecture in conjunction with abundant quantities of labeled training data is the most promising approach for most vision tasks.  However, annotating a large-scale dataset for training such deep neural networks is costly and time-consuming, even with the availability of scalable crowdsourcing platforms like Amazon’s Mechanical Turk.  As a result, there are relatively few public large-scale datasets (e.g., ImageNet) from which it is possible to learn generic visual representations from scratch. So, we organize the WebVision challenge to advance the area of learning knowledge and representation from web data.|141|70.0|2018|2.0|None|None|74.0|https://competitions.codalab.org/competitions/18622||||||computer vision||classification|||top-1 accuracy|accuracy |False|False|True|False||[1.3727986617722223]
Human Behavior Challenge (HBC2018) for understanding human behavior|Predicting the destination of trajectory obtained from BLE beacon signals|44|90.0|2018|2.0|100 USD|100|183.0|https://competitions.codalab.org/competitions/17401|||||urbanisme, navigation|signal processing|predict which of 9 locations are start and goal for test trajectories|classification|||Prediction score|accuracy|False|False|True|False||[1.4245690678569731]
Fact Extraction and VERification (FEVER) Challenge|The purpose of the FEVER challenge is to evaluate the ability of a system to verify information using evidence from Wikipedia. Given a factual claim involving one or more entities (resolvable to Wikipedia pages), the system must extract textual evidence (sets of sentences from Wikipedia pages) that support or refute the claim and use the evidence to predict whether the claim is supported or refuted from this evidence.|335|1915.0|2018|3.0|None|None|116.0|https://competitions.codalab.org/competitions/18814|fever.ai|https://arxiv.org/pdf/1803.05355.pdf|||linguistics|NLP|Given a factual claim involving one or more entities (resolvable to Wikipedia pages), the system must extract textual evidence (sets of sentences from Wikipedia pages) that support or refute the claim. Using this evidence, label the claim as Supported, Refuted given the evidence or NotEnoughInfo|multi-class classification|||FEVER Score|accuracy, evidence recall|True|True|True|False||[1.2479215092527438]
NEWS 2018 (Standard Submissions)|Shared Task on Named Entity Transliteration (Standard Submissions only)|26|562.0|2018|19.0|None|None|12.0|https://competitions.codalab.org/competitions/18905|http://workshop.colips.org/news2018/index.html|http://workshop.colips.org/news2018/documents/news2018whitepaper.pdf|||linguistics|NLP|neural machine translation, transliteration of named entities|machine translation||Five different datasets comprising a total of 14 different transliteration tasks|Accuracy|Word Accuracy in Top-1 (ACC), Fuzziness in Top-1 (Mean F-score), Mean Reciprocal Rank (MRR), Mean Average Precision (MAP)|True|True|True|False||[3.1999999999999997]
MCS 2018. Adversarial Attacks on Black Box Face Recognition|Participants will try to spoof black box face recognition network|155|140.0|2018|2.0|300000 RUB|4700|37.0|https://competitions.codalab.org/competitions/19090||||||computer vision|test the vulnerability of black-box face recognition systems|DL, adversarial attacks|||Score|spesific score https://competitions.codalab.org/competitions/19090#learn_the_details-evaluation|True|True|True|False||[0.0]
Telenav MapAI Contest|serious enough?|110|74.0|2018|1.0|10000|10000|122.0|https://competitions.codalab.org/competitions/19024|||||urbanisme|computer vision|detecting navigation features in street-level imagery|image recognition|||Accuracy|accuracy|False|False|True|False||[1.503398767549711]
ChaLearn LAP 2015 - Track 2: Action Recognition|<p>Recognizing actions/interactions using 235 performances of 11 action/interaction categories recorded and manually labeled in continuous RGB sequences of different people performing natural isolated and collaborative behaviors.</p>|82|28.0|2014|2.0|1000|1000|109.0|https://competitions.codalab.org/competitions/2241|||||multimedia|computer vision|action and interaction recognition from RGB data sequences|action recognition|||Overlap|Jaccard Index (overlapping)|False|False|True|False||[1.0]
ChaLearn LAP 2015 - Track 4: Cultural Event Recognition|<p>For this track more than 11000 images are labeled with the objective of performing automatic cultural event recognition from 50 categories in still images. This is the first dataset on cultural events from all around the globe. The Cultural Event Recognition challenge aims to investigate the performance of recognition methods based on several cues like garments, human poses, objects, background, etc. To this end, the cultural event dataset contains significant variability in terms of clothes, actions, illumination, localization and context.</p>|92|132.0|2015|2.0|1000|1000|56.0|https://competitions.codalab.org/competitions/2611|||||multimedia|computer vision|image recognition|multi-class classification|11000 images||mAP|average precision (AP)|False|False|True|False||[1.0]
ChaLearn LAP 2015 - Track 1: Human Pose Recovery|<p>More than 8,000 frames of continuous RGB sequences are recorded and labeled with the objective of performing human pose recovery by means of recognizing more than 120,000 human limbs of different people.</p>|86|187.0|2014|2.0|1000|1000|109.0|https://competitions.codalab.org/competitions/2231|||||multimedia|computer vision|human pose recovery from RGB data|image recognition|||Overlap|Jaccard Index (overlapping)|False|False|True|False||[1.0]
ChaLearn LAP 2015 - Track 1: Age Estimation|<p>For this track nearly 5000 images were collectively labeled with the objective of performing automatic apparent age estimation from RGB face still images. This is the first dataset on age estimation containing annotations on apparent age. The Age Estimation challenge aims to investigate the performance of estimation methods on apparent age rather than real age.</p>|250|1824.0|2015|2.0|6000|6000|89.0|https://competitions.codalab.org/competitions/4711|||||multimedia|computer vision|age estimation/recognition|classification or regression problem|||Error|Error|False|False|True|False||[0.0]
ChaLearn LAP 2015 - Track 2: Cultural Event Recognition|<p>For this track more than 28000 images are labeled with the objective of performing automatic cultural event recognition from 100 categories in still images. These images belongs to 99 different cultural events and one non-class. This is the first dataset on cultural events from all around the globe. The Cultural Event Recognition challenge aims to investigate the performance of recognition methods based on several cues like garments, human poses, objects, background, etc. To this end, the cultural event dataset contains significant variability in terms of clothes, actions, illumination, localization and context.</p>|122|327.0|2015|2.0|6000|6000|89.0|https://competitions.codalab.org/competitions/4081|||||multimedia|computer vision|image recognition|multi-class classification|28000 images||mAP|average precision (AP)|False|False|True|False||[1.5711318795430946]
VQA Real Image Challenge (Multiple-Choice)|This challenge evaluates algorithms on the VQA Multiple-Choice task for the dataset built on top of MSCOCO test2015 real images.|162|2124.0|2015|3.0|None|None|243.0|https://competitions.codalab.org/competitions/6971|||||multimedia|computer vision|pick an option out of the provided answers|multi-class classification|||Overall|new evaluation metric which is robust to inter-human variability in phrasing the answers|False|False|True|False||[0.0]
VQA Real Image Challenge (Open-Ended)|This challenge evaluates algorithms on the VQA Open-Ended task for the dataset built on top of MSCOCO test2015 real images.|531|24724.0|2015|3.0|None|None|243.0|https://competitions.codalab.org/competitions/6961|||||multimedia|computer vision, NLP|open-ended questions about images (VQA: Visual Question Answering)|text generation|||Overall|new evaluation metric which is robust to inter-human variability in phrasing the answers|False|False|True|False||[0.0]
VQA Abstract Scene Challenge (Multiple-Choice)|This challenge evaluates algorithms on the VQA Multiple-Choice task for the dataset built on top of VQA abstract scenes.|35|41.0|2015|2.0|None|None|228.0|https://competitions.codalab.org/competitions/6991|||||multimedia|computer vision|pick an option out of the provided answers|multi-class classification|||Overall|new evaluation metric which is robust to inter-human variability in phrasing the answers|False|False|True|False||[0.0]
VQA Abstract Scene Challenge (Open-Ended)|This challenge evaluates algorithms on the VQA Open-Ended task for the dataset built on top of VQA abstract scenes.|47|67.0|2015|2.0|None|None|228.0|https://competitions.codalab.org/competitions/6981|||||multimedia|computer vision, NLP|open-ended questions about images (VQA: Visual Question Answering)|text generation|||Overall|new evaluation metric which is robust to inter-human variability in phrasing the answers|False|False|True|False||[0.0]
Dialog State Tracking Challenge 5 (DSTC5)|Dialog state tracking is one of the key sub-tasks of dialog management, which defines the representation of dialog states and updates them at each moment on a given on-going conversation. To provide a common testbed for this task, the first Dialog State Tracking Challenge (DSTC) was organized. More recently, Dialog State Tracking Challenges 2 & 3 and Dialog State Tracking Challenge 4 have been successfully completed. In this fifth edition of the Dialog State Tracking Challenge, we will continue evaluating the dialog state tracking task on human-human dialogs. Different from DSTC4, in this edition of the Challenge we will focus on cross-language DST. In addition to this main task, we also propose a series of pilot tracks for the core components in developing end-to-end dialog systems based on the same dataset. We expect these shared efforts on human dialogs will contribute to progress in developing much more human-like systems.|15|57.0|2016|2.0|None|None|104.0|https://competitions.codalab.org/competitions/10851|||||linguistics|NLP|dialog state tracking|dialogue generation||35 dialogs, 31034 utterances, 273580 words|Accuracy|Accuracy, Precision, Recall, F-measure|True|False|True|False|https://github.com/codalab/codalab-competitions/wiki/User_Participating-in-a-Competition|[0.0]
ChaLearn LAP Large-scale Isolated Gesture Recognition Challenge|Recognition of isolated gestures from RGB+D videos.|78|245.0|2016|2.0|None|None|48.0|https://competitions.codalab.org/competitions/10331||||||computer vision|gesture recognition|gesture recognition||50,000 gestures (videos)|Accuracy|accuracy|False|True|True|False||[2.0899055668147133]
ChaLearn LAP Large-scale Continuous Gesture Recognition Challenge|Continuous gesture recognition from RGB+D data.|70|32.0|2016|2.0|None|None|48.0|https://competitions.codalab.org/competitions/10341||||||computer vision|gesture recognition|gesture recognition|||Mean Jaccard Index|Mean Jaccard Index|False|False|True|False||[2.8087366557400038]
ChaLearn LAP. Apparent Personality Analysis: First Impressions (second round)|This competition is devoted to all aspects of computer vision and pattern recognition for the analysis of human personality from images and videos. Co-located with a ICPR 2016 workshop there is a second round challenge on first impressions, that is, recognizing personality traits for users after seeing a short video.|86|203.0|2016|2.0|None|None|48.0|https://competitions.codalab.org/competitions/10751||https://www.researchgate.net/publication/316446311_ChaLearn_Joint_Contest_on_Multimedia_Challenges_Beyond_Visual_Analysis_An_overview|||HR|computer vision|automatic recommendations based on multi-media CVs (videos): predicting two types of variables (1) personality traits (context variables) and (2) whether a person should be invited to a job interview (decision variable)|classification|||AVG|accuracy|False|False|True|False||[1.0233513917429478]
Data Science Game 2016: Final|DSG16 final hackathon|48|1148.0|2014|2.0|None|None|697.0|https://competitions.codalab.org/competitions/11711|||||insurance|NLP|insurance quote conversion model|binary classification|||Score|binary log-loss|False|False|True|False|https://www.kaggle.com/wiki/LogarithmicLoss|[0.0]
Context of Experience Challenge|This is the Mediaeval Context of Experience challenge.|20|22.0|2016|2.0|None|None|162.0|https://competitions.codalab.org/competitions/10581|||||media|computer vision|video recommendation focuses on predicting personal preferences: viewers watching movies on an airplane|binary classification|||F1 score|Precision, Recall and F1 score|True|False|True|False||[1.0687543798580965]
SemEval-2017 Task 9 (parsing subtask): English Biomedical AMR parsing|Given a Biomedical paper sentence, produce an AMR.|21|36.0|2016|2.0|None|None|172.0|https://competitions.codalab.org/competitions/15437|||||medicine|NLP|English Biomedical AMR parsing||||NF Smatch|automated metric Smatch, Precision, Recall|True|False|True|False||[0.0]
SemEval-2017 Task 9 (generation subtask): English News/Forum generation from AMR|Given a News/Discussion Forum AMR, produce an English sentence.|17|25.0|2016|2.0|None|None|173.0|https://competitions.codalab.org/competitions/15436||||||NLP|English News/Forum generation from AMR||||BLEU|human judgement/BLEU|False|False|True|False||[4.618937644341802]
SemEval-2017 Task 4C Arabic, Sentiment Analysis in Twitter|This is the competition for Subtask C in Arabic of Task 4|6|19.0|2016|2.0|None|None|174.0|https://competitions.codalab.org/competitions/15940|||||linguistics, tweets|NLP|Given a message, classify sentiment conveyed in the tweet towards the topic on a five-point scale (-2,-1,0,1,2).|multi-class classification|||MAE_M|macroaveraged mean absolute error, recall|True|False|True|False||[0.0]
SemEval-2017 Task 4C English, Sentiment Analysis in Twitter|This is the competition for Subtask C in English of Task 4|36|71.0|2016|2.0|None|None|174.0|https://competitions.codalab.org/competitions/15937|||||linguistics|NLP|Given a message, classify sentiment conveyed in the tweet towards the topic on a five-point scale (-2,-1,0,1,2).|multi-class classification|||MAE_M|macroaveraged mean absolute error, recall|True|False|True|False||[0.0]
SemEval-2017 Task 4A Arabic, Sentiment Analysis in Twitter|This is the competition for Subtask A in Arabic of Task 4|24|159.0|2016|2.0|None|None|174.0|https://competitions.codalab.org/competitions/15887|||||linguistics|NLP|Given a message, classify whether the message is of positive, negative, or neutral sentiment.|multi-class classification|||AverageR|macro-averaged recall (recall averaged across the three classes)|False|False|True|False||[1.3973021124968186]
SemEval-2017 Task 4E English, Sentiment Analysis in Twitter|This is the competition for Subtask E in English of Task 4|20|41.0|2016|2.0|None|None|174.0|https://competitions.codalab.org/competitions/15939|||||linguistics|NLP|Given a set of tweets about a given topic, estimate the distribution of the tweets across a five-point scale: the five classes of a five-point scale.|ordinal regression|||EMD|Earth Mover’s Distance|False|False|True|False||[0.0]
SemEval-2017 Task 4A English, Sentiment Analysis in Twitter|This is the competition for Subtask A in English of Task 4|103|504.0|2016|2.0|None|None|174.0|https://competitions.codalab.org/competitions/15885|||||linguistics|NLP|Given a message, classify whether the message is of positive, negative, or neutral sentiment.|multi-class classification|||AverageR|macro-averaged recall (recall averaged across the three classes)|False|False|True|False||[1.2673751126713793]
SemEval-2017 Task 4E Arabic, Sentiment Analysis in Twitter|This is the competition for Subtask E in Arabic of Task 4|10|17.0|2016|2.0|None|None|174.0|https://competitions.codalab.org/competitions/15968|||||linguistics|NLP|Given a set of tweets about a given topic, estimate the distribution of the tweets across a five-point scale: the five classes of a five-point scale.|ordinal regression|||EMD|Earth Mover’s Distance|False|False|True|False||[1.2857142857142856]
SemEval-2017 Task 4D Arabic, Sentiment Analysis in Twitter|This is the competition for Subtask D in Arabic of Task 4|8|21.0|2016|2.0|None|None|182.0|https://competitions.codalab.org/competitions/15967|||||linguistics|NLP|"Given a set of tweets about a given topic, estimate the distribution of the tweets across a two-point scale: the ""Positive"" and ""Negative"" classes"|binary classification|||KLD|KLD, Pearson Divergence|True|False|True|False||[1.253146853146853]
SemEval-2017 Task 4B Arabic, Sentiment Analysis in Twitter|This is the competition for Subtask B in Arabic of Task 4|15|31.0|2016|2.0|None|None|182.0|https://competitions.codalab.org/competitions/15889|||||linguistics|NLP|Given a message, classify whether the message is of positive or negative sentiment towards the topic.|binary classification|||AverageF1|F1, recall, accuracy|True|False|True|False||[1.1980631052796002]
SemEval-2017 Task 4B English, Sentiment Analysis in Twitter|This is the competition for Subtask B in English of Task 4|50|151.0|2016|2.0|None|None|182.0|https://competitions.codalab.org/competitions/15888|||||linguistics|NLP|Given a message, classify whether the message is of positive or negative sentiment towards the topic.|binary classification|||AverageF1|F1, recall, accuracy|True|False|True|False||[1.2521983819908549]
SemEval-2017 Task 4D English, Sentiment Analysis in Twitter|This is the competition for Subtask D in English of Task 4|31|48.0|2016|2.0|None|None|182.0|https://competitions.codalab.org/competitions/15938|||||linguistics|NLP|"Given a set of tweets about a given topic, estimate the distribution of the tweets across a two-point scale: the ""Positive"" and ""Negative"" classes"|binary classification|||KLD|KLD, Pearson Divergence|True|False|True|False||[0.0]
SemEval-2017 Task 6: #HashtagWars Learning a Sense of Humor - Subtask B|This the the official competition for the SemEval-2017 Task 6 - Subtask B|21|18.0|2016|2.0|None|None|183.0|https://competitions.codalab.org/competitions/15689|https://alt.qcri.org/semeval2017/task6/||||linguistics, humour|NLP|characterize the sense of humor represented in this show. Given a set of hashtags, the goal is to predict which tweets the show will find funnier within each hashtag|classificaiton (with cardianlity)||||edit distance|False|False|True|False||[0.0]
SemEval-2017 Task 6: #HashtagWars Learning a Sense of Humor - Subtask A|This the the official competition for the SemEval-2017 Task 6 - Subtask A|26|64.0|2016|2.0|None|None|183.0|https://competitions.codalab.org/competitions/15682|||||linguistics|NLP|Given a set of hashtags, the goal is to predict which tweets the show will find funnier within each hashtag|ordinal regression|||Accuracy|micro-averaged accuracy|False|False|True|False||[0.0]
|This is the SemEval 2017 Task 3 Subtask C competition.|21|26.0|2016|2.0|None|None|183.0|https://competitions.codalab.org/competitions/15636|||||linguistics|NLP|rerank the 100 comments (10 questions x 10 comments) according to their relevance with respect to the original question|ordinal regression|||MAP|MAP, Average Recall, and MRR|True|False|True|False||[1.383495145631068]
SemEval-2017 Task 3 Subtask A|This is the SemEval 2017 Task 3 Subtask A competition.|44|107.0|2016|2.0|None|None|183.0|https://competitions.codalab.org/competitions/15632|||||linguistics|NLP|rerank these 10 comments according to their relevance with respect to the question|ordinal regression|||MAP|MAP, Average Recall, and MRR|False|False|True|False||[1.1419470435994425]
SemEval-2017 Task 3 Subtask D|This is the SemEval 2017 Task 3 Subtask D competition.|13|68.0|2016|2.0|None|None|183.0|https://competitions.codalab.org/competitions/15637|||||linguistics|NLP|rerank the 30 question-answer pairs according to their relevance with respect to the original question|ordinal regression|||MAP|MAP, Average Recall, and MRR|False|False|True|False||[1.231984205330701]
SemEval-2017 Task 3 Subtask B|This is the SemEval 2017 Task 3 Subtask B competition.|36|137.0|2016|2.0|None|None|183.0|https://competitions.codalab.org/competitions/15635|||||linguistics|NLP|rerank the related 10 questions according to their similarity with respect to the original question|ordinal regression|||MAP|MAP, Average Recall, and MRR|False|False|True|False||[1.101272827891533]
RumourEval-2017, subtask B (closed)|RumourEval - Determining rumour veracity and support for rumours. This is subtask B, determining the veracity of a message (closed variant).|13|41.0|2016|2.0|None|None|185.0|https://competitions.codalab.org/competitions/16173|||||linguistics|NLP|predict the veracity of a given rumour and confidence rating|binary classification|||Score|accuracy|False|True|True|False||[0.0]
RumourEval-2017, subtask A|RumourEval - Determining rumour veracity and support for rumours. This is subtask A, determining the stance of a message.|36|39.0|2016|2.0|None|None|185.0|https://competitions.codalab.org/competitions/16171|||||linguistics|NLP|label the type of interaction between a given statement (rumourous tweet) and a reply tweet (the latter can be either direct or nested replies). Each tweet in the tree-structured thread will have to be categorised into one of the four categories|multi-class classification|||Accuracy|accuracy|False|False|True|False||[0.0]
SemEval-2017 Task 1: Semantic Textual Similarity|Official competition site for the SemEval-2017 Task 1: Semantic Textual Similarity|68|127.0|2016|3.0|None|None|196.0|https://competitions.codalab.org/competitions/16051|||||linguistics|NLP|Given two sentences, participating systems are asked to return a continuous valued similarity score on a scale from 0 to 5, with 0 indicating that the semantics of the sentences are completely independent and 5 signifying semantic equivalence.|ordinal regression||||Pearson correlation|False|False|True|False||[0.0]
MediaEval 2016 Context of Experience Task|This is the Mediaeval Context of Experience task for 2016.|10|19.0|2017|2.0|None|None|119.0|https://competitions.codalab.org/competitions/11191|||||media|computer vision|predicting the multimedia content that users find most fitting to watch in specific viewing situations|binary classification|||F1 score|Precision, Recall and F1 score|True|False|True|False||[1.070198472067159]
Fake News Challenge Stage 1 (FNC-I) - Stance Detection|The official leaderboard for the Fake News Challenge Stage 1 (FNC-I) - Stance Detection.|200|457.0|2017|2.0|None|None|6.0|https://competitions.codalab.org/competitions/16843|http://www.fakenewschallenge.org/||||media|NLP|Stance Detection: estimating the stance of a body text from a news article relative to a headline. Specifically, the body text may agree, disagree, discuss or be unrelated to the headline|multi-class classification|||score|two-level scoring system|False|False|True|False||[1.1568604189465326]
ChaLearn LAP Real Versus Fake Expressed Emotion Challenge @ICCV 2017|Recognition of fake and real emotions from videos|63|267.0|2017|2.0|None|None|73.0|https://competitions.codalab.org/competitions/16611||||||computer vision|recognize deceit and the authenticity of emotional displays|binary classification||12 videos representing 6 basic emotions (angry, happy, sad, disgust, contempt, surprise) for real and fake expressions|Perfomance rate|accuracy|False|False|True|False||[1.0322605619082272]
ChaLearn LAP Large-scale Isolated Gesture Recognition Challenge (Round 2) @ICCV 2017|Recognition of isolated gestures from RGB+D videos.|52|81.0|2017|2.0|None|None|73.0|https://competitions.codalab.org/competitions/16491||||||computer vision|gesture recognition, Large-scale Learning, User Independent|multi-class classification||50,000 gestures (videos)|Accuracy|accuracy|False|True|True|False||[1.0449890806948228]
ChaLearn LAP Large-scale Continuous Gesture Recognition Challenge (Round 2) @ICCV 2017|Continuous gesture recognition from RGB+D data.|48|51.0|2017|2.0|None|None|73.0|https://competitions.codalab.org/competitions/16499||||||computer vision|gesture recognition, Large-scale Learning, User Independent|multi-class classification||47933 RGB-D gestures in 22535 RGB-D gesture videos|Accuracy|accuracy|False|True|True|False||[1.392822222492979]
Textbook Question Answering - CVPR 2017 Workshop|Answer science questions from textual and visual source material|34|89.0|2017|2.0|6000|6000|45.0|https://competitions.codalab.org/competitions/16931||||||computer vision, NLP|Track 1- Text Questions Track, Track 2- Diagram Questions Track|multi-class classification|||Question Types|accuracy|False|True|True|False||[0.0]
THOR Challenge - CVPR 2017 Workshop|Navigate and find objects in a virtual environment|40|82.0|2017|2.0|6000|6000|45.0|https://competitions.codalab.org/competitions/16929|http://vuchallenge.org/thor.html|||||RL|The agents that the challenge participants provide receive visual input from THOR and should perform an action based on the visual input|reinforcement learning|||Score|average of the optimum number of actions (ot) over the predicted set of actions (pt) over all the tasks|False|False|True|False||[0.0]
Charades Challenge - CVPR 2017 Workshop|Recognize and localize actions in the Charades Dataset|81|127.0|2017|2.0|6000|6000|45.0|https://competitions.codalab.org/competitions/16932||||||computer vision, NLP|classification track is to recognize all activity categories for given videos ('Activity Classification'), where multiple overlapping activities can occur in each video. The localization track is to find the temporal locations of all activities in a video ('Activity Localization')|classification and localization|||mAP|mean average precision (mAP)|False|True|True|False||[0.0]
RedICA Text-Image Matching (RICATIM) Challenge|Image annotation casted as a binary classification problem. Participants will be provided with a classification data set in which the feature space of each instance encodes a pair (image-keyword), the class of the instance being +1 (when the keyword is relevant for describing the image) and 0 (when the keyword is irrelevant. Additionally, the raw images and words will be made publicly available, so that participants can take advantage of such information. Classification performance will be used to determine the winners of the challenge. Participation of members of RedICA is encouraged, although the challenge is open to anyone (see the rules).|48|522.0|2017|2.0|None|None|45.0|https://competitions.codalab.org/competitions/17120||||||computer vision, NLP||binary classification|||Accuracy|Accuracy, Recall, Precision , F1|True|True|True|False||[1.0220780478357703]
Large-Scale Video Classification Challenge|Welcome to the website of the Large-Scale Video Classification Challenge workshop. Recognizing visual contents in unconstrained videos has become a very important problem for many applications, such as Web video search and recommendation, smart advertising, robotics, etc. This workshop and challenge aims at exploring new challenges and approaches for large-scale video classification with large number of classes from open source videos in a realistic setting, based upon an extension of Fudan-Columbia Video Dataset (FCVID).|34|106.0|2017|2.0|None|None|57.0|https://competitions.codalab.org/competitions/17332||https://dl.acm.org/doi/abs/10.1145/3123266.3138874|||multimedia|computer vision|large-scale video classification with large number of classes from open source videos in a realistic setting, based upon an extension of Fudan-Columbia Video Dataset (FCVID)|multi-class classification|||mAP|mean Average Precision (mAP)|False|False|True|False||[1.1657352088648592]
The Large Scale Movie Description Challenge (LSMDC) 2017 : Movie Description|<p>The goal of the challenge is to compare different algorithms for generating automatic video descriptions. The challenge is based on two large-scale movie description datasets: MPII-MD and M-VAD.</p>|69|173.0|2017|2.0|None|None|60.0|https://competitions.codalab.org/competitions/6121|||||movies|computer vision, NLP|Automatically describing open-domain videos using rich natural sentences|text generation|||CIDEr-D|BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L, METEOR and CIDEr-D|True|False|True|False||[1.4928861788617882]
UH COSC 7336 - Author Profiling|This is the first competiition for the course.|29|266.0|2017|2.0|None|None|26.0|https://competitions.codalab.org/competitions/17449||||||NLP|predict demographic information about the author of a given document|multi-class classification|||Overall Accuracy|accuracy, F1|True|False|True|False||[1.1601430149192264]
2017 Visual Domain Adaptation (VisDA2017) Classification Challenge|The VisDA challenge aims to test domain adaptation methods’ ability to transfer source knowledge and a it to novel target domains.|105|376.0|2017|2.0|None|None|134.0|https://competitions.codalab.org/competitions/17052|https://ai.bu.edu/visda-2017/|||||computer vision|test domain adaptation methods’ ability to transfer source knowledge and adapt it to novel target domains|multi-class classification|||MeanAcc|mean accuracies for each category and the overall mean of these accuracies|True|False|True|False|https://github.com/VisionLearningGroup/taskcv-2017-public/tree/master/classification|[0.0]
COSC7336 - Text Summarizations|This is the second competiition for the course.|38|120.0|2017|2.0|None|None|60.0|https://competitions.codalab.org/competitions/17545||||||NLP|text summarization|text summarization|||Average BLEU|average BLEU|False|False|True|False||[5.62139879139926]
COSC7336 - Community Question & Answering|This is the third and final competiition for the course.|26|263.0|2017|1.0|None|None|87.0|https://competitions.codalab.org/competitions/17581||||||NLP|a new question Q (aka the original question), and the set of the first ten related questions from the forum retrieved by a search engine, each associated with its first ten comments appearing in its thread, the goal is to rank the 100 comments according to their relevance with respect to the original question|ordinal regression|||F1|F1, mean Average Precision (mAP)|True|False|True|False||[0.0]
SemEval-2018 task 3 - Irony detection in English tweets|This is SemEval-2018 task 3 on irony detection with two subtasks A and B.|389|262.0|2017|5.0|None|None|161.0|https://competitions.codalab.org/competitions/17468||https://aclanthology.org/S18-1087.pdf|||linguistics, humour, social media|NLP|The first subtask is a two-class (or binary) classification task where the system has to predict whether a tweet is ironic or not. The second subtask is a multiclass classification task where the system has to predict one out of four labels describing i) verbal irony realized through a polarity contrast, ii) verbal irony without such a polarity contrast (i.e., other verbal irony), iii) descriptions of situational irony, iv) non-irony|binary and multi-class classification|||F1-score|accuracy, precision, recall and F1|True|True|True|True||[1.4215839561849848]
NTIRE 2018 Image Dehazing Challenge - Track 2: Outdoor|The goal is to promote and compare state-of-the-art solutions for single image dehazing. We provide for this challenge a novel dataset of hazy images obtained in indoor and outdoor environments with ground truth haze-free references which allows for an accurate benchmarking of the performances achieved by the dehazing methods.|251|301.0|2018|2.0|16000|16000|62.0|https://competitions.codalab.org/competitions/18047||||||computer vision|Outdoor single image dehazing competition|||35 hazy images|PSNR|Peak Signal To Noise Ratio (PSNR) and, complementary, the Structural Similarity (SSIM) index|True|False|True|False||[nan]
NTIRE 2018 Image Dehazing Challenge - Track 1: Indoor|The goal is to promote and compare state-of-the-art solutions for single image dehazing. We provide for this challenge a novel dataset of hazy images obtained in indoor and outdoor environments with ground truth haze-free references which allows for an accurate benchmarking of the performances achieved by the dehazing methods.|251|476.0|2018|2.0|16000|16000|62.0|https://competitions.codalab.org/competitions/18046||||||computer vision|Indoor single image dehazing competition|||25 hazy images|PSNR|Peak Signal To Noise Ratio (PSNR) and, complementary, the Structural Similarity (SSIM) index|True|False|True|False||[nan]
NTIRE 2018 Spectral Reconstruction Challenge - Track 1: Clean|To gauge the current state-of-the-art in spectral reconstruction from RGB images, to compare and to promote different solutions we are organizing an NTIRE challenge in conjunction with the CVPR 2018 conference. The largest dataset to date will be introduced with the challenge. It is the first spectral reconstruction from RGB images online challenge.|219|121.0|2018|2.0|16000|16000|70.0|https://competitions.codalab.org/competitions/18034||||||computer vision|spectral reconstruction from RGB images, that is, the task of restoration of hyperspectral images (high spectral resolution) for a single RGB input image based on a set of prior examples with hyperspectral and corresponding RGB images. Track 1: “Clean” recovering hyperspectral data from uncompressed 8-bit RGB images created by applying a know response function to ground truth hyperspectral information|image transformation|||MRAE|MRAE, RMSE|True|False|True|False||[0.0]
NTIRE 2018 Spectral Reconstruction Challenge - Track 2: Real World|To gauge the current state-of-the-art in spectral reconstruction from RGB images, to compare and to promote different solutions we are organizing an NTIRE challenge in conjunction with the CVPR 2018 conference. The largest dataset to date will be introduced with the challenge. It is the first spectral reconstruction from RGB images online challenge.|130|46.0|2018|2.0|16000|16000|70.0|https://competitions.codalab.org/competitions/18033||||||computer vision|spectral reconstruction from RGB images, that is, the task of restoration of hyperspectral images (high spectral resolution) for a single RGB input image based on a set of prior examples with hyperspectral and corresponding RGB images. Track 2: “Real World” recovering hyperspectral data from jpg-compressed 8-bit RGB images created by applying an unknown response function to ground truth hyperspectral information.|image transformation|||MRAE|MRAE, RMSE|True|False|True|False||[0.0]
CALCS 2018 [ENG-SPA] - Named Entity Recognition on Code-switched Data|The third workshop of Computational Approaches on Linguistic Code-Switching, CALCS, presents an NER shared task for English-Spanish data.|40|1060.0|2018|2.0|None|None|27.0|https://competitions.codalab.org/competitions/18725|||||social media|NLP|predict the right entity type using the IOB scheme|multi-class classification|||FB1 Average|harmonic mean F1, Surface Forms F1|True|False|True|False||[1.086074391239112]
CALCS 2018 [MSA-EGY] - Named Entity Recognition on Code-switched Data|The third workshop of Computational Approaches on Linguistic Code-Switching, CALCS, presents an NER shared task for Modern Standard Arabic-Egyptian data.|25|134.0|2018|2.0|None|None|27.0|https://competitions.codalab.org/competitions/18724|||||social media|NLP|predict the right entity type using the IOB scheme|multi-class classification|||FB1 Average|harmonic mean F1, Surface Forms F2|True|False|True|False||[1.0699158764252519]
The DAVIS Challenge on Video Object Segmentation @ CVPR 2018|Submission site for the DAVIS Challenges on Video Object Segmentation.|259|1559.0|2017|3.0|0|0|341.0|https://competitions.codalab.org/competitions/16526|https://davischallenge.org/challenge2018/semisupervised.html|||||computer vision|video object segmentation|multi-class classification|||Global-Mean|mean over all objects and between the Region (IoU) and Boundary measures|True|False|True|False||[0.0]
eHealth KD @TASS18|Automated knowledge extraction from electronic health documents.|34|195.0|2018|3.0|100|100|105.0|https://competitions.codalab.org/competitions/18188|||||medicine|NLP|Subtask A: Identification of keyphrases, Subtask B: Classification of key phrases, Subtask C: Setting semantic relationships|keyword extraction, multi-class classification|||Score (Average F1)|F1, precision, recall|True|True|True|False||[1.2210526315789476]
WIDER Face & Pedestrain Challenge - Track 1: Face Detection|ECCV2018|190|240.0|2018|2.0|None|None|71.0|https://competitions.codalab.org/competitions/19053||||||computer vision|face detection|face detection||32,203 images and 393,703 faces bounding box annotations|Average AP|average AP|False|False|True|False||[1.233109725116529]
WIDER Face & Pedestrain Challenge - Track 3: Person Search|ECCV2018|134|42.0|2018|2.0|None|None|71.0|https://competitions.codalab.org/competitions/19055|||||multimedia|NLP|an image of a target cast and some candidates (frames of a movie with person bounding boxes), you are asked to search for all the instances belonging to that cast|multi-class classification||192 movies, among which 115 are used for training, 19 for validation and 58 for testing|mAP|mean Average Precision (mAP)|False|False|True|False||[1.1968639076334986]
WIDER Face & Pedestrain Challenge - Track 2: Pedestrian Detection|ECCV2018|253|219.0|2018|2.0|None|None|71.0|https://competitions.codalab.org/competitions/19118|||||multimedia|computer vision|find all the pedestrians and cyclists and submit their bounding boxes and scores in the images|regression|||Average AP|average AP|False|True|True|False||[1.4313298549771987]
Chalearn LAP Inpainting Competition Track 2 - Video decaptioning|This is a competition on video inpainting that consists in removing subtitles in video clips.|68|98.0|2018|2.0|None|None|141.0|https://competitions.codalab.org/competitions/18421|||||multimedia|computer vision|text removal in video sequences|video reconstruction|||<Rank>/MSE|Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), Structural Dissimilarity (DSSIM)|True|False|True|False||[0.0]
Chalearn LAP Inpainting Competition Track 1 - Inpainting of still images|A competition consisting in reconstructing masked parts of human centered images while taking into account human pose knowledge.|53|11.0|2018|2.0|None|None|141.0|https://competitions.codalab.org/competitions/18423|||||multimedia|computer vision|reconstructing masked parts of human centered images while taking into account human pose knowledge|image reconstruction|||<Rank>/MSE|Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), Structural Dissimilarity (DSSIM)|True|False|True|False||[1.0]
Chalearn LAP Inpainting Competition Track 3 - Fingerprint Denoising and Inpainting|The objective of this competition is to benchmark the algorithms that can inpaint and denoise fingerprint images that contain artifacts like noise, scratches, etc.|97|83.0|2018|2.0|None|None|141.0|https://competitions.codalab.org/competitions/18426|||||multimedia|computer vision|fingerprint denoising and inpainting|image reconstruction|||<Rank>/MSE|Peak Signal-to-Noise Ratio (PSNR), Structural Dissimilarity (DSSIM)|True|False|True|False||[0.0]
PIRM 2018 Spectral Image Super-Resolution Challenge- Track 1||79|192.0|2018|3.0|None|None|70.0|https://competitions.codalab.org/competitions/19226|||||multimedia|computer vision|leverage the increased spatial resolution of colour (RGB) cameras and the link between spectral and trichromatic images of the scene|image reconstruction|||APPSA|mean of relative absolute error (MRAE), Spectral Information Divergence (SID), mean observer score (MOS)|True|False|True|False||[0.0]
PIRM 2018 Spectral Image Super-Resolution Challenge- Track 2 Colour Guidance||54|47.0|2018|3.0|None|None|70.0|https://competitions.codalab.org/competitions/19227|||||multimedia|computer vision|obtain 3x spatially super-resolved spectral images and pseudo-colour images making use of stereo pairs|image reconstruction|||APPSA|mean of relative absolute error (MRAE), Spectral Information Divergence (SID), mean observer score (MOS)|True|False|True|False||[0.0]
COCO 2018 DensePose Challenge|This challenge is designed to push the state of the art in dense human pose estimation.|25|50.0|2018|2.0|None|None|49.0|https://competitions.codalab.org/competitions/19636|||||multimedia|computer vision|human body detection, human body segmentation, mapping all image pixels that belong to a human body to the 3D surface of the body.|image recognition|||AP|AP|False|False|True|False||[1.1808118081180812]
ECCV 2018 PoseTrack DensePose Challenge|This challenge is designed to advance the state of the art in dense human pose tracking.|45|27.0|2018|2.0|None|None|50.0|https://competitions.codalab.org/competitions/19650|||||multimedia|computer vision|human body detection, human body segmentation, mapping all image pixels that belong to a human body to the 3D surface of the body, tracking the estimated mappings over time.|image recognition|||AP|AP|False|False|True|False||[1.1374407582938386]
2018 Visual Domain Adaptation (VisDA2018) Detection Challenge|The VisDA challenge aims to test domain adaptation methods’ ability to transfer source knowledge and adapt it to novel target domains.|61|205.0|2018|2.0|None|None|101.0|https://competitions.codalab.org/competitions/18892|||||multimedia|computer vision|test domain adaptation methods’ ability to transfer source knowledge and adapt it to novel target domains|image recognition|||mAP|mAP, Per Category Average Precision (AP)|True|False|True|False||[0.0]
2018 Visual Domain Adaptation (VisDA2018) Open-Set Classification Challenge|The VisDA challenge aims to test domain adaptation methods’ ability to transfer source knowledge and adapt it to novel target domains.|82|240.0|2018|2.0|None|None|101.0|https://competitions.codalab.org/competitions/19113|||||multimedia|computer vision|test domain adaptation methods’ ability to transfer source knowledge and adapt it to novel target domains|multi-class classification|||Mean|mean accuracy,  Per Category accuracy|True|False|True|False||[0.0]
The 1st Large-scale Video Object Segmentation Challenge|YouTube-VOS is the first large-scale dataset for video object segmentation. Our dataset contains 4000+ YouTube videos, 70+ common objects and densely-sampled high-quality pixel-level annotations.|1850|24318.0|2018|2.0|None|None|74.0|https://competitions.codalab.org/competitions/19544|||||multimedia|computer vision|segmenting a particular object isntance throughout the entire video sequence given only the object mask of the first frame|object segmentation|||Overall|Region Jaccard (J) and Boundary F measure (F)|True|False|True|False||[1.1969496021220158]
AutoML3 ::  AutoML for Lifelong Machine Learning|Create a fully Automatic Machine Learning solution capable of adapting to drift.|195|348.0|2018|2.0|15000|15000|98.0|https://competitions.codalab.org/competitions/19836||||||AutoML|drifting concepts, getting away from the simpler i.i.d. cases|binary classification|||<Rank>|Area Under the ROC Curve (AUC|False|False|True|True||[0.0]
SemEval 2019 Task 9 - SubTask A - Suggestion Mining from Online Reviews and Forums|Suggestion Mining from Text|45|10.0|2018|2.0|None|None|166.0|https://competitions.codalab.org/competitions/19955|||||IT forums, hotel reviews|NLP|extraction of suggestions|binary classification|||F1-SCORE|precision, recall and F1 score|True|False|True|False||[1.412362883103432]
MLCAS 2019 Challenge - Sorghum head detection|This is a competition for sorghum head detection in UAV images.|46|99.0|2019|1.0|3700|3700|15.0|https://competitions.codalab.org/competitions/20130|||||agriculture|computer vision|head detection|image recognition|||MAP|mean average precision (MAP)|False|False|True|True||[1.2518301610541727]
WIDER Face & Person Challenge 2019 - Track 3: Cast Search by Portrait|ICCV2019|93|104.0|2019|2.0|None|None|84.0|https://competitions.codalab.org/competitions/20140|||||multimedia|computer vision|search for a person in a large database with just a single image|image recognition|||mAP|mAP|False|False|True|False||[1.1343689091016478]
WIDER Face & Person Challenge 2019 - Track 4: Person Search by Language|ICCV2019|133|347.0|2019|2.0|None|None|84.0|https://competitions.codalab.org/competitions/20142|||||multimedia|computer vision, NLP|searching persons in large-scale image databases with the query of natural language description|image recognition|||top-1 |accuracy|False|False|True|False||[2.174363807728558]
WIDER Face & Person Challenge 2019 - Track 2: Pedestrian Detection|ICCV2019|571|286.0|2019|2.0|None|None|84.0|https://competitions.codalab.org/competitions/20132|||||multimedia|computer vision|pedestrian detection|image recognition|||Average AP|average AP|False|False|True|False||[1.1721351783724563]
WIDER Face & Person Challenge 2019 - Track 1: Face Detection|ICCV2019|252|1139.0|2019|3.0|None|None|90.0|https://competitions.codalab.org/competitions/20146|||||multimedia|computer vision|face detection|image recognition|||Average AP|average AP|False|False|True|False||[1.0596943648519581]
Quantification of tumor heterogeneity|Successful treatment of cancer is still a challenge and this is partly due to a wide heterogeneity of cancer composition across patient population. Unfortunately, accounting for such heterogeneity is very difficult. Clinical evaluation of tumor heterogeneity often requires the expertise of anatomical pathologists and radiologists.This challenge is dedicated to the quantification of intra-tumor heterogeneity using appropriate statistical methods on cancer omics data.In particular, it focuses on estimating cell types and proportion in biological samples based on averaged DNA methylation and full patient history. The goal is to explore various statistical methods for source separation/deconvolution analysis (Non-negative Matrix Factorization, Surrogate Variable Analysis, Principal component Analysis, Latent Factor Models, …).|39|11.0|2019|1.0|None|None|32.0|https://competitions.codalab.org/competitions/20369|||||medicine|computer vision|quantification of intra-tumor heterogeneity,  estimating cell types and proportion in biological samples based on averaged DNA methylation and full patient history|multi-class classification, regression|||MAE_total |MAE|False|True|True|False||[0.0]
The 2nd Large-scale Video Object Segmentation Challenge - Track 2: Video Instance Segmentation|YouTube-VOS is the first large-scale dataset for video object segmentation. Our dataset contains 4000+ YouTube videos, 70+ common objects and densely-sampled high-quality pixel-level annotations.|2184|35260.0|2019|2.0|None|None|90.0|https://competitions.codalab.org/competitions/20128|https://youtube-vos.org/|https://arxiv.org/abs/1905.04804|||multimedia|computer vision|video instance segmentation: simultaneous detection, segmentation and tracking of instances in videos|video recognition||2,883 high-resolution YouTube videos: 2,238 training videos, 302 validation videos and 343 test videos|mAP|AP, AR|True|True|True|False||[1.2394189132560416]
The 2nd Large-scale Video Object Segmentation Challenge - Track 1: Video Object Segmentation|YouTube-VOS is the first large-scale dataset for video object segmentation. Our dataset contains 4000+ YouTube videos, 70+ common objects and densely-sampled high-quality pixel-level annotations.|2401|10620.0|2019|2.0|None|None|90.0|https://competitions.codalab.org/competitions/20127|||||multimedia|computer vision|video object segmentation: dense-prediction video, segmenting a particular object isntance throughout the entire video sequence given only the object mask of the first frame|video recognition||4000+ high-resolution videos clips|Overall|Region Jaccard (J) and Boundary F measure (F)|True|False|True|False||[1.1197809719370293]
2019 PRMU challenge on old Japanese character recognition|Recognizing successive three letters in old Japanese documents|59|466.0|2019|2.0|None|None|122.0|https://competitions.codalab.org/competitions/20388||||||NLP|recognizing successive three characters in old Japanese documents, and output Unicode of a set of three characters|multi-class classification|||recognition rate|recognition rate (number of correct samples / number of all samples)|False|False|True|False||[1.6158965385699833]
AIM 2019 RAW to RGB Mapping Challenge - Track 2: Perceptual|The target of the RAW to RGB mapping challenge is to promote research on this topic and to introduce a novel dataset with pairs of RAW (phone camera) and RGB (DSLR camera) corresponding images.|91|55.0|2019|2.0|None|None|52.0|https://competitions.codalab.org/competitions/20159|||||multimedia|computer vision|"example-based RAW to RGB mapping, that is, the task of mapping an input RAW image to an output RGB image based on a set of prior examples of input-output images. Track 2: ""Perceptual"", the target is to achieve an output image with the best perceptual quality similar to the ground truth output image, as measure by Mean Opinion Score (MOS)."|image transformation|||PSNR|Mean Opinion Score (MOS)|False|False|True|False||[1.125326577468134]
AIM 2019 Bokeh Effect Challenge - Track 1: Fidelity|To gauge the state-of-the-art and promote the research in synthesized Bokeh effect we are organizing this challenge using a novel dataset with pairs of Bokeh-free and optical Bokeh (DSLR camera) corresponding images.|117|108.0|2019|2.0|None|None|53.0|https://competitions.codalab.org/competitions/20156|||||multimedia|computer vision|"Track 1: ""Fidelity"", the target is to achieve a Bokeh effect image with the highest pixel fidelity to the ground truth as measured by PSNR."|image transformation|||PSNR|Peak Signal To Noise Ratio (PSNR) and, complementary, the Structural Similarity (SSIM) index|False|False|True|False||[1.048324421269388]
AIM 2019 Extreme Super-Resolution Challenge - Track 1: Fidelity|Our objectives are to gauge the s-o-t-a in example-based single image extreme super-resolution, to promote research on this topic and to introduce a novel DIV8K benchmark.|219|196.0|2019|2.0|None|None|52.0|https://competitions.codalab.org/competitions/20235|||||multimedia|computer vision|Track 1: Fidelity, the aim is to obtain a network design / solution capable to produce high resolution results with the best fidelity (PSNR) to the ground truth.|image transformation|||PSNR|Peak Signal To Noise Ratio (PSNR) and, complementary, the Structural Similarity (SSIM) index|False|False|True|False||[1.029913920197841]
AIM 2019 Bokeh Effect Challenge - Track 2: Perceptual|To gauge the state-of-the-art and promote the research in synthesized Bokeh effect we are organizing this challenge using a novel dataset with pairs of Bokeh-free and optical Bokeh (DSLR camera) corresponding images.|89|113.0|2019|2.0|None|None|52.0|https://competitions.codalab.org/competitions/20157|||||multimedia|computer vision|"Track 2: ""Perceptual"", the target is to achieve a Bokeh effect image with the best perceptual quality similar to the ground truth Bokeh image, as measure by Mean Opinion Score (MOS)."|image transformation|||PSNR|Mean Opinion Score (MOS)|False|False|True|False||[1.0715203426124198]
AIM 2019 Real World Super-Resolution Challenge - Track 2: Target domain|Our objectives are to gauge the s-o-t-a in real world image super-resolution, to promote research on this topic and to introduce a novel benchmark.|135|67.0|2019|2.0|None|None|56.0|https://competitions.codalab.org/competitions/20164|||||multimedia|computer vision|"Track 2: ""Target domain RWSR"" the aim is to obtain a super-resolved image with the best perceptual quality (measured by Mean Opinion Score (MOS)) similar to the target domain of output images."|image transformation|||PSNR|Mean Opinion Score (MOS)|False|False|True|False||[1.0667549938850387]
AIM 2019 Real World Super-Resolution Challenge - Track 1: Source domain|Our objectives are to gauge the s-o-t-a in real world image super-resolution, to promote research on this topic and to introduce a novel benchmark.|146|66.0|2019|2.0|None|None|56.0|https://competitions.codalab.org/competitions/20163|||||multimedia|computer vision|"Track 1: ""Same domain RWSR"" the aim is to obtain a super-resolved image with the best perceptual quality (measured by Mean Opinion Score (MOS)) similar to source domain of the input image."|image transformation|||PSNR|Mean Opinion Score (MOS)|False|False|True|False||[1.0452818519173]
AIM 2019 Constrained Super-Resolution Challenge - Track 2: Inference optimization|Our objectives are to gauge the s-o-t-a in example-based constrained single image super-resolution, to promote research on this topic and to introduce a novel benchmark.|78|60.0|2019|2.0|None|None|51.0|https://competitions.codalab.org/competitions/20168|||||multimedia|computer vision|Track 2: Inference, the aim is to obtain a network design / solution with the lowest inference time (runtime) on a common GPU (ie. Titan Xp) while being constrained to maintain or improve over MSRResNet (Ledig et al, 2017 & Wang et al, 2018) in terms of number of parameters and the PSNR result.|image transformation|||PSNR|Peak Signal To Noise Ratio (PSNR)|False|False|True|False||[1.0169491525423726]
AIM 2019 RAW to RGB Mapping Challenge - Track 1: Fidelity|The target of the RAW to RGB mapping challenge is to promote research on this topic and to introduce a novel dataset with pairs of RAW (phone camera) and RGB (DSLR camera) corresponding images.|134|211.0|2019|2.0|None|None|52.0|https://competitions.codalab.org/competitions/20158|||||multimedia|computer vision|"example-based RAW to RGB mapping, that is, the task of mapping an input RAW image to an output RGB image based on a set of prior examples of input-output images. Track 1: ""Fidelity"", the target is to obtain an output image with the highest pixel fidelity to the ground truth as measured by PSNR."|image transformation|||PSNR|Peak Signal To Noise Ratio (PSNR)|False|False|True|False||[1.1314796218705634]
AIM 2019 Constrained Super-Resolution Challenge - Track 3: Fidelity optimization|Our objectives are to gauge the s-o-t-a in example-based constrained single image super-resolution, to promote research on this topic and to introduce a novel benchmark.|76|70.0|2019|2.0|None|None|51.0|https://competitions.codalab.org/competitions/20169|||||multimedia|computer vision|Track 3: Fidelity, the aim is to obtain a network design / solution with the best fidelity (PSNR) while being constrained to maintain or improve over MSRResNet (Ledig et al, 2017 & Wang et al, 2018) in terms of number of parameters and inference time on a common GPU (ie. Titan Xp).|image transformation|||PSNR|Peak Signal To Noise Ratio (PSNR)|False|False|True|False||[1.0119221038104729]
AIM 2019 Video Extreme Super-Resolution Challenge - Track 2: Perceptual|Our objectives are to gauge the s-o-t-a in example-based video extreme super-resolution, to promote research on this topic and to introduce a novel benchmark.|49|57.0|2019|2.0|None|None|52.0|https://competitions.codalab.org/competitions/20249|||||multimedia|computer vision|Track 2: Perceptual, the aim is to obtain a network design / solution capable to produce high resolution results with the best perceptual quality and similar to the ground truth.|image transformation|||PSNR|Mean Opinion Score (MOS)|False|False|True|False||[1.0462083400418611]
AIM 2019 Video Extreme Super-Resolution Challenge - Track 1: Fidelity|Our objectives are to gauge the s-o-t-a in example-based video extreme super-resolution, to promote research on this topic and to introduce a novel benchmark.|61|78.0|2019|2.0|None|None|52.0|https://competitions.codalab.org/competitions/20248|||||multimedia|computer vision|Track 1: Fidelity, the aim is to obtain a network design / solution capable to produce high resolution results with the best fidelity (PSNR) to the ground truth.|image transformation|||PSNR|Peak Signal To Noise Ratio (PSNR)|False|False|True|False||[1.036858753954064]
AIM 2019 Video Temporal Super-Resolution Challenge|Our objectives are to gauge the s-o-t-a in example-based video temporal super-resolution (aka frame interpolation), to promote research on this topic and to introduce a novel benchmark.|101|131.0|2019|2.0|None|None|52.0|https://competitions.codalab.org/competitions/20244|||||multimedia|computer vision|super-resolving in the temporal domain (increasing the number of frames) an input video with a upsampling factor x4 based on a set of prior examples of high frame-rate videos|image transformation|||PSNR|Peak Signal To Noise Ratio (PSNR)|False|False|True|False||[1.052345158327948]
AIM 2019 Constrained Super-Resolution Challenge - Track 1: Parameters optimization|Our objectives are to gauge the s-o-t-a in example-based constrained single image super-resolution, to promote research on this topic and to introduce a novel benchmark.|92|109.0|2019|2.0|None|None|51.0|https://competitions.codalab.org/competitions/20167|||||multimedia|computer vision|Track 1: Parameters, the aim is to obtain a network design / solution with the lowest amount of parameters while being constrained to maintain or improve the PSNR result and the inference time (runtime) of MSRResNet (Ledig et al, 2017 & Wang et al, 2018).|image transformation|||PSNR|Peak Signal To Noise Ratio (PSNR)|False|False|True|False||[1.008651499839787]
AIM 2019 Extreme Super-Resolution Challenge - Track 2: Perceptual|Our objectives are to gauge the s-o-t-a in example-based single image extreme super-resolution, to promote research on this topic and to introduce a novel DIV8K benchmark.|83|43.0|2019|2.0|None|None|52.0|https://competitions.codalab.org/competitions/20236|||||multimedia|computer vision|Track 2: Perceptual, the aim is to obtain a network design / solution capable to produce high resolution results with the best perceptual quality and similar to the ground truth.|image transformation|||PSNR|Mean Opinion Score (MOS)|False|False|True|False||[1.0348518636508444]
VisDA2019: Multi-Source Domain Adaptation Challenge|The VisDA challenge aims to test domain adaptation methods’ ability to transfer source knowledge and adapt it to novel target domains.|64|270.0|2019|2.0|None|None|108.0|https://competitions.codalab.org/competitions/20256|||||multimedia|computer vision|test domain adaptation methods’ ability to transfer source knowledge and adapt it to novel target domains|multi-class classification|||mean_acc_all|mean accuracy per class, overall accuracy|True|False|True|True|https://github.com/VisionLearningGroup/visda-2019-public|[1.236972343522562]
VisDA2019: Semi-Supervised Domain Adaptation Challenge|The VisDA challenge aims to test domain adaptation methods’ ability to transfer source knowledge and adapt it to novel target domains.|61|579.0|2019|2.0|None|None|108.0|https://competitions.codalab.org/competitions/20257|http://ai.bu.edu/visda-2019/||||multimedia|computer vision|test domain adaptation methods’ ability to transfer source knowledge and adapt it to novel target domains|multi-class classification|||mean_acc_all|mean accuracy per class, overall accuracy|True|False|True|True||[1.2051915945611866]
VATEX Captioning Challenge 2019 - Multilingual Video Captioning|Benchmark for (multilingual) video captioning, aiming to automatically describe the activities in videos with various languages.|79|440.0|2019|4.0|None|None|56.0|https://competitions.codalab.org/competitions/20696|||||multimedia|computer vision, NLP|depict the important activities in a video clip, which requires high-quality, diverse captions that describe a wide variety of videos at scale|multi-class classification||41,250 videos and 825,000 captions in both English and Chinese|RANK|BLEU-4, ROUGE-L, METEOR, CIDEr|True|False|True|False||[1.773377811255784]
DSTC 8: End-to-End Multi-Domain Dialog Challenge Track - Task 1|DSTC 8: End-to-End Multi-Domain Dialog Challenge Track - Task 1|98|56.0|2019|2.0|None|None|112.0|https://competitions.codalab.org/competitions/20162|https://sites.google.com/dstc.community/dstc8/important-dates?authuser=0||||tourisme|NLP|build an end-to-end multi-domain dialog system for tourist information desk settings based on the recently released MultiWOZ dataset which we enrich with further annotation to support a wider range of learning approaches|dialogue generation||||success rate, average reward, number of turms, precision, recall, and F1|True|False|True|False||[0.0]
IROS 2019 Lifelong Robotic Vision Challenge: Lifelong SLAM|Lifelong SLAM Challenge|22|172.0|2019|1.0|None|None|71.0|https://competitions.codalab.org/competitions/21505|https://lifelong-robotic-vision.github.io/competition/SLAM.html||||robotics|computer vision, NLP|build a visual or visual-inertial SLAM (Simultaneous localization and mapping) system||||Total Score|correct rate (CR), Re-localization Score, total score|True|False|True|False||[1.7443930070407314]
DSTC 8: End-to-End Multi-Domain Track - Task 2 - Fast Adaptation|DSTC 8|62|34.0|2019|2.0|None|None|119.0|https://competitions.codalab.org/competitions/20152|||||tourisme|NLP|build an end-to-end multi-domain dialog system for tourist information desk settings based on the recently released MultiWOZ dataset which we enrich with further annotation to support a wider range of learning approaches|dialogue generation|||ContextScore|success rate, average reward, number of turms, precision, recall, and F1|True|False|True|False||[nan]
IROS 2019 Lifelong Robotic Vision Challenge: Lifelong SLAM Final Round|Lifelong SLAM Challenge - Final Round|19|83.0|2019|1.0|None|None|86.0|https://competitions.codalab.org/competitions/21504||||||NLP|predict or generate a user response to a dialogue from any domain|multi-class classification|||Total Score|RMSE, CR1&30, reloc|True|False|True|False|https://docs.google.com/spreadsheets/d/1KxbD6Dlx5M0J82_8ty4b9A7snhinAashZcLVJOhySbk/edit#gid=173123261|[1.6111398775263923]
2019 Untapped Energy reCLAIM Data Competition: Classification Challenge|Classification Challenge.|66|100.0|2019|1.0|None|None|18.0|https://competitions.codalab.org/competitions/21522|||||oil&gas|NLP|training a machine learning process to identify the status of a well, given a set of characteristics from its operating profile|multi-class classification|||F1|F1|False|False|True|False||[1.0785027264889315]
2019 Untapped Energy reCLAIM Data Competition: Regression Challenge|Regression Challenge.|62|130.0|2019|1.0|None|None|18.0|https://competitions.codalab.org/competitions/21523|||||oil&gas|NLP|predict the initial production of a given well|regression|||MAE|MAE|False|False|True|False||[0.0]
Health Data Challenge 2019|[!] The subscription is only open to people physically attending the challenge in Aussois [!]Successful treatment of cancer is still a challenge and this is partly due to a wide heterogeneity of cancer composition across patient population. Unfortunately, accounting for such heterogeneity is very difficult. Clinical evaluation of tumor heterogeneity often requires the expertise of anatomical pathologists and radiologists. This challenge is dedicated to the quantification of intra-tumor heterogeneity using appropriate statistical methods on cancer omics data. In particular, it focuses on estimating cell types and proportion in biological samples based on averaged DNA methylation and full patient history. The goal is to explore various statistical methods for source separation/deconvolution analysis (Non-negative Matrix Factorization, Surrogate Variable Analysis, Principal component Analysis, Latent Factor Models, ...).|59|766.0|2019|3.0|None|None|4.0|https://competitions.codalab.org/competitions/21949|||||medicine|NLP|quantification of intra-tumor heterogeneity using appropriate statistical methods on cancer omics data|regression|||MAE|MAE|False|False|True|True||[0.0]
RecoGym Challenge|RecoGym Competion|91|815.0|2019|2.0|3000|3000|65.0|https://competitions.codalab.org/competitions/21546|https://sites.google.com/view/recogymchallenge/home||||ecommerce|NLP|build a recommender system that optimises Click-Through Rate|regression|||CTR (q0.500), %|Click-Through Rate value|False|False|True|True|https://github.com/criteo-research/reco-gym|[1.6921620008821316]
DriveML Huawei Autonomous Vehicles Challenge|This is a competition to solve the multi-lane driving task using reinforcement learning.|250|1319.0|2020|3.0|6000|6000|46.0|https://competitions.codalab.org/competitions/21639|||||autonomous driving|RL|design an agent that is capable of driving safely and efficiently across a variety of simulated maps: train a driverless car from scratch to traverse a multi-lane highway as fast as possible whilst avoiding collisions with social vehicles|reinforcement learning|||<Rank>|total distance|False|False|True|True||[1.0]
ChaLearn LAP: Identity-preserved Human Detection (Track 3: Depth-Thermal fusion)|Given spatiotemporally aligned depth-thermal images of people and bounding-box annotations, the participants will develop their automatic methods to detect people on new unforseen images. The methods will need to output a list of bounding boxes per frame containing each person along with the detected boxes' associated confidence scores.|19|21.0|2019|2.0|None|None|77.0|https://competitions.codalab.org/competitions/21928|https://chalearnlap.cvc.uab.cat/challenge/34/description/||||multimedia|computer vision|human detection in depth and/or thermal images: output a list of bounding boxes (along with associated confidence scores) per frame containing each person in it|binary classification|||AP@0.50|AP|False|False|True|False||[1.1306716556162653]
ChaLearn LAP: Identity-preserved Human Detection (Track 2: Thermal)|Given thermal images of people and bounding-box annotations, the participants will develop their automatic methods to detect people on new unforseen images. The methods will need to output a list of bounding boxes per frame containing each person along with the detected boxes' associated confidence scores.|34|476.0|2019|2.0|None|None|77.0|https://competitions.codalab.org/competitions/21927|https://chalearnlap.cvc.uab.cat/challenge/34/description/||||multimedia|computer vision|human detection in depth and/or thermal images: output a list of bounding boxes (along with associated confidence scores) per frame containing each person in it|binary classification|||AP@0.50|AP|False|False|True|False||[1.4922518167090602]
ChaLearn LAP: Identity-preserved Human Detection (Track 1: Depth)|Given depth images of people and bounding-box annotations, the participants will develop their automatic methods to detect people on new unforseen images. The methods will need to output a list of bounding boxes per frame containing each person along with the detected boxes' associated detection confidence scores.|37|226.0|2019|2.0|None|None|77.0|https://competitions.codalab.org/competitions/21926|https://chalearnlap.cvc.uab.cat/challenge/34/description/||||multimedia|computer vision|human detection in depth and/or thermal images: output a list of bounding boxes (along with associated confidence scores) per frame containing each person in it|binary classification|||AP@0.50|AP|False|False|True|False||[1.525681148178482]
Fake Life Recognition Contest|SUBMISSIONS are now OPEN!Can a computer tell apart living from non-living things as well as we can? The objective of the Fake Life Recognition Contest aims to build a model that would.Think you hacked it? Tell us how you did it... and submit your code! Cash prize for the best algorithm: USD1000!|45|81.0|2019|1.0|1000|1000|188.0|https://competitions.codalab.org/competitions/20612||||||NLP|identifying living systems from non-living|binary classification||time series of trajectory positions in 2D|Score|accuracy|False|False|True|True||[1.0]
OSACT4 Shared Task on Offensive Language Detection (Subtask A)|OSACT4 Shared Task on Offensive Language Detection|62|110.0|2020|2.0|None|None|29.0|https://competitions.codalab.org/competitions/22825|||||linguistics, multimedia|NLP|offensive language detection|binary classification||10,000 tweets|F1-Score|macro-averaged F1|False|False|True|False||[1.0417323071210618]
OSACT4 Shared Task on Offensive Language Detection (Subtask B)|OSACT4 Shared Task on Offensive Language Detection (Subtask B for hate speech detection)|38|108.0|2020|2.0|None|None|29.0|https://competitions.codalab.org/competitions/22826|||||linguistics, multimedia|NLP|identification of offensive content and hate speech in Arabic language Twitter posts|binary classification||10,000 tweets|F1-Score|macro-averaged F1|False|False|True|False||[1.92404924089422]
GramEval-2020|Сompetition for the complete parsing of Russian texts.|26|661.0|2020|2.0|None|None|24.0|https://competitions.codalab.org/competitions/22902|||https://github.com/DanAnastasyev/GramEval2020||linguistics|NLP|build systems that implement complete morphological and syntactic markup with lemmatization within the framework of Universal Dependencies|binary classification|||Overall quality|POS tagging accuracy, Morphological features accuracy, LAS accuracy (syntax), Lemmatization accuracy.|True|False|True|False||[1.0445188103233016]
Chalearn Multi-modal Cross-ethnicity Face anti-spoofing Recognition Challenge@CVPR2020|Multi-modal Cross-ethnicity Face anti-spoofing Recognition Challenge|303|114.0|2019|2.0|None|None|79.0|https://competitions.codalab.org/competitions/22036|||https://arxiv.org/pdf/1912.02340.pdf||multimedia|computer vision|face anti-spoofing attack detection based on multi-modal data|NN attacks|||APCER|APCER, BPCER, ACER |True|False|True|False||[0.0]
Chalearn Single-modal (RGB) Cross-ethnicity Face anti-spoofing Recognition Challenge@CVPR2020|Single-modal (RGB) Cross-ethnicity Face anti-spoofing Recognition Challenge|329|391.0|2019|2.0|None|None|79.0|https://competitions.codalab.org/competitions/22151|||https://arxiv.org/pdf/1912.02340.pdf||multimedia|computer vision|face anti-spoofing attack detection based on multi-modal data, participates are permitted to use the single RGB modality|NN attacks|||APCER|APCER, BPCER, ACER |True|False|True|False||[0.0]
OffensEval 2020 - Greek|Automatically detecting abusive language in online content, for Greek|73|109.0|2020|1.0|None|None|15.0|https://competitions.codalab.org/competitions/23258|||||linguistics, multimedia|NLP|multilingual offensive language identification in social media|binary classification||||macro-averaged F1|False|False|True|False||[0.0]
OffensEval 2020 - Turkish|This is the CodaLab site for OffensEval 2020 Turkish data set|91|194.0|2020|1.0|None|None|14.0|https://competitions.codalab.org/competitions/23020|||||linguistics, multimedia|NLP|multilingual offensive language identification in social media|binary classification||||macro-averaged F2|False|False|True|False||[0.0]
OffensEval 2020 - Danish|Automatically detecting abusive language in online content, for Danish|84|298.0|2020|1.0|None|None|14.0|https://competitions.codalab.org/competitions/22998|||||linguistics, multimedia|NLP|multilingual offensive language identification in social media|binary classification||||macro-averaged F3|False|False|True|False||[0.0]
OffensEval 2020 - Arabic|Automatically detecting abusive language in online content, for Arabic.|118|171.0|2020|1.0|None|None|15.0|https://competitions.codalab.org/competitions/23021|||||linguistics, multimedia|NLP|multilingual offensive language identification in social media|binary classification||||macro-averaged F4|False|False|True|False||[0.0]
CVPM2020 challenge: The 1st Challenge on Remote Physiological Signal Sensing (RePSS)|This is the first challenge on remote physiological signal measurement, which will be focusing on average heart rate measurement from a short-time color facial videos.|150|117.0|2019|1.0|None|None|380.0|https://competitions.codalab.org/competitions/22287|||||medicine|signal processing, computer vision|measuring the average heart rate from color facial videos|regression|||MAE|MAE, RMSE, Pearson correlation coefficient|True|False|True|False||[0.0]
RuREBus_v2|RuREBus shared task|17|218.0|2020|2.0|None|None|15.0|https://competitions.codalab.org/competitions/23407|||||linguistics|NLP|Named entity recognition, Relation extraction, End-to-end relation extraction|relation extraction|||NER|micro F measure|False|True|True|False|https://github.com/dialogue-evaluation/RuREBus|[1.0]
Multimodal (Audio, Facial and Gesture) based Emotion Recognition challenge|Emotion recognition has a key role in affective computing. People express emotions through different modalities. Integration of verbal and non-verbal communication channels creates a system in which the message is easier to understand. Expanding the focus to several expression forms can facilitate research on emotion recognition as well as human-machine interaction. We plan to organise a competition around two important problems which are: (1) recognition of compound emotions, that require, in addition to performing an effective visual analysis, to deal with recognition of micro emotions. The database includes 31250 facial faces with different emotions of 115 subjects whose gender distribution is almost uniform. (2) recognition of multi-modal emotions composed of three modalities, namely, facial expressions, body movement and gestures, and speech. The corpora contains recordings registered in studio conditions, acted out by 16 professional actors (8 male and 8 female).|83|13.0|2019|3.0|None|None|123.0|https://competitions.codalab.org/competitions/20224|||||multimedia|computer vision|emotion recognition|multi-class classification||||accuracy|False|True|True|True||[0.0]
Emotion challenge|The main challenge of this new dataset is automatic recognition of these multi-emotions from facial expressions and handling analysis of micro emotions. For this challenge the data will be divided into 3 sets - 70 people for initial training, 30 for validation round and 25 for final tests.|224|862.0|2019|2.0|None|None|126.0|https://competitions.codalab.org/competitions/15971|||||multimedia|computer vision|emotion recognition|multi-class classification|||Misclassification |error rate|False|False|True|False|https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8352857&tag=2|[0.0]
Sentimix Spanglish|Sentiment analysis of code-mixed tweets. SemEval 2020 task 9. Spanish sub task.|183|284.0|2019|1.0|None|None|190.0|https://competitions.codalab.org/competitions/20789|||||linguistics|NLP|task of sentiment analysis in code-mixed social media text: predict the sentiment of a given code-mixed tweet|multi-class classification|||Misclassification |average F1|False|True|True|False||[1.1995960240259398]
NTIRE 2020 Image Deblurring Challenge - Track 1 on Desktop|Our objectives are to gauge the s-o-t-a in example-based image deblurring, to promote research on this topic and to introduce a novel benchmark.|213|53.0|2019|2.0|None|None|90.0|https://competitions.codalab.org/competitions/22233|||||multimedia|computer vision|Image Deblurring: task of restoring the contents in a blurry input image based on a set of prior examples of clean and blurry images|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[0.0]
NTIRE 2020 Spectral Reconstruction Challenge - Track 2: Real World|To gauge the current state-of-the-art in spectral reconstruction from RGB images, to compare and to promote different solutions we are organizing an NTIRE challenge in conjunction with the CVPR 2020 conference. The largest dataset to date will be introduced with the challenge.|241|112.0|2019|2.0|None|None|97.0|https://competitions.codalab.org/competitions/22226|||||multimedia|computer vision|spectral reconstruction from RGB images|image transformation|||MRAE|MRAE, RMSE, Back projection, Weighted Accuracy|True|False|True|False||[0.0]
NTIRE 2020 Video Deblurring Challenge - Track 3 Video|Our objectives are to gauge the s-o-t-a in example-based video deblurring, to promote research on this topic and to introduce a novel benchmark.|141|32.0|2019|2.0|None|None|90.0|https://competitions.codalab.org/competitions/22235|||||multimedia|computer vision|Video Deblurring|video transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[0.0]
NTIRE 2020 NonHomogeneous Dehazing Challenge|Our objectives are to gauge the s-o-t-a in example-based single image dehazing, to promote research on this topic and to introduce a novel benchmark.|300|1070.0|2020|2.0|None|None|85.0|https://competitions.codalab.org/competitions/22236|||||multimedia|computer vision|NonHomogeneous Dehazing: restoring the clean contents from a hazy input image based on a set of prior examples of hazy and hazy-free images|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[-16.0]
NTIRE 2020 Real Image Denoising Challenge - Track 1 - rawRGB|Our objectives are to gauge the s-o-t-a in image denoising, to promote research on this topic and to introduce a novel benchmark.|367|547.0|2019|2.0|None|None|97.0|https://competitions.codalab.org/competitions/22230|||||multimedia|computer vision|Real Image Denoising: restoring the clean contents in an input image|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[6.714285714285714]
NTIRE 2020 Demoireing Challenge - Track 1: Single image|Based on a novel dataset we propose a image demoireing challenge to promote research on this topic.|281|1142.0|2020|2.0|None|None|84.0|https://competitions.codalab.org/competitions/22223|||||multimedia|computer vision|Burst Demoireing: removing the moire effect from an input image based on a set of examples of images with and without the moire effect|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[-2.5999999999999996]
NTIRE 2020 Perceptual Extreme Super-Resolution Challenge|Our objectives are to gauge the s-o-t-a in example-based single image extreme super-resolution, to promote research on this topic and to introduce a novel DIV8K benchmark.|566|502.0|2019|2.0|None|None|97.0|https://competitions.codalab.org/competitions/22217|||||multimedia|computer vision|super-resolving (increasing the resolution) an input image with a magnification factor x16 based on a set of prior examples of low and corresponding high resolution images|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[8.5]
NTIRE 2020 Video Quality Mapping - Track 1: Supervised|Our objectives are to gauge the s-o-t-a in example-based video quality mapping, to promote research on this topic and to introduce a novel benchmark.|207|124.0|2019|2.0|None|None|107.0|https://competitions.codalab.org/competitions/20247|||||multimedia|computer vision|examble-based Video Quality Mapping, that is, the task of mapping in quality an input video (from a known source domain) to a target video domain based on sets of prior examples of videos|image transformation|||MOS|MOS|False|False|True|False||[1.07433729095932]
NTIRE 2020 Real World Super-Resolution Challenge - Track 1: Image Processing artifacts|Learn why most super-resolution methods fail on images from real cameras and beat SOTA.|818|252.0|2019|2.0|None|None|94.0|https://competitions.codalab.org/competitions/22220|||||multimedia|computer vision|super-resolve images from the Source Domain to the Target Domain|image transformation|||MOS|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|False|False|True|False||[1.0978588319134464]
OffensEval 2020 - English - Evaluation Phase|This is the competition site for the TEST set of the second edition of OffensEval organized at SemEval 2020 (Task 12). SemEval 2020 will take place on September 13 and 13, 2020 co-located with COLING in Barcelona, Spain.|241|559.0|2020|3.0|None|None|32.0|https://competitions.codalab.org/competitions/23285|||||linguistics, multimedia|NLP|Multilingual Offensive Language Identification in Social Media|binary classification||||macro-averaged F4|False|False|True|False||[0.0]
AutoML 2018 challenge :: PAKDD2018|AutoML 2018 Challenge.|343|1258.0|2017|2.0|3000|3000|852.0|https://competitions.codalab.org/competitions/17767|https://automl.chalearn.org/|https://hal.archives-ouvertes.fr/hal-01906197/document||||AutoML|provide code for solving real world classification problems without any human intervention|binary classification|||<Rank>|AUC|False|False|True|True||[0.0]
DeepFashion2 Challenge 2020 - Track 2 Clothes Retrieval|In this challenge, we provide a more realistic setting. Instead of being provided the ground truth query clothing item, participants should detect clothing items in images from consumers. For each detected clothing item, participants need to submit the top-10 retrieved clothing items detected from shop images. Top-10 retrieval accuracy is employed as the evaluation metric. We emphasize the retrieval performance while still consider the influence of detector. If a clothing item fails to be detected, this query item is counted as missed.|115|321.0|2020|2.0|None|None|66.0|https://competitions.codalab.org/competitions/22967|||||fashion|computer vision|Given a detected item from a consumer-taken photo, this task aims to search the commercial images in the gallery for the items that are corresponding to this detected item|multi-class classification|||Top-10|accuracy|False|False|True|False||[1.5342519032895077]
DeepFashion2 Challenge 2020 - Track 1 Clothes Landmark Estimation|This task aims to predict landmarks for each detected clothing item in an image. In this task, there are 192K images for training, 32K images for validation and 63K images for test. We adopt the same evaluation metric employed in the cocodataset. Different from coco dataset, where only one category has key-points, a total of 294 landmarks of 13 categories in DeepFashion2 are presented. Besides the coordinates of 294 landmarks of a detected clothing item, its category should also be included in final prediction files.|97|159.0|2020|2.0|None|None|66.0|https://competitions.codalab.org/competitions/22966|||||fashion|computer vision|predict landmarks for each detected clothing item in an image|multi-class classification|||AP|AP|False|False|True|False||[1.6559573176452849]
2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE)|A competition to predict all possible translations of a given input prompt, with scoring based on frequencies derived from real learner data, and 5 language tracks.|50|357.0|2020|2.0|None|None|43.0|https://competitions.codalab.org/competitions/23643|http://sharedtask.duolingo.com/#introduction||||linguistics|NLP|Simultaneous Translation And Paraphrase for Language Education (STAPLE)|multi-class classification|||Weighted F1|macro-weighted F1|False|False|True|False||[0.0]
The Shared Task on Sarcasm Detection|Shared Task in Workshop on Figurative Language Processing|216|1983.0|2020|2.0|None|None|88.0|https://competitions.codalab.org/competitions/22247|||||linguistics|NLP|sarcasm detection|binary classification|||f-measure|F1|False|True|True|False||[1.266755050064543]
The Second Shared Task on Metaphor Detection|Shared Task in Workshop on Figurative Language Processing|134|1102.0|2020|4.0|None|None|96.0|https://competitions.codalab.org/competitions/22188|||||linguistics|NLP|detect, at word level, all content-word metaphors in a given text|binary classification|||f-measure|F1|False|True|True|False||[1.1445493837041778]
Interspeech Shared Task: Automatic Speech Recognition for Non-Native Children’s Speech|A joint shared task between ETS and FBK.|32|74.0|2020|2.0|None|None|11.0|https://competitions.codalab.org/competitions/23672|||||linguistics|NLP|non-native children's speech recognition|binary classification|||wer|Word Error Rate (WER)|False|False|True|False||[0.0]
CLVision challenge|Continual Learning for Computer Vision Challenge.|93|340.0|2019|3.0|2300|2300|443.0|https://competitions.codalab.org/competitions/23317||||||computer vision|creating autonomous agents which can learn continuously and acquire new complex skills and knowledge|Continuous Learning|||<Rank>|accuracy,time,disk,memory|True|True|True|False||[0.0]
eHealth KD @ IberLEF 2020|Automated knowledge extraction from electronic health documents.|29|35.0|2020|2.0|None|None|71.0|https://competitions.codalab.org/competitions/23454|https://knowledge-learning.github.io/ehealthkd-2020/||||medicine|NLP|modeling the human language in a scenario in which Spanish electronic health documents could be machine readable from a semantic point of view: Entity recognition, Relation extraction|multi-class classification|||F1|F1, precision, recall|True|True|True|False||[0.0]
COMP90042-2020 Project 1: Climate Change Misinformation Detection|Class project of COMP90042 Natural Language Processing|319|6442.0|2020|2.0|None|None|40.0|https://competitions.codalab.org/competitions/24205|||||climate|NLP|detect whether a document contains climate change misinformation|binary classification|||F1_Score|F1|False|False|True|False||[1.535243553008596]
The Semi-Supervised DAVIS Challenge on Video Object Segmentation @ CVPR 2020|Submission site for the 2020 DAVIS Challenge on Video Object Segmentation for the semi-supervised track.|345|4849.0|2019|2.0|None|None|396.0|https://competitions.codalab.org/competitions/20516|https://davischallenge.org/challenge2020/semisupervised.html|||||computer vision|video object segmentation|video recognition|||Global Mean|Region Similarity (J) and Contour Accuracy (F)|True|False|True|False||[1.1117654128109602]
CAPITEL-EVAL 2020 - NERC|The CAPITEL corpus is a collection of news articles in Spanish. This website is for the subtask on Named Entity Recognition and Classification (NERC)|30|18.0|2020|1.0|None|None|7.0|https://competitions.codalab.org/competitions/23011|||||news|NLP|identifying and classifying NEs in Spanish news articles|multi-class classification|||F1|F1, precision, recall|True|False|True|False||[0.0]
CAPITEL-EVAL 2020 - UD|The CAPITEL corpus is a collection of news articles in Spanish. This website is for the subtask on Universal Dependency Parsing (UD).|17|13.0|2020|1.0|None|None|7.0|https://competitions.codalab.org/competitions/23178|||||news|NLP|Universal Dependency parsing of Spanish news articles||||LAS|Unlabeled Attachment Score (UAS), Labeled Attachment Score (LAS)|True|False|True|False||[0.0]
Fashion IQ Challenge 2020|Fashion Image Retrieval Challenge.We offer cash prizes for top winners.|61|231.0|2020|2.0|None|None|103.0|https://competitions.codalab.org/competitions/22946|https://sites.google.com/view/cvcreative2020/fashion-iq?authuser=0#h.p_f0ZXPhQ1JOPz||||fashion|computer vision, NLP|retrieving images by natural language based feedback|conversational image search retrieval systems|||AVG|recall|False|False|True|False||[1.4952076677316295]
EPIC-KITCHENS-55 Action Recognition|Classify trimmed action segments from the EPIC-KITCHENS-55 dataset.|153|557.0|2020|1.0|None|None|335.0|https://competitions.codalab.org/competitions/20115||||||computer vision, NLP|Classify trimmed action segments from seen and unseen kitchens by action verb and noun|multi-class classification|||Top-1 Accuracy (%)|accuracy, precision, recall and F1|True|False|True|False||[0.0]
EPIC-Kitchens-55 Action Anticipation|Anticipate a future action in an action sequence from EPIC-Kitchens-55.|125|535.0|2020|1.0|None|None|335.0|https://competitions.codalab.org/competitions/20071||||||computer vision|anticipation of a future action from the observation of a preceding video segment|multi-class classification|||Top-1 Accuracy (%)|accuracy, precision, recall and F2|True|False|True|False||[0.0]
OpenKBP - 2020 AAPM Grand Challenge|Develop a method to predict patient-specific guidelines for radiation therapy using real patient images.|300|2370.0|2020|2.0|None|None|100.0|https://competitions.codalab.org/competitions/23428|||||medicine|computer vision|dose prediction methods for knowledge-based planning (KBP): clinical quality dose distribution for a patient given only a contoured CT image|regression||340 contoured CT images and their corresponding dose distributions|Dose score|MAE|False|True|True|False||[0.0]
YouMakeup VQA Challenge|A competition held by CVPR LVVU Workshop 2020|17|42.0|2020|4.0|None|None|56.0|https://competitions.codalab.org/competitions/24108|https://languageandvision.github.io/youmakeup_vqa/index.html|||||computer vision, NLP|VQA: Facial Image Ordering Sub-Challenge and Step Ordering Sub-Challenge.|multi-class classification||2,800 videos from YouTube, spanning more than 420 hours in total|Accuracy|accuracy|False|True|True|False||[1.109185050801408]
HACS Temporal Action Localization Challenge - Supervised Learning Track|A CVPR'20 video recognition challenge.|30|53.0|2020|1.0|None|None|51.0|https://competitions.codalab.org/competitions/24433||||||computer vision|temporally localize actions in untrimmed videos|multi-class classification||200 action classes, nearly 140K action segments annotated in nearly 50K videos|Mean Average Precision|mAP|False|False|True|True||[1.4383269961977185]
NightOwls Detetection Challenge 2020|NightOwls Detection competition as part of Scalability in Autonomous Driving, CVPR 2020|45|55.0|2020|2.0|None|None|160.0|https://competitions.codalab.org/competitions/24781|||||autonomous driving|computer vision|Pedestrian detection at night|autonomous driving|||Average Miss Rate (Reasonable)|Average Miss Rate|False|True|True|True||[0.0]
BDD100K Multiple Object Tracking Challenge|<p>This challenge evaluates algorithm for object detection with bounding box output on BDD100K dataset.</p>|48|160.0|2020|2.0|None|None|164.0|https://competitions.codalab.org/competitions/24910|||||autonomous driving|computer vision|multiple object tracking: emporal association of objects within videos|autonomous driving|||mMOTA|mean Multiple Object Tracking Accuracy (mMOTA, mean of MOTA of each category)|False|False|True|False||[0.0]
Real-time distortion classification in laparoscopic videos|ICIP2020 Challenge for Distortion classification in laparoscopic videos in real-time|35|287.0|2020|2.0|None|None|15.0|https://competitions.codalab.org/competitions/25104|||||medicine|computer vision|automated video enhancement systems: real-time classification of distortions within a laparoscopic video|multi-class classification|||F1-score (Single + Multi distortions)|speed, accuracy, F1|True|False|True|False||[1.1237150263698936]
FACT@IberLEF2020|NLP competition - FACT|29|35.0|2020|1.0|None|None|91.0|https://competitions.codalab.org/competitions/23717|||||linguistics|NLP|classify events in Spanish texts, according to their factuality status|multi-class classification|||Macro-F|Precision, Recall, F1 score for each category, Macro-F1, Global accuracy.|True|True|True|False||[1.2910969310240046]
Fincausal 2020|This is a competition for FinCausal-2020 Shared Task - Financial Document Causality Detection.|76|628.0|2020|6.0|None|None|150.0|https://competitions.codalab.org/competitions/23748|||||finance|NLP|experiment causality detection, and focuses on determining causality associated to an event|binary classification, relation extraction|||F1|Precision, Recall, F1|True|True|True|False||[1.1480525824407426]
Learning to Run a Power Network|Train controlers to conduct a power grid for as long as possible while avoiding incidents.|74|40.0|2019|2.0|None|None|412.0|https://competitions.codalab.org/competitions/20767|||||electricity|RL|predict flows in the French long distance high voltage electricity transmission grid|reinforcement learning|||score|score|False|False|True|True||[0.0]
ECCV 2020 ChaLearn LAP Fair Face Recognition challenge|This Challenge focuses on bias analysis and mitigation methodologies, which will result in more fair face recognition and analysis systems.|193|1806.0|2020|2.0|2000|2000|85.0|https://competitions.codalab.org/competitions/24184|https://chalearnlap.cvc.uab.cat/challenge/38/description/||||multimedia|computer vision|develop their fair face verification method aiming for a reduced bias in terms of gender and skin color (protected attributes)|binary classification|||Ranking|bias (positive,negative pairs), accuracy|True|False|True|False||[0.0]
2020 VIPriors Image Classification Challenge|Saving data by adding visual knowledge priors to Deep Learning.|72|51.0|2020|2.0|None|None|130.0|https://competitions.codalab.org/competitions/23713|||||multimedia|computer vision|image classification|multi-class classification|||Top1 accuracy|accuracy|False|False|True|False||[1.3006681514476615]
2020 VIPriors Action Recognition Challenge|Saving data by adding visual knowledge priors to Deep Learning.|45|97.0|2020|2.0|None|None|130.0|https://competitions.codalab.org/competitions/23706|||||multimedia|computer vision|action recognition|multi-class classification|||Top1 accuracy|accuracy|False|False|True|False||[0.0]
2020 VIPriors Object Detection Challenge|Saving data by adding visual knowledge priors to Deep Learning.|81|102.0|2020|2.0|None|None|130.0|https://competitions.codalab.org/competitions/23661|||||multimedia|computer vision|object detection|multi-class classification|||AP @ 0.50:0.95|accuracy|False|False|True|False||[1.3340857787810385]
2020 VIPriors Semantic Segmentation Challenge|Saving data by adding visual knowledge priors to Deep Learning.|70|140.0|2020|2.0|None|None|130.0|https://competitions.codalab.org/competitions/23712|||||multimedia|computer vision|semantic segmentation|multi-class classification|||IoU Class|mean Intersection-over-Union (mIoU)|False|False|True|False||[1.2538860103626943]
AIM 2020 Efficient Super-Resolution Challenge|Our objectives are to gauge the s-o-t-a in example-based efficient single image super-resolution, to promote research on this topic and to introduce a novel benchmark.|202|370.0|2020|2.0|None|None|69.0|https://competitions.codalab.org/competitions/24684|||||multimedia|computer vision|super-resolving (increasing the resolution) an input image with a magnification factor x4 based on a set of prior examples of low and corresponding high resolution images|image reconstruction|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[2.0]
AIM 2020 Image Extreme Inpainting Challenge Track 2 Semantic Guidance|Our objectives are to gauge the s-o-t-a in image inpainting, to promote research on this topic and to introduce a novel benchmark.|90|62.0|2020|2.0|None|None|69.0|https://competitions.codalab.org/competitions/24676|||||multimedia|computer vision|obtain a network design / solution capable of producing high quality results with the best perceptual quality and similarity to the reference ground truth|image reconstruction|||MOS|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[-0.0]
AIM 2020 Video Extreme Super-Resolution Challenge - Track 1: Fidelity|Our objectives are to gauge the s-o-t-a in example-based video extreme super-resolution, to promote research on this topic and to introduce a novel benchmark.|93|156.0|2020|2.0|None|None|69.0|https://competitions.codalab.org/competitions/24685|||||multimedia|computer vision|super-resolving (increasing the resolution) an input image with a magnification factor x16 based on a set of prior examples of low and corresponding high resolution images|video transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[6.5]
AIM 2020 Real Image Super-Resolution Challenge - Track 1 Upscaling x2|Our objectives are to gauge the s-o-t-a in real image super-resolution for which training pairs of low and high resolution images are available, to promote research on this topic and to introduce a novel benchmark.|211|500.0|2020|2.0|None|None|69.0|https://competitions.codalab.org/competitions/24680|||||multimedia|computer vision|obtain a network design / solution capable to produce high quality results with the best fidelity to the reference ground truth|image reconstruction|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[14.666666666666668]
AIM 2020 Image Extreme Inpainting Challenge Track 1 Classic Inpainting|Our objectives are to gauge the s-o-t-a in image inpainting, to promote research on this topic and to introduce a novel benchmark.|123|174.0|2020|2.0|None|None|69.0|https://competitions.codalab.org/competitions/24675|||||multimedia|computer vision|obtain a network design / solution capable of producing high quality results with the best perceptual quality and similarity to the reference ground truth|image reconstruction|||MOS|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[19.0]
AIM 2020 Relighting Challenge Track 1: One-to-one relighting|Our objectives are to gauge the s-o-t-a in image relighting, or illumination style transfer, to promote research on this topic and to introduce a novel benchmark.|116|825.0|2020|2.0|None|None|69.0|https://competitions.codalab.org/competitions/24671|||||multimedia|computer vision|obtain a network design / solution capable of producing high-quality results with the best perceptual quality and similar to the reference ground truth|image reconstruction|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[-8.333333333333334]
AIM 2020 Video Temporal Super-Resolution Challenge|Our objectives are to gauge the state-of-the-art example-based video temporal super-resolution (aka frame interpolation) to promote research on this topic and to introduce a novel benchmark.|132|39.0|2020|2.0|None|None|77.0|https://competitions.codalab.org/competitions/24584|||||multimedia|computer vision|super-resolving in the temporal domain (increasing the number of frames) an input video with an upsampling factor x4 based on a set of prior examples of high frame-rate videos|video transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[nan]
AIM 2020 Learned Smartphone ISP Challenge - Track 1: Fidelity|The target of the learned smartphone ISP (aka RAW to RGB mapping) challenge is to promote research on this topic and to introduce a novel dataset with pairs of RAW (phone camera) and RGB (DSLR camera) corresponding images.|156|400.0|2020|2.0|None|None|69.0|https://competitions.codalab.org/competitions/24718|||||multimedia|computer vision|obtain an output image with the highest pixel fidelity to the ground truth as measured by PSNR / MS-SSIM scores|image reconstruction|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[27.0]
AIM 2020 Rendering Realistic Bokeh Challenge - Track 1: on CPU|To gauge the state-of-the-art and promote the research in synthesized Bokeh effect we are organizing this challenge using a novel dataset with pairs of Bokeh-free and optical Bokeh (DSLR camera) corresponding images.|293|235.0|2020|2.0|None|None|72.0|https://competitions.codalab.org/competitions/24716|||||multimedia|computer vision|achieve a Bokeh effect image with the best perceptual quality similar to the ground truth as measured by the Mean Opinion Score (MOS)|image reconstruction|||MOS|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[-24.0]
ECCV Challenge on Image Restoration for Under-display Camera - Track 2 - P-OLED|This prize challenge aims at promoting the brand new image restoration problem caused by Under-Display Camera (UDC). It is a new trend of design for full-screen devices and is being explored by many high-tech companies. We provide a sample dataset for training and testing collected from multiple different display types and seek to find the most efficient and high-performed algorithms for UDC restoration.|79|247.0|2020|2.0|None|None|104.0|https://competitions.codalab.org/competitions/24293|||||multimedia|computer vision|obtain restored RGB images with the highest PSNR from low-quality T-OLED UDC images|image reconstruction|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[1.1714888487040382]
ECCV Challenge on Image Restoration for Under-display Camera - Track 1 - T-OLED|This prize challenge aims at promoting the brand new image restoration problem caused by Under-Display Camera (UDC). It is a new trend of design for full-screen devices and is being explored by many high-tech companies. We provide a sample dataset for training and testing collected from multiple different display types and seek to find the most efficient and high-performed algorithms for UDC restoration.|93|228.0|2020|2.0|None|None|106.0|https://competitions.codalab.org/competitions/24275|||||multimedia|computer vision|obtain restored RGB images with the highest PSNR from low-quality P-OLED UDC images|image reconstruction|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[1.1397597421623205]
VisDA 2020: Domain Adaptive Pedestrian Re-identification|The 4th Visual Domain Adaptation Challenge|170|926.0|2020|2.0|None|None|85.0|https://competitions.codalab.org/competitions/24664|||||urbanisme|computer vision|test domain adaptation methods’ ability to transfer source knowledge and adapt it to novel target domains: Domain Adaptive Pedestrian Re-identification||||mAP|mAP, CMC|True|False|True|False||[1.5443477960122436]
1st Tiny Object Detection Challenge|1st Tiny Object Detection Challenge for ECCV2020 Workshop|210|3753.0|2020|1.0|None|None|96.0|https://competitions.codalab.org/competitions/24551|||||multimedia|computer vision|object detection|binary classification||610 images with 72651 box-level annotations|AP50_tiny|AP, MR|True|False|True|False||[1.2212576156238129]
Holistic 3D Vision Challenge - Structured3D General Room Layout Estimation Track @ ECCV 2020|Learning to reconstruct the indoor scene with Structured3D dataset|73|196.0|2020|2.0|None|None|67.0|https://competitions.codalab.org/competitions/24183||||||computer vision|reconstruct the enclosing structure of the indoor scene, consisting of walls, floor, and ceiling|image reconstruction|||Mean||||True|False||[1.1669361447651112]
ACRV Probabilistic Object Detection (PrOD) Challenge ECCV 2020|An ACRV Robotic Vision Challenge focused on probabilistic object detection. Competitors must provide estimates of spatial and semantic uncertainty for all objects detected.|41|21.0|2019|1.0|2500|2500|339.0|https://competitions.codalab.org/competitions/20597|||||robotics|computer vision|object detection: detect objects in video data, provide accurate estimates of spatial and semantic uncertainty|object recognition|||PDQ Score|robability-based detection quality|False|False|True|False||[1.2151381709107154]
COCO Keypoint Challenge|This challenge evaluates algorithm for keypoint detection on the COCO dataset.|976|9944.0|2019|3.0|None|None|363.0|https://competitions.codalab.org/competitions/12061||||||computer vision|simultaneously detecting people and localizing their keypoint|image recognition|||AP|AP, AR|True|False|True|False||[1.1681983762291375]
COCO Panoptic Segmentation Challenge|This challenge is designed to push the state of the art in scene segmentation for both things and stuff classes.|207|1252.0|2018|4.0|None|None|770.0|https://competitions.codalab.org/competitions/19507||||||computer vision|stuff and thing classes, unifying the typically distinct semantic and instance segmentation tasks|image recognition|||PQ|PQ, SQ|True|False|True|False||[1.352903451492537]
COCO 2020 DensePose Challenge|This challenge is designed to push the state of the art in dense human pose estimation.|64|64.0|2019|2.0|None|None|373.0|https://competitions.codalab.org/competitions/20660|||||multimedia|computer vision|human body detection, human body segmentation, mapping all image pixels that belong to a human body to the 3D surface of the body.|image recognition|||AP|AP|False|False|True|False||[1.2106824925816022]
RVC Challenge: COCO Detection (Bounding Box)|This challenge evaluates algorithm for object detection with bounding box output on COCO dataset.|49|44.0|2019|3.0|None|None|370.0|https://competitions.codalab.org/competitions/25334|||||multimedia|computer vision|object detection using bounding box|object detection|||AP|AP|False|False|True|False||[1.2519685039370079]
WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets||123|236.0|2020|2.0|None|None|19.0|https://competitions.codalab.org/competitions/25845|||||multimedia, tweets, covid|NLP|identification of informative COVID-19 English Tweets: develop systems that automatically identify whether an English Tweet related to the novel coronavirus (COVID-19) is informative or not|binary classification|||F1-score|accuracy, precision, recall, F1|True|False|True|False||[1.081721569348195]
COVID-19 Retweet Prediction Challenge|CIKM 2020 AnalytiCup - COVID-19 Retweet Prediction Challenge|240|2113.0|2020|2.0|2810|2810|78.0|https://competitions.codalab.org/competitions/25276|||||multimedia, tweets, covid|NLP|predict the number of times it will be retweeted (#retweets)|regression|||Prediction score (MSLE)|MSLE|False|False|True|False||[0.0]
Mouse Behavior Challenge MBC2020|Normal or Parkinson's disease ? Classifying mice from their trajectories|43|196.0|2019|2.0|None|None|274.0|https://competitions.codalab.org/competitions/22266|||||medicine|NLP|classify mice trajectories into normal or PD (parkinson), provided training mice trajectories|binary classification|||recognition rate|accuracy|False|False|True|False||[1.2870607974733517]
Basketball Behavior Challenge BBC2020|With or without teamwork? Classifying screen-play in basketball from players’ trajectories|18|87.0|2019|2.0|None|None|274.0|https://competitions.codalab.org/competitions/23905|||||sports|NLP|classify screen-play in basketball from players ’ trajectories: identify if the attackers are using the screen-play in their offence from trajectories of the three players and the ball|binary classification|||recognition rate|accuracy|False|False|True|False||[1.0496086694762192]
HASOC 2020 (FIRE 2020)|Hate Speech and Offensive Content Identification in Indo-European Languages|175|312.0|2020|6.0|None|None|39.0|https://competitions.codalab.org/competitions/26027|||||linguistics|NLP|Hate Speech and Offensive Content Identification in Indo-European Languages: 1) Identifying Hate, offensive and profane content; 2) iscrimination between Hate, profane and offensive posts|binary classification, multi-class classification|||Subtask A F1 Macro average|macro-average F1|False|True|True|False||[nan]
The 4th Look Into Person (LIP) Challenge - Track 4 Video Virtual Try-on Challenge|The video virtual try-on track of the 4th LIP Challenge|18|16.0|2020|1.0|None|None|223.0|https://competitions.codalab.org/competitions/23472|||||multimedia|computer vision|given a clothing image, a target person image, and a pose sequence, the participators are asked to design algorithms to transfer the desired clothing onto a person to produce a high-quality video sequence while preserving the identity information of the person, clothing texture details and the temporal coherence of the synthesized video|video transformation||791 videos of fashion model catwalk|MeanSSIM|SSIM (Structural SIMilarity Index), AMT(Amazon Mechanical Turk)|True|False|True|False||[1.0052277819268112]
Fake News classicifation|A competition on Fake News classification on Russian MediaIMPORTANTDear participants, please send your Colab notebooks with the classification task results for organizers to check using the emails: rybolos@gmail.com and  m_tikhonova94@mail.ru We are also happy to invite you to participate at RuCode track seminar this Saturday, 03.10.2020|90|675.0|2020|1.0|None|None|23.0|https://competitions.codalab.org/competitions/26284|||||news|NLP|fake news classification|binary classification|||correct|F1|False|False|True|False||[1.0920154017852268]
NeurIPS 2021 BEETL Competition - Benchmarks for EEG Transfer Learning|The Benchmarks for EEG Transfer Learning (BEETL) is a competition that aims to stimulate the development of transfer and meta-learning algorithms applied to a prime example of what makes the use of biosignal data hard, EEG data. BEETL acts as a much-needed benchmark for domain adaptation algorithms in EEG decoding and provides a real-world stimulus goal for transfer learning and meta-learning developments for both academia and industry. Given the multitude of different EEG-based algorithms that exist, we offer two specific challenges - Task 1 is a cross-subject sleep stage decoding challenge reflecting the need for transfer learning in clinical diagnostics, and Task 2 is a cross-dataset motor imagery decoding challenge reflecting the need for transfer learning in human interfacing.|131|1382.0|2021|4.0|None|None|23.0|https://competitions.codalab.org/competitions/33427|https://beetl.ai/introduction||||medicine|signal processing, computer vision|Task 1 is centred on transfer learning in the field of medical diagnostics, addressing automatic sleep stage annotation. The challenge lies in transferring from a large control population data set to clinically relevant cohorts with very little training data (transfer across subjects). Task 2 is centred on transfer learning for Brain-Computer Interfaces (BCI), addressing motor imagery decoding|transfer learning|||correct|accuracy|False|False|True|False||[1.531544701464729]
Exoplanet imaging data challenge|Community-wide data challenge for the comparison of exoplanet (direct) detection algorithms|68|31.0|2019|2.0|None|None|388.0|https://competitions.codalab.org/competitions/20693|https://exoplanet-imaging-challenge.github.io/||||astrophysics|computer vision|detecting exoplanets with ground-based high-contrast imaging|image recognition|||F1-score|F1, precision, TPR, FPC|True|False|True|False||[0.0]
MAFAT Radar Challenge - Can you distinguish between humans and animals in radar tracks?|In this competition, MAFAT’s DDR&D (Directorate of Defense Research & Development) would like to tackle the challenge of classifying living, non-rigid objects detected by doppler-pulse radar systems.|1218|4426.0|2020|2.0|40000|40000|92.0|https://competitions.codalab.org/competitions/25389||||||computer vision|classifying living, non-rigid objects detected by doppler-pulse radar systems|binary classification||6,656 radar segments, labeled as either animals or humans, in addition to a supportive (auxiliary) data set containing 49,071 segments|ROC AUC|ROC AUC|False|False|True|False||[1.160563044256233]
Diagnostic Questions - The NeurIPS 2020 Education Challenge||488|3696.0|2020|8.0|5000|5000|100.0|https://competitions.codalab.org/competitions/25449|https://onedrive.live.com/?authkey=%21AAoqjjcEN9EjMes&cid=9836C680BC708C16&id=9836C680BC708C16%211190&parId=root&o=OneUp||||education|NLP|1) accurately predicting which answers the students provide; 2) accurately predicting which questions have high quality; and 3) determining a personalized sequence of questions for each student that best predicts the student’s answers|regression|||score|accuracy|False|True|True|False||[1.1171680541745468]
TC4 Competition and Workshop on Human Identification at a Distance 2020 (HID2020)|The goal of the competition is to provide an evaluation for state-of-the-arts on human identification at a distance.|147|836.0|2020|2.0|3300|3300|69.0|https://competitions.codalab.org/competitions/26085||||||computer vision|Human Identification at a Distance|object detection|||correct|accuracy|False|False|True|False||[2.125513980263158]
CelebA-Spoof: Face Anti-Spoofing Challenge ECCV@2020|This challenge evaluates algorithms for face anti-spoofing on the CelebA-Spoof hidden test set.|149|289.0|2020|1.0|15000|15000|64.0|https://competitions.codalab.org/competitions/26210|||||multimedia|computer vision|determine whether a presented face is live or spoof|binary classification|||TPR@FPR=5E-3|FPR, TPR|True|False|True|False||[1.0377239981867137]
DeeperForensics Challenge 2020 @ ECCV SenseHuman Workshop|This competition aims at soliciting new ideas to advance the state of the art in real-world face forgery detection.|133|258.0|2020|2.0|15000|15000|64.0|https://competitions.codalab.org/competitions/25228|||||multimedia|computer vision|real-world face forgery detection|object detection|||BCELoss|binary cross-entropy|False|False|True|False||[0.0]
Predicting Generalization in Deep Learning|A competition of the NeurIPS 2020 competition track|213|1929.0|2020|2.0|None|None|110.0|https://competitions.codalab.org/competitions/25301||||||multi-domain|submit a python function whose input is a trained neural network and its training data and output is a complexity measure or generalization predictor that quantifies how well the trained model generalizes on the test data|regression|||Prediction score (MSLE)|conditional mutual information|False|False|True|True||[3.7350016757904148]
the anomalous diffusion challenge|The anomalous diffusion (AnDi) challenge aims at assessing the performance of published and unpublished methods for characterizing anomalous diffusion from single trajectories. The AnDi challenge consists of different tasks based on three main competition modalities: - Inference of the anomalous diffusion exponent. - Classification of the diffusion model. - Segmentation of trajectories. Each problem will further include sub-tasks for different number of dimensions.|87|282.0|2020|3.0|None|None|245.0|https://competitions.codalab.org/competitions/23601|||||physics|NLP|Task 1 - Inference of the anomalous diffusion exponent α. Task 2 - Classification of the diffusion model. Task 3 - Segmentation of trajectories|multi-class classification, regression, segmentation|||MAE|MAE, F1, RMSE|True|True|True|False||[0.0]
Process Discovery Algorithm Challenge 2020|Implementation of Heuristic Miner Algorithm with best quality and performance.|122|594.0|2020|2.0|None|None|32.0|https://competitions.codalab.org/competitions/26953||https://www.semanticscholar.org/paper/Process-mining-with-the-HeuristicsMiner-algorithm-Weijters-Aalst/e61c748f9a2df9c3fbda3a8361fdc3d847b7e3ae?p2df||||NLP|implement Heuristic Miner Algorithm||||Graph Identity, Duration, PyLint Score|Graph Identity, Duration, PyLint Score|True|False|True|True||[5.1443937799602475]
NeurIPS 2020 Hide-and-Seek Privacy Challenge|Synthetic Data Generation vs. Patient Re-identification with Clinical Time-series Data|132|379.0|2020|2.0|None|None|173.0|https://competitions.codalab.org/competitions/25312||https://arxiv.org/abs/2007.12087|||medicine|multi-domain|de-identification to preserve privacy while retaining data utility, prevent patient re-identification by limiting vulnerability to membership inference attacks||||NO NAMING|accuracy, re-identification score|True|True|True|True||[0.0]
SDU@AAAI-21 - Shared Task 1, Acronym Identification|"Acronym Identification competition is one of the shared tasks of ""the AAAI-21 Workshop on Scientific Document Understanding"". This task aims to identify acronyms (i.e., short-forms) and their meanings (i.e., long-forms) from the documents. The participants are invited to present their work at SDU@AAAI-21 workshop."|61|264.0|2020|2.0|None|None|95.0|https://competitions.codalab.org/competitions/26609|||||linguistics|NLP|sentence-level sequence labeling problem: acronym identification|binary classification|||F1|macro-averaged precision, recall, F1|True|False|True|False||[1.0276745146633623]
SDU@AAAI-21 - Shared Task 2, Acronym Disambiguation|"Acronym Disambiguation competition is one of the shared tasks of ""the AAAI-21 Workshop on Scientific Document Understanding"". This task aims to find the correct long-form of an ambiguous acronym with multiple potential long-forms in text. The participants are invited to present their work at SDU@AAAI-21 workshop."|47|198.0|2020|2.0|None|None|95.0|https://competitions.codalab.org/competitions/26611|||||linguistics|NLP|sentence-level sequence labeling problem: acronym disambiguation|multi-class classification||62,441 sentences and a dictionary of 732 ambiguous acronyms.|F1|macro-averaged precision, recall, F1|True|False|True|False||[1.055264848985587]
MetaDL Challenge|Few-shot learning with Deep Learning.|94|275.0|2020|2.0|1000|1000|69.0|https://competitions.codalab.org/competitions/26638||||||NLP|train a meta-learner on a meta-train set and produce a learner (a machine learning algorithm), which will be used to train on classification tasks generated from the meta-test set and evaluated|meta-learning, multi-class classification|||score|accuracy, duration|True|False|True|True|https://github.com/ebadrian/metadl|[1.7556924725421912]
L2RPN WCCI 2020 Competition|Train controllers to conduct a power grid for as long as possible while avoiding incidents.|209|985.0|2020|2.0|6000|6000|224.0|https://competitions.codalab.org/competitions/24902|https://l2rpn.chalearn.org/||||electricity |RL|operate energy grid|reinforcement learning|||score||||True|True||[-37.45366308912739]
Short-duration Speaker Verification (SdSV) Challenge 2020 - Task 1 : Text-Dependent|Evaluate New Technologies in Short Duration Scenarios|121|1007.0|2020|2.0|1000|1000|351.0|https://competitions.codalab.org/competitions/22393||||||signal processing, NLP|speaker verification in text-dependent mode: given a test segment of speech and the target speaker's enrollment data, automatically determine whether a specific phrase and the test segment was spoken by the target speaker|binary classification|||MinDCF|normalized minimum Detection Cost Function|False|False|True|False||[0.0]
Short-duration Speaker Verification (SdSV) Challenge 2020 - Task 2 : Text-Independent|Evaluate New Technologies in Short Duration Scenarios|141|1441.0|2020|2.0|1000|1000|351.0|https://competitions.codalab.org/competitions/22472||||||signal processing, NLP|given a test segment of speech and the target speaker enrollment data, automatically determine whether the test segment was spoken by the target speaker|binary classification|||MinDCF|normalized minimum Detection Cost Function|False|False|True|False||[0.0]
Dravidian-CodeMix - FIRE 2020|Sentiment Analysis for Davidian Languages in Code-Mixed Text|156|44.0|2020|1.0|None|None|203.0|https://competitions.codalab.org/competitions/25215|||||multimedia|NLP|sentiment analysis: identify sentiment polarity of the code-mixed dataset of comments/posts in Dravidian Languages (Malayalam-English and Tamil-English) collected from social media|multi-class classification||||weighted averaged Precision, weighted averaged Recall and weighted averaged F-Score across all the classes|True|False|True|True||[0.0]
Hope Speech Detection for Equality, Diversity, and Inclusion-EACL 2021|Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at First Workshop on Language Technology for Equality, Diversity, Inclusion (LT-EDI-2021) @EACL 2021|174|21.0|2020|1.0|None|None|52.0|https://competitions.codalab.org/competitions/27653|||||linguistics|NLP|"we define the hope speech for our problem as ""YouTube comments / posts that offer support, reassurance, suggestions, inspiration and insight"""|binary classification||||weighted averaged Precision, weighted averaged Recall and weighted averaged F-Score across all the classes|True|False|True|True||[0.0]
Offensive Language Identification in Dravidian Languages-EACL 2021|Shared task on offensive language identification in Dravidian Languages at DravidianLangTech 2021 workshop @EACL 2021|152|33.0|2020|1.0|None|None|56.0|https://competitions.codalab.org/competitions/27654|||||linguistics|NLP|Offensive Language Identification: identify offensive language content of the code-mixed dataset of comments/posts in Dravidian Languages ( (Tamil-English, Malayalam-English, and Kannada-English)) collected from social media|binary classification||||weighted averaged Precision, weighted averaged Recall and weighted averaged F-Score across all the classes|True|False|True|True||[0.0]
SemEval Task 9: Statement Verification and Evidence Finding with Tables (SEM-TAB-FACT)|Tables are ubiquitous in documents and presentations for conveying important information in a concise manner. This is true in many domains, stretching from scientific to government documents.  In fact,  surrounding text in these articles are often statements summarizing or highlighting some information derived from the primary source data in tables. Describing all the information provided in a table in a readable manner would be lengthy and considerably more difficult to understand. We present a task for statement verification and evidence finding using tables from scientific articles. This important task promotes proper interpretation of the surrounding article.|71|204.0|2020|3.0|None|None|89.0|https://competitions.codalab.org/competitions/27748|||||linguistics|NLP|statement verification and evidence finding using tables from scientific articles|binary classification, multi-class classification|||F1|F1|False|True|True|False||[1.0]
1st ACRE Cascade Competition|Multi-class segmentation of RGB images to distinguish crop, weeds, and the background.|495|5366.0|2020|3.0|None|None|105.0|https://competitions.codalab.org/competitions/27176||||||computer vision|segment RGB images to distinguish between crop, weeds, and background|multi-class classification|||Global IoU|mean Intersection over Union|False|False|True|False||[0.0]
SemEval 2021 Task 5: Toxic Spans Detection|This task concerns the evaluation of systems that detect the spans that make a text toxic, when detecting such spans is possible.|557|14692.0|2020|3.0|None|None|184.0|https://competitions.codalab.org/competitions/25623|||||linguistics|NLP|Toxic Spans Detection|binary classification, multi-class classification|||correct|F1|False|False|True|True||[0.0]
SemEval-2021 Task 4: Reading Comprehension of Abstract Meaning||252|526.0|2020|3.0|None|None|122.0|https://competitions.codalab.org/competitions/26153|||||linguistics|NLP|fill out abstract words removed from human written summaries|multi-class classification|||Accuracy_task1 , Accuracy_task2|accuracy|False|True|True|True||[1.3283547164926641]
Shared Task on Sarcasm and Sentiment Detection in Arabic (Subtask 2 - Sentiment Analysis)|Shared Task on Sarcasm and Sentiment Detection in Arabic|89|72.0|2021|2.0|None|None|36.0|https://competitions.codalab.org/competitions/28002|||||linguistics|NLP|Subtask 2 (Sentiment Analysis): Identifying the sentiment of a tweet and assigning one of three labels (Positive, Negative, Neutral), multiclass classification task.|multi-class classification|||nada|precision, recall, accuracy, F1|True|True|True|True||[0.0]
Shared Task on Sarcasm and Sentiment Detection in Arabic (Subtask 1 - Sarcasm Detection)|Shared Task on Sarcasm and Sentiment Detection in Arabic|111|157.0|2021|2.0|None|None|36.0|https://competitions.codalab.org/competitions/28003|||||linguistics|NLP|Subtask 1 (Sarcasm Detection): Identifying whether a tweet is sarcastic or not, this is a binary classification task. |binary classification|||nada|precision, recall, accuracy, F2|True|True|True|True||[0.0]
DECONbench Benchmarking platform for deconvolution methods for tumor heterogeneity quantification|Quantification of tumor heterogeneity is essential to better understand cancer progression and to adapt therapeutic treatments to patient specificities.We present DECONBench, a web-based application to benchmark computational methods dedicated to quantification of cell-type heterogeneity in cancer. DECONBench includes benchmark datasets, computational methods and  performance evaluation. DECONBench allows submission of new methods.|15|30.0|2020|1.0|None|None|366.0|https://competitions.codalab.org/competitions/27453|https://deconbench.github.io/||||medicine|computer vision|estimating cell types and proportion in biological samples based on methylation and transcriptome data sets|multi-class classification, regression|||MAE (mean)|MAE|False|False|True|True||[0.0]
FinSBD3 Financial Document Structure Boundary Detection|Financial Document Structure Boundary Detection, an extension of Sentence Boundary Detection|48|83.0|2021|2.0|1000|1000|35.0|https://competitions.codalab.org/competitions/28485|||||finance|NLP||extracting structure boundaries|||macro F1|F1|False|False|True|False||[2.089041095890411]
RuShiftEval Competition|shared task on automatic detection of semantic shift for Russian|38|227.0|2021|3.0|None|None|59.0|https://competitions.codalab.org/competitions/28340||https://www.dialog-21.ru/media/5536/pivovarovalpluskutuzova151.pdf|||linguistics|NLP|detection of word meaning shift from diachronic corpora||||Average score|rank|False|False|True|False||[1.810111345170027]
CVPR 2021 ChaLearn LAP Large Scale Signer Independent Isolated SLR Challenge (RGB TRACK)|This Challenge focuses on isolated sign language recognition from signer-independent non-controlled RGB data.|150|1369.0|2020|2.0|None|None|80.0|https://competitions.codalab.org/competitions/27901|https://chalearnlap.cvc.uab.cat/challenge/43/description/||https://ieeexplore.ieee.org/abstract/document/9210578||multimedia|computer vision|isolated sign language recognition from signer-independent non-controlled RGB-D data involving a large number of sign categories (>200)|multi-class classification||226 sign labels and 36,302 isolated sign video samples that are performed by 43 different signers|Rec. Rate |recognition rate|False|False|True|False||[1.1179837709962122]
CVPR 2021 ChaLearn LAP Large Scale Signer Independent Isolated SLR Challenge (RGB+D TRACK)|This Challenge focuses on isolated sign language recognition from signer-independent non-controlled RGB+D data.|64|209.0|2020|2.0|None|None|80.0|https://competitions.codalab.org/competitions/27902|https://chalearnlap.cvc.uab.cat/challenge/43/description/||||multimedia|computer vision|isolated sign language recognition from signer-independent non-controlled RGB-D data involving a large number of sign categories (>200)|multi-class classification||226 sign labels and 36,302 isolated sign video samples that are performed by 43 different signers|Rec. Rate |recognition rate|False|False|True|False||[1.0754537516372482]
PBVS 2022 Multi-modal Aerial View Object Classification Challenge - Track 2 (SAR+EO)|Our objectives are to gauge the s-o-t-a in Multi-modal Aerial view Imagery Classification, to promote research on this topic and to introduce a novel benchmark.|188|633.0|2020|2.0|None|None|373.0|https://competitions.codalab.org/competitions/28095|||||multimedia|computer vision|Multi-modal Aerial View Object Classification: obtain a network design / solution capable to produce high quality classification results with the best accuracy according to the ground truth labels|multi-class classification|||Accuracy (top-1)[%]|accuracy|False|False|True|False||[4.178423603669148]
NTIRE 2021 Video Super-Resolution Challenge - Track2. Spatio-Temporal|Our objectives are to gauge the state-of-the-art video super-resolution, to promote research on this topic and to introduce a novel benchmark.|253|117.0|2021|2.0|None|None|74.0|https://competitions.codalab.org/competitions/28072|||||multimedia|computer vision|super-resolving an input video with a upsampling factor x4 based on a set of prior examples of high-resolution videos. The aim of the challenge is to obtain a solution capable to produce high-resolution results with the best fidelity (PSNR, SSIM) to the ground truth|video transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[1.3744750379126214]
NTIRE 2021 High Dynamic Range Challenge - Track 2 Multiple Frames|Our objectives are to gauge the s-o-t-a in High Dynamic Range, to promote research on this topic and to introduce a novel benchmark.|285|445.0|2021|2.0|None|None|67.0|https://competitions.codalab.org/competitions/28162|||||multimedia|computer vision|obtain a network design / solution capable to produce high quality results with the best similarity (fidelity) to the reference ground truth|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[1.0873347047256228]
NTIRE 2021 High Dynamic Range Challenge - Track 1 Single Frame|Our objectives are to gauge the s-o-t-a in High Dynamic Range, to promote research on this topic and to introduce a novel benchmark.|378|229.0|2021|2.0|None|None|67.0|https://competitions.codalab.org/competitions/28161|||||multimedia|computer vision|obtain a network design / solution capable to produce high quality results with the best similarity (fidelity) to the reference ground truth|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[1.2461480601247492]
NTIRE 2021 Defocus Deblurring using Dual-pixel Images Challenge|Our objectives are to gauge the s-o-t-a in example-based defocus deblurring using dual-pixel images, to promote research on this topic and to introduce a novel benchmark.|217|550.0|2021|2.0|None|None|78.0|https://competitions.codalab.org/competitions/28049|||||multimedia|computer vision|deblur the input images with the best quantitative results|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[-2.8125]
NTIRE 2021 Depth Guided Relighting Challenge Track 2: Any-to-any relighting|Our objectives are to gauge the s-o-t-a in depth guided image relighting, or illumination style transfer, to promote research on this topic and to introduce a novel benchmark.|95|112.0|2020|2.0|None|None|80.0|https://competitions.codalab.org/competitions/28031|||||multimedia|computer vision|examble-based single image relighting, that is, the task of manipulating an input image that was captured under certain illumination settings (light source position and color temperature) to make it look like it was taken under different settings|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[-10.0]
NTIRE 2021 Quality enhancement of heavily compressed videos Challenge - Track 1 Fixed QP, Fidelity|Our objectives are to gauge the s-o-t-a in enhancement of compressed videos, to promote research on this topic and to introduce a novel benchmark.|229|413.0|2021|2.0|None|None|78.0|https://competitions.codalab.org/competitions/28033|||||multimedia|computer vision|Quality enhancement of heavily compressed videos Challenge, that is, the task of restoring the clean contents from a compressed video based on a the knowledge of the compression method|video transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[1.0478666610335814]
NTIRE 2021 Video Super-Resolution Challenge - Track1. Spatial|Our objectives are to gauge the state-of-the-art video super-resolution to promote research on this topic and to introduce a novel benchmark.|285|143.0|2021|2.0|None|None|78.0|https://competitions.codalab.org/competitions/28051|||||multimedia|computer vision|super-resolving an input video with a upsampling factor x4 based on a set of prior examples of high-resolution videos. The aim of the challenge is to obtain a solution capable to produce high-resolution results with the best fidelity (PSNR, SSIM) to the ground truth|video transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[1.104074439524189]
NTIRE 2021 Burst Super-Resolution Challenge - Track 1 Synthetic|Our objectives are to gauge the s-o-t-a in burst image super-resolution, to promote research on this topic and to introduce a novel benchmark.|215|270.0|2021|2.0|None|None|78.0|https://competitions.codalab.org/competitions/28078|https://github.com/goutamgmb/NTIRE21_BURSTSR||||multimedia|computer vision|generate a denoised, demosaicked, higher-resolution image, given a RAW burst as input|image generation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[1.162435732390619]
NTIRE 2021 Depth Guided Relighting Challenge Track 1: One-to-one relighting|Our objectives are to gauge the s-o-t-a in depth guided image relighting, or illumination style transfer, to promote research on this topic and to introduce a novel benchmark.|156|210.0|2020|2.0|None|None|80.0|https://competitions.codalab.org/competitions/28030|||||multimedia|computer vision|examble-based single image relighting, that is, the task of manipulating an input image that was captured under certain illumination settings (light source position and color temperature) to make it look like it was taken under different settings|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[13.5]
NTIRE 2021 Image Deblurring Challenge - Track1. Low Resolution|Our objectives are to gauge the state-of-the-art image deblurring, to promote research on this topic and to introduce a novel benchmark.|413|269.0|2021|2.0|None|None|74.0|https://competitions.codalab.org/competitions/28073|||||multimedia|computer vision|examble-based Image Deblurring, that is, the task of deblurring an input image with a upsampling factor x4 based on a set of prior examples of sharp videos. The aim of the challenge is to obtain a solution capable to produce sharp results with the best fidelity (PSNR, SSIM) to the ground truth|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[1.0472228681213147]
NTIRE 2021 Image Deblurring Challenge - Track2. JPEG Artifacts|Our objectives are to gauge the state-of-the-art image deblurring, to promote research on this topic and to introduce a novel benchmark.|276|167.0|2021|2.0|None|None|74.0|https://competitions.codalab.org/competitions/28074|||||multimedia|computer vision|examble-based Image Deblurring, that is, the task of deblurring an input image with a upsampling factor x4 based on a set of prior examples of sharp videos. The aim of the challenge is to obtain a solution capable to produce sharp results with the best fidelity (PSNR, SSIM) to the ground truth|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[1.0475284340554079]
NTIRE 2021 NonHomogeneous Dehazing Challenge|Our objectives are to gauge the s-o-t-a in example-based single image dehazing, to promote research on this topic and to introduce a novel benchmark.|440|1465.0|2020|2.0|None|None|80.0|https://competitions.codalab.org/competitions/28032|||||multimedia|computer vision|examble-based single image dehazing, that is, the task of restoring the clean contents from a hazy input image based on a set of prior examples of hazy and hazy-free images|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[0.0]
NTIRE 2021 Perceptual Image Quality Assessment Challenge|Our objectives are to gauge the s-o-t-a in perceptual image quality assessment, to promote research on this topic and to introduce a novel benchmark.|365|2271.0|2021|2.0|None|None|78.0|https://competitions.codalab.org/competitions/28050|||||multimedia|computer vision|Image Quality Assessment (IQA): measure the performance of image restoration algorithms, task of predicting the perceptual quality of an image based on a set of prior examples of images and their perceptual quality labels|image recognition, regression|||MainScore|Pearson linear correlation coefficient (PLCC), Spearman rank-order correlation coefficients (SROCC)|True|False|True|False||[1.2653923556079765]
Interspeech Shared Task: Automatic Speech Recognition for Non-Native Children’s Speech|A joint shared task between FBK, ETS and Cambridge|54|80.0|2021|4.0|None|None|11.0|https://competitions.codalab.org/competitions/28890|||||linguistics|NLP|non-native children's speech recognition|binary classification|||wer (%) |Word Error Rate (WER)|False|False|True|False||[0.0]
Mobile AI 2021 Real-Time Video Super-Resolution Challenge|Designing deep learning based video super-resolution models for mobile devices @ Powered by OPPO & ETH Zurich & SNU|149|84.0|2021|2.0|None|None|75.0|https://competitions.codalab.org/competitions/28112|||||multimedia, mobile|computer vision|restoring high-resolution image frames from their low-resolution|video transformation|||PSNR [dB]|PSNR, SSIM, score(PSNR,runtime)|True|False|True|False||[1.0322439099305978]
Mobile AI 2021 Real Image Denoising Challenge|Designing deep learning based image denoiser for mobile devices @ Powered by Samsung & ETH Zurich|241|660.0|2021|2.0|None|None|79.0|https://competitions.codalab.org/competitions/28120|||||multimedia, mobile|computer vision|real image denoising|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[1.0751484756136278]
Mobile AI 2021 Real-Time Image Super-Resolution Challenge|Designing deep learning based image super-resolution models for smart TV devices @ Powered by Synaptics & ETH Zurich|219|268.0|2021|2.0|None|None|79.0|https://competitions.codalab.org/competitions/28119|||||multimedia, mobile|computer vision|real-time image super-resolution challenge, where the target is to obtain an output image with the highest pixel fidelity to the ground truth high resolution image as measured by PSNR / MS-SSIM scores and at the same time the model should be very efficient as measured by inference time on Synaptics smart TV platform with a dedicated NPU|image transformation|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[1.0921070430647126]
Mobile AI 2021 Monocular Depth Estimation Challenge|Designing deep learning based monocular depth estimator for mobile devices @ Powered by Raspberry Pi & ETH Zurich|198|479.0|2021|2.0|None|None|79.0|https://competitions.codalab.org/competitions/28122|||||multimedia, mobile|computer vision|obtain an output depth image with the highest fidelity to the ground truth and at the same time the model should be very efficient as measured by inference time on the mobile platform|image transformation|||Score1|si-RMSE, RMSE|True|False|True|False||[0.0]
Mobile AI 2021 Real-Time Camera Scene Detection Challenge|Designing deep learning based camera scene detection algorithm for mobile devices @ Powered by ETH Zurich|138|479.0|2021|2.0|None|None|75.0|https://competitions.codalab.org/competitions/28113|||||multimedia, mobile|computer vision|scene detection: obtain high precision results as measured by Top-1 / Top-3 accuracy, and at the same time the model should be very efficient as measured by inference time on the mobile platform|scene recognition|||Accuracy [%]|accuracy, runtime, GPU|True|False|True|False||[1.1118041318362364]
Mobile AI 2021 Learned Smartphone ISP Challenge|Designing deep learning based learned camera ISP for mobile devices @ Powered by MediaTek & ETH Zurich|274|674.0|2021|2.0|3000|3000|79.0|https://competitions.codalab.org/competitions/28054|||||multimedia, mobile|computer vision|obtain an output image with the best trade-off between pixel fidelity (PSNR) and latency on mobile devices: accurately recover the information coming from the tiny mobile camera sensors or render the missing image details|image recognition|||PSNR|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index, runtime, CPU|True|False|True|False||[1.083275458418748]
DiCOVA 2021 Challenge|A competition to evaluate Diagnosis of COVID-19 using Acoustics (DiCOVA)|77|496.0|2021|1.0|None|None|22.0|https://competitions.codalab.org/competitions/29640|||||covid|signal processing|The score files that participants are required to submit should indicate the probability of COVID positive, for each test recording|regression|||Test AUC|AUC|False|False|True|False||[1.1514517239649598]
RuNormAS||10|221.0|2021|1.0|None|None|41.0|https://competitions.codalab.org/competitions/29216|||||linguistics|NLP|Russian Normalization of Annotated Spans|text modification|||Generic spans|accuracy|False|False|True|False|https://github.com/dialogue-evaluation/RuNormAS|[1.2598881676200269]
Multimodal Emotion Recognition on Comics scenes (EmoRecCom)|The emotions of comic characters are described by the Visual information, the Text in speech Balloons or Captions and the Onomatopoeia (Comic drawings of words that phonetically imitates, resembles, or suggests the sound that it describes). The task hence is a multi-modal analysis task which can take advantages from both fields computer vision and natural language processing which are two of the main interests of the ICDAR community.|159|615.0|2020|3.0|None|None|105.0|https://competitions.codalab.org/competitions/27884|||||comics|computer vision, NLP|the emotion recognition of comic scenes: extracting the emotions of comic characters in comic scenes (panels). The emotions of comic characters are described by the Visual information, the Text in speech Balloons or Captions and the Onomatopoeia (Comic drawings of words that phonetically imitates, resembles, or suggests the sound that it describes)|multi-class classification, regression|||ROC-AUC|AUC|False|False|True|False||[1.098989898989899]
ICDAR 2021 Competition on Scene Video Text Spotting|To support this competition, we extend the Larger-Scale Video Text Dataset released in YORO [1], and release a dataset containing 129 video clips from 21 real-life scenarios for this competition.[1] Cheng Z., Lu, J., Niu, Y., Pu, S., Wu, F., and Zhou, S. 2019. You Only Recognize Once: Towards Fast Video Text Spoting. In ACM Multimedia 2019.|208|1473.0|2021|3.0|None|None|41.0|https://competitions.codalab.org/competitions/27667||||||computer vision, NLP|Scene video text spotting (SVTS) is a text spotting system of localizing and recognizing text from continuous video frames, which usually contains multiple modules: video text detection, text tracking and the final recognition|multi-domain, spotting text from video frames|||F_score|F1, Precision, Recall, ATA, MOTA         MOTP|True|False|True|False||[0.0]
DeepFake Game Competition (DFGC) @ IJCB 2021|A competition to evaluate the status of adversarial game between Deepfake creation and detection.|194|2135.0|2021|6.0|8000|8000|42.0|https://competitions.codalab.org/competitions/29583|||||multimedia|computer vision|DeepFake video creation and detection|video transformation, video generation|||Score (C/D)|ID level similarity to the donor ID, image level similarity to the target frame, and the deception ability against detection models; ability to detect DeepFakes|True|True|True|False||[1.476330599594974]
TC4 Competition on Human Identification at a Distance 2021 (HID2021)|The goal of the competition is to provide an evaluation for state-of-the-arts on human identification at a distance. The features used can be gait, body shape and any other cues on human body.|134|1212.0|2021|2.0|3300|3300|50.0|https://competitions.codalab.org/competitions/29552||||||computer vision|Human Identification at a Distance|object detection|||correct|accuracy|False|False|True|False||[1.9399567390169308]
ICLR 2021 Workshop MLPCP Track 1 Entity-aware medical dialogue generation|None|115|368.0|2021|1.0|None|None|61.0|https://competitions.codalab.org/competitions/29703|||||medicine|NLP||dialogue generation|||Total Score|BLEU, Entity-F1, F1|True|False|True|False|https://competitions.codalab.org/competitions/30755|[1.5397447012047474]
ICLR 2021 Workshop MLPCP Track 2 Medical Dialogue System for Automatic Diagnosis|None|171|571.0|2021|1.0|None|None|61.0|https://competitions.codalab.org/competitions/29706|||||medicine|NLP|building task-oriented dialogue system for automatic medical diagnosis, which converses with patients to collect additional symptoms beyond their self-reports and makes a disease diagnosis in the end|dialogue generation|||Total Score|accuracy, F1|True|False|True|False||[1.3960369854235113]
UG2+ 2021 Track2.2 - Semi-supervised Action Recognition in the Dark|Semi-supervised Action Recognition in the Dark for track2.2 challenge, UG2+ workshop, CVPR 2021|47|80.0|2021|2.0|None|None|112.0|https://competitions.codalab.org/competitions/28118|||||multimedia|computer vision|action recognition in the dark|action recognition||643 videos from 5 classes|Accuracy |accuracy|False|False|True|True||[1.0884243428696925]
CAp21/Criteo challenge|CAp21/Criteo challenge: prediction without identifiability.|44|281.0|2021|2.0|None|None|45.0|https://competitions.codalab.org/competitions/30003|||||privacy protection|NLP|research data representations that will balance predictive performance with anonymity guarantees: design a privacy function that maps individual data to a data representation|generate data represenation|||Sum of Losses|prediction loss, privacy prediction score|True|True|True|False||[0.0]
4th UG2+ challenge (CVPR 2021) Track 1.1 - Object Detection in the Hazy Condition|Object detection in the hazy condition for track 1.1, 4th UG2+ challenge, CVPR 2021|175|2405.0|2020|2.0|None|None|125.0|https://competitions.codalab.org/competitions/28022|||||multimedia|computer vision|Object Detection in the Hazy Condition|object recognition||4,322 real-world hazy images collected from traffic surveillance|mAP|mAP|False|False|True|True||[1.446382309400444]
UG2+ 2021 Track1.2 - Face Detection in the Low-Light Condition|Face Detection in the Low-Light Condition for track1.2 challenge, UG2+ workshop, CVPR 2021|157|1724.0|2021|2.0|None|None|89.0|https://competitions.codalab.org/competitions/28029|||||multimedia|computer vision|Face Detection in the Low-Light Condition|face recognition||10,000 real-world low light images captured during the nighttime, at teaching buildings, streets, bridges, overpasses, parks etc., all labeled with bounding boxes for of human face|mAP|mAP|False|False|True|True||[1.246776412016759]
CLEF2021 CheckThat! Lab - Task 1|Given a piece of text, detect whether it is worth fact-checking.|36|213.0|2020|11.0|None|None|174.0|https://competitions.codalab.org/competitions/30853|||||multimedia, tweets|NLP|ranking a stream of tweets according to their check-worthiness|ordinal regression|||Mean-Average-Precision|mAP|False|False|True|False||[1.5621761658031088]
CLEF2021 CheckThat! Lab - Task 2|Given a check-worthy claim, and a set of previously fact-checked claims, determine whether the claim has been previously fact-checked|21|74.0|2020|6.0|None|None|174.0|https://competitions.codalab.org/competitions/30949|||||multimedia, tweets|NLP|rank the previously fact-checked claims in order of usefulness to fact-check the input claim|ordinal regression|||MAP@5|mAP, MRR, precisio|True|False|True|False||[1.0975761342448727]
CLEF2021 CheckThat! Lab - Task 3|Fake News Detection at CLEF2021-CheckThat!|57|142.0|2021|2.0|None|None|9.0|https://competitions.codalab.org/competitions/31238|||||multimedia, tweets, fake news|NLP|Multi-class fake news detection of news articles|multi-class classification|||correct|macro F1|False|False|True|False||[nan]
Fake News Detection Competition|This is a competition for detection of fake news in a spanish corpus|76|166.0|2021|2.0|None|None|72.0|https://competitions.codalab.org/competitions/29545|||||multimedia, tweets, fake news|NLP|Multi-class fake news detection of news articles|multi-class classification, binary classification|||F1|F1|False|False|True|True||[1.1495715509854323]
FloodNet Challenge @ EARTHVISION 2021 - Track 1|This challenge offers to build semi-supervised image classification and semantic segmentation models for post-disaster damage assessment.|234|1021.0|2021|2.0|None|None|50.0|https://competitions.codalab.org/competitions/30290|||||disaster management|computer vision|Image Classification and Semantic Segmentation for post-disaster damage assessment|multi-class classification|||Overall |accuracy, F1, mean IoU|True|True|True|False||[1.3460020146470282]
FloodNet Challenge @ EARTHVISION 2021 - Track 2|This challenge offers to build Visual Question Answering model for post-disaster damage assessment purposes.|107|432.0|2021|2.0|None|None|50.0|https://competitions.codalab.org/competitions/30320|||||disaster management|computer vision, NLP|Image Classification and Semantic Segmentation for post-disaster damage assessment|VQA|||Accuracy |accuracy|False|False|True|False||[1.2422897476248533]
The 3rd Large-scale Video Object Segmentation Challenge - Track 1: Video Object Segmentation|YouTube-VOS is the first large-scale dataset for video object segmentation. Our dataset contains 4000+ YouTube videos, 70+ common objects and densely-sampled high-quality pixel-level annotations.|179|43.0|2021|2.0|None|None|103.0|https://competitions.codalab.org/competitions/28987|||||multimedia|computer vision|video object segmentation: dense-prediction video, segmenting a particular object isntance throughout the entire video sequence given only the object mask of the first frame|video recognition|||Overall |Region Jaccard (J) and Boundary F measure (F)|True|False|True|False||[1.0146187277755827]
The 3rd Large-scale Video Object Segmentation Challenge - Track 2: Video Instance Segmentation|YouTube-VOS is the first large-scale dataset for video object segmentation. Our dataset contains 4000+ YouTube videos, 70+ common objects and densely-sampled high-quality pixel-level annotations.|1320|4432.0|2021|2.0|None|None|103.0|https://competitions.codalab.org/competitions/28988|||||multimedia|computer vision|video instance segmentation: simultaneous detection, segmentation and tracking of instances in videos|video recognition||3,859 high-resolution YouTube videos|mAP|AP, AR|True|True|True|False||[1.286257727056586]
High Resolution Human Parsing|This is a competition for high-resolution human parsing. We put forward a dataset named HRHP, including 20 human body-parts categories. There are 6000 images for training, 500 images for validation and 1000 images for testing.|110|223.0|2021|2.0|None|None|44.0|https://competitions.codalab.org/competitions/30375|||||multimedia|computer vision|high-resolution human parsing: predict the semantic segmentation categories of each pixel|multi-class classification||7500 images|EIoU |EIoU(Edge IoU), mIoU(mean IoU)|True|False|True|True||[1.3823912974331245]
ADoBo — Automatic Detection of Borrowings|Detecting emerging borrowings from English into Spanish (words like 'smartphone' or 'fake news') that appear in the Spanish press|50|16.0|2021|1.0|None|None|21.0|https://competitions.codalab.org/competitions/28771|||||linguistics|NLP|detection of lexical borrowing|binary classification||||precision, recall and F1|True|False|True|True||[0.0]
The 3rd Large-scale Video Object Segmentation - Track 3: Referring Video Object Segmentation|YouTube-VOS is the first large-scale dataset for video object segmentation. Our dataset contains 3900+ YouTube videos, densely-sampled high-quality pixel-level annotations, and 15k language expressions.|465|2502.0|2021|2.0|None|None|65.0|https://competitions.codalab.org/competitions/29139|https://youtube-vos.org/||||multimedia|computer vision, NLP|video object segmentation: referring video object segmentation, which targets at segmenting a particular object instance throughout the entire video sequence given only the language expressions|video recognition|||Overall |Region Jaccard (J), Boundary F measure (F)|True|False|True|False||[1.2035690680766689]
MeOffendEs@IberLEF 2021|Offensive language detection in Spanish Variants|82|318.0|2021|6.0|None|None|100.0|https://competitions.codalab.org/competitions/28679|||||multimedia, tweets|NLP|offensive language detection and classification|multi-class classification|||Micro precision|Micro-averaged precision, recall and f-score|True|True|True|False||[1.0]
EmoEvalEs@IberLEF 2021|Workshop en Emotion detection and Evaluation for Spanish|85|135.0|2021|2.0|None|None|100.0|https://competitions.codalab.org/competitions/28682|||||multimedia, tweets|NLP|emotion detection|multi-class classification|||Accuracy |weighted averaged Precision, weighted averaged Recall and weighted averaged F-Score across all the classes|True|False|True|False||[1.1036819582156536]
The 4th Look Into Person (LIP) Challenge - Track 5 Dark Complexion Portrait Segmentation Challenge|The dark complexion portrait segmentation challenge of the 4th LIP Challenge|49|674.0|2020|1.0|None|None|434.0|https://competitions.codalab.org/competitions/24206|||||multimedia|computer vision|predict the segmentation map of an image containing dark complexion portrait|regression||12165 training and 2570 test images, in which there are 14735 dark complexion portrait images in total|Mean IOU|Mean IoU|False|False|True|False||[1.062295739125109]
CVPR-NAS 2021: Unseen Data Track|NAS Competiton on Unseen Datasets for CVPR-NAS 2021: How well do NAS algorithms transfer to entirely novel datasets?|74|426.0|2021|2.0|1000|1000|78.0|https://competitions.codalab.org/competitions/29853||||||multi-domain|produce a NAS algorithm that, when given a dataset, outputs a well-performing, robust neural architecture||||Overall Score|accuracy|False|False|True|True||[3.456920903954803]
DynamicEarthNet Weakly-Supervised Multi-Class Change Detection Challenge|The weakly-supervised track of DynamicEarthNet Challenge|145|340.0|2021|2.0|None|None|47.0|https://competitions.codalab.org/competitions/30441|http://www.classic.grss-ieee.org/earthvision2021/challenge.html|https://arxiv.org/pdf/2203.12560.pdf|||multimedia|computer vision|perform monthly multi-class change detection with a limited amount of multi-class change labels|multi-class classification|||mIoU|SCS, BC, SC, mIoU|True|False|True|False||[1.0974826622255047]
OVIS (Occluded Video Instance Segmentation) Please submit to the new evaluation server!|OVIS (short for Occluded Video Instance Segmentation) is a new large scale benchmark dataset for video instance segmentation task. It is designed with the philosophy of perceiving object occlusions in videos, which could reveal the complexity and the diversity of real-world scenes.|172|134.0|2021|1.0|None|None|123.0|https://competitions.codalab.org/competitions/28757|http://songbai.site/ovis/||||multimedia|computer vision|Occluded Video Instance Segmentation|multi-class classification||296k high-quality instance masks from 25 semantic categories|Score ||||True|False||[1.5635319454414933]
AILBIZ 2021 - TRACK 1|Unsupervised Approaches for Understanding Legal Work|27|27.0|2021|2.0|None|None|83.0|https://competitions.codalab.org/competitions/29503|||||legal|NLP|unsupervised learning methods for breaking down legal matters into meaningful units of work:  assign a single code from the specified codeset to each of the timecards in the provided data collection|multi-class classification|||Similarity |Precision, Recall, F1|True|False|True|False||[1.1168911277886295]
Realistic Instance-level Product Retrieval (Product1M) Track on LID CVPR 2021 Challenge|Realistic Instance-level Product Retrieval (Product1M) Track on LID CVPR 2021 ChallengeWorkshop link: https://l2id.github.io/|42|146.0|2021|1.0|None|None|109.0|https://competitions.codalab.org/competitions/30123|||||ecommerce|computer vision, NLP|Given an image containing multiple product instances and a user-provided description, this task aims to retrieve the correct single product image in the gallery|multi-class classification|||mAR@10|precision, mAP, mAR|True|False|True|False||[1.4347826086956523]
ActivityNet-Entities Object Localization Challenge 2021 (Sub-task II -  Generated Captions)|This is the official evaluation server for ActivityNet-Entities dataset on the object localization task. This sub-task is about grounding on generated captions. More details please see https://github.com/facebookresearch/ActivityNet-Entities#activitynet-entities-object-localization-challenge-2020 or http://activity-net.org/challenges/2020/tasks/guest_anet_eol.html (to be updated for 2021, rules are the same)|12|16.0|2020|2.0|None|None|463.0|https://competitions.codalab.org/competitions/24334|||||ecommerce|computer vision, NLP|evaluate how grounded or faithful a description (could be generated or ground-truth) is to the video they describe|regression|||F1_all_per_sent|F1,  localization accuracy|True|True|True|False||[0.0]
BDD100K Multiple Object Tracking Challenge|<p>This challenge evaluates algorithm for object detection with bounding box output on BDD100K dataset.</p>|9|21.0|2021|2.0|None|None|60.0|https://competitions.codalab.org/competitions/30620|||||multimedia|computer vision|multiple object tracking|multi-class classification|||mMOTA |mMOTA, mMOTP|True|False|True|True||[0.0]
CVPR 2021 Human-centric video matting - TRACK 1 (FVC)|This challenge aims to perform efficient and accurate portrait matting in videos, which can be applied to real video conferencing scenarios such as setting virtual background....|194|1814.0|2021|2.0|None|None|63.0|https://competitions.codalab.org/competitions/30523|||||multimedia|computer vision|portrait matting in videos|video transformation|||MSE|MSE, SAD|True|False|True|True||[0.0]
2021 HACS Temporal Action Localization Challenge - Supervised Learning Track|A CVPR'21 video recognition challenge.|31|35.0|2021|1.0|None|None|42.0|https://competitions.codalab.org/competitions/31307|||||multimedia|computer vision|temporally localize actions in untrimmed videos: predicting the start and end times of each action as well as the action label|multi-class classification, regression|||Mean Average Precision|mAP|False|False|True|True||[1.599321128082909]
Shared Task on Multilingual protest news detection CASE 2021 @ ACL-IJCNLP 2021|Extracting Protests from News Using Automated Methods: Multilingual protest news detection|40|477.0|2021|13.0|None|None|39.0|https://competitions.codalab.org/competitions/31247|||||news|NLP|protest news detection|binary classification, multi-class classification|||score |macro-F1|False|False|True|False||[1.0878491802578247]
Mobile AI 2021 High Dynamic Range Challenge|Designing deep learning based HDR for mobile devices @ Powered by Huawei & ETH Zurich|151|59.0|2021|2.0|None|None|152.0|https://competitions.codalab.org/competitions/28662|||||multimedia|computer vision|recovering an HDR image from one or multiple input Low Dynamic Range (LDR) images that are affected by noise, quantization errors, and might suffer from  over- and under-exposed regions due to the sensor limitations|image transformation|||muPSNR |Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[1.165266240937245]
Woodscape Fisheye Semantic Segmentation for Autonomous Driving - CVPR 2021 OmniCV Workshop Challenge|Woodscape is a multi-task, multi-camera fisheye dataset for autonomous driving. The objective of this challenge is to generate semantic segmentation masks for fisheye images, and in particular to advance the state of the art and to benchmark techniques for semantic segmentation on fisheye images.|87|403.0|2021|1.0|None|None|54.0|https://competitions.codalab.org/competitions/30993|||||autonomous driving|computer vision|generate semantic segmentation masks for fisheye images, and in particular to advance the state of the art and to benchmark techniques for semantic segmentation on fisheye images|autonomous driving, image transformation|||score |mIoU|False|False|True|False||[1.356521739130435]
CVPR 2021 Human-centric video coding for analytics - Track 2 (FVC)|This challenge aims to develop new image/video pre-editing methods for human-centric frame reconstruction jointly with the related analysis, which provides the technical basis for highly efficient video compression and transmission for conferencing scenarios. The competitors are allowed to pre-edit video frames. Then, we will compress these frames with the same bit-rate constraint with existing codecs, e.g. HEVC, and calculate the evaluation metrics on the decoded frames.|56|57.0|2021|1.0|None|None|2.0|https://competitions.codalab.org/competitions/30111|||||multimedia|computer vision|develop new image/video pre-editing methods for human-centric frame reconstruction jointly with the related analysis, which provides the technical basis for highly efficient video compression and transmission for conferencing scenarios|image transformation, video transformation|||correct|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|False||[0.0]
Chalearn 3D High-Fidelity Mask Face Presentation Attack Detection Challenge@ICCV2021|Face presentation attack detection (PAD) is essential to secure face recognition systems primarily from high-fidelity mask attacks. Most existing 3D mask PAD benchmarks suffer from several drawbacks: 1) a limited number of mask identities, types of sensors, and a total number of videos, 2) low-fidelity quality of facial masks. Basic deep models and remote photoplethysmography (rPPG) methods achieved acceptable performance on these benchmarks but still far from the needs of practical scenarios. To bridge the gap to real-world applications, we introduce a large-scale High-Fidelity Mask dataset, namely HiFiMask. Specifically, a total amount of 54,600 videos are recorded from 75 subjects with 225 realistic masks by 7 new kinds of sensors.|229|976.0|2021|2.0|None|None|62.0|https://competitions.codalab.org/competitions/30910|||||multimedia|computer vision|face anti-spoofing attack detection based on multi-modal data, participates are permitted to use the single RGB modality|NN attacks|||RANK<ACER>|Attack Presentation Classification Error Rate (APCER), Normal/Bona Fide Presentation Classification Error Rate (NPCER/BPCER) and Average Classification Error Rate (ACER)|True|False|True|False||[0.0]
The 4th Look Into Person (LIP) Challenge - Track 1 Multi-Person Human Parsing Challenge|The multi-person human parsing challenge of the 4th LIP Challenge|67|425.0|2020|1.0|None|None|496.0|https://competitions.codalab.org/competitions/23431|||||multimedia|computer vision|predict the semantic part segmentation map and instance-level human parsing map of an image which contains multiple persons|regression|||Rank, Mean IOU|Mean IoU, APr|True|False|True|False||[1.1340138490926457]
The 4th Look Into Person (LIP) Challenge - Track 3 Image-based Multi-pose Virtual Try-on Challenge|The image-based multi-pose virtual try-on track of the 4th LIP Challenge|47|67.0|2020|1.0|None|None|496.0|https://competitions.codalab.org/competitions/23471|||||multimedia|computer vision|given an input person image, a desired clothing image, and a desired pose, the participator is asked to design algorithm to transfer the desired clothing to the person image and manipulate human poses|image transformation||37,723/14,360 person/clothes images, with the resolution of 256x192|MeanSSIM |SSIM (Structural SIMilarity Index), AMT(Amazon Mechanical Turk)|True|False|True|False||[1.139957801667839]
HEART-MET Gesture Recognition Challenge|Gesture recognition from video|23|25.0|2021|2.0|None|None|83.0|https://competitions.codalab.org/competitions/30454|||||multimedia|computer vision|recognize gestures from videos|gesture recognition|||True positive rate|TPR|False|False|True|False||[1.0896933027331526]
Disambiguation of German Verbal Idioms|This is a shared task on the disambiguation of German verbal idioms in context.|13|13.0|2021|2.0|None|None|46.0|https://competitions.codalab.org/competitions/31715|||||linguistics|NLP|identification of verbal multiword expressions: automatically decide for cases like this if the expression has its literal or its idiomatic meanin|binary classification|||F1-all|F1|False|False|True|False||[1.521315185197511]
MMAct Challenge 2021 Task 2. Cross-Modal Untrimmed Action Temporal Localization|The MMAct Challenge 2021 will be hosted in the CVPR'21 International Challenge on Activity Recognition (ActivityNet) Workshop. This challenge asks participants to propose cross-modal video action recognition/localization approaches for addressing shortcomings in visual only approaches using MMAct Dataset.|13|42.0|2021|1.0|None|None|62.0|https://competitions.codalab.org/competitions/31385|||||multimedia|computer vision|propose cross-modal video action recognition/localization approaches for addressing shortcomings in visual only approaches using MMAct Dataset|action recognition|||AP|AP|False|False|True|False||[1.8273882738827387]
MMAct Challenge 2021 Task 1. Cross-Modal Trimmed Action Recognition|The MMAct Challenge 2021 will be hosted in the CVPR'21 International Challenge on Activity Recognition (ActivityNet) Workshop. This challenge asks participants to propose cross-modal video action recognition/localization approaches for addressing shortcomings in visual only approaches using MMAct Dataset.|28|121.0|2021|1.0|None|None|62.0|https://competitions.codalab.org/competitions/31438|||||multimedia|computer vision|propose cross-modal video action recognition/localization approaches for addressing shortcomings in visual only approaches using MMAct Dataset|action recognition|||mAP|mAP|False|False|True|False||[2.470300333704116]
Discover the mysteries of the Maya @ ECML PKDD 2021- Integrated Image Segmentation Challenge|Explore the potential of the Sentinel satellite data, in combination with the available lidar data, for integrated image segmentation in order to locate and identify “lost” ancient Maya settlements (aguadas, buildings and platforms), hidden under the thick forest canopy.|115|1130.0|2021|1.0|5300|5300|91.0|https://competitions.codalab.org/competitions/30429|||||multimedia|computer vision|Explore the potential of the Sentinel satellite data, in combination with the available lidar data, for integrated image segmentation in order to locate and identify “lost” ancient Maya settlements (aguadas, buildings and platforms), hidden under the thick forest canopy|image recognition|||Avg. IOU|IoU|False|False|True|False||[1.1642370476621087]
GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments|GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments (more info on the shared task website)|30|46.0|2021|1.0|None|None|9.0|https://competitions.codalab.org/competitions/32854|https://germeval2021toxic.github.io/SharedTask/||||linguistics|NLP|identification of offensive comments. To this end, we extend the focus to two other classes of comments that are highly relevant to moderators and community managers on online discussion platforms: engaging comments and fact-claiming comments|binary classification||4,000 Facebook user comments that have been labeled||precision, recall, macro-average F1|True|True|True|False||[0.0]
GtX 2021 quantitative competition|Geothermal experience competition|121|986.0|2021|2.0|None|None|37.0|https://competitions.codalab.org/competitions/31819|||||oil&gas|NLP|The questions is what wells are showing geothermal potential and based on those, which areas deserve further evaluation|binary classification|||mae|MAE|False|False|True|False||[0.0]
Anti-UAV|The 2nd Anti-UAV Workshop & Challenge|170|1864.0|2021|2.0|None|None|59.0|https://competitions.codalab.org/competitions/23887|||||drones|computer vision|detecting and tracking UAVs (unmanned aerial vehicle)|binary classification|||correct|accuracy|False|True|True|False||[1.1292346298619822]
ETCI 2021 Competition on Flood Detection|2021 ETCI Flood Detection Competition|196|1205.0|2021|2.0|None|None|91.0|https://competitions.codalab.org/competitions/30440|||||disaster management|computer vision|identify flood pixels after training their algorithm against a training set of synthetic aperture radar (SAR) images. Participants are required to submit binary classification maps, and performance will be evaluated using the intersection over union (IOU) score|binary classification|||IOU Score|IoU|False|False|True|False||[1.3999307416115332]
The 2nd Remote Physiological Signal Sensing (RePSS) Challenge & Workshop Associated with ICCV 2021|This is the second challenge on remote physiological signal measurement, which will be focusing on continuous heart rate measurement from a short-time color facial video.|64|242.0|2021|1.0|None|None|20.0|https://competitions.codalab.org/competitions/30855|||||medicine|computer vision|reconstruct the IBI (inter-beat-interval)  curve from facial videos, which can be then processed to achieve detailed cardiac activity analysis. Track2: respiration measurement from facial videos|image generation, regression|||mean of IBI|MAE, RMSE, R|True|True|True|False||[0.0]
MOCHA@ICMI2021|MOCHA - Multimodal cOntent annotation CHAllenge|54|13.0|2021|2.0|None|None|14.0|https://competitions.codalab.org/competitions/31432|||||multimedia|computer vision|automated detection of questionable comic mischief in videos. By comic mischief, we mean the appearance of mild harm inflicted on video characters in a manner that is intended to be humorous or funny|binary classification|||F2 score|F2, precision, recall|False|True|True|False||[1.057720819590049]
CODI-CRAC 2021 Shared-Task: Anaphora Resolution in Dialogues|Anaphora resolution in Dialogues from different domains.|74|233.0|2021|8.0|None|None|117.0|https://competitions.codalab.org/competitions/30312|||||linguistics|NLP|anaphora resolution in dialogue||||Anaphora Resolution (CoNLL)|CoNLL Avg. F1|False|True|True|False||[0.0]
Triangular MT: Using English to improve Russian-to-Chinese machine translation|A competition to automatically translate text from Russian to Chinese using Chinese, English and Russian bi-lingual data.|37|100.0|2021|3.0|None|None|108.0|https://competitions.codalab.org/competitions/30446|||||linguistics|NLP|Using English to improve Russian-to-Chinese machine translation|machine translation|||BLEU|BLEU|False|True|True|False||[1.2410071942446044]
Multi-Disease, Multi-View & Multi-Center Right Ventricular Segmentation in Cardiac MRI (M&Ms-2)|The M&Ms-2 challenge.|115|674.0|2021|2.0|None|None|74.0|https://competitions.codalab.org/competitions/31559|||||medicine|computer vision|ventricular segmentation|multi-class classification|||HD|Hausdorff Distance, Dice coefficient|True|False|True|False||[0.0]
Vision For Vitals (V4V)|In conjunction with ICCV 2021|55|203.0|2021|3.0|None|None|71.0|https://competitions.codalab.org/competitions/31978|||||medicine|computer vision|predict Heart Rate (HR) and Respiration Rate (RR)|regression|||MAE|MAE, RMSE, Pearson Correlation Coefficient|True|True|True|False||[0.0]
ML Based Electrical Substation extraction from Images|MACHINE LEARNING BASED FEATURE EXTRACTION OF ELECTRICAL SUBSTATIONS FROM SATELLITE DATA USING OPEN-SOURCE TOOLS|73|85.0|2021|3.0|None|None|148.0|https://competitions.codalab.org/competitions/32132|||||aerial|computer vision|feature extraction of Electrical Substations from Satellite Data|feature extraction|||IoU|IoU|False|False|True|True||[1.190537908383231]
The 1st Occluded Video Instance Segmentation (OVIS) Challenge in conjunction with ICCV 2021|OVIS (short for Occluded Video Instance Segmentation) is a new large scale benchmark dataset for video instance segmentation task. It is designed with the philosophy of perceiving object occlusions in videos, which could reveal the complexity and the diversity of real-world scenes.|324|1458.0|2021|2.0|None|None|62.0|https://competitions.codalab.org/competitions/32377|||||multimedia|computer vision|Occluded Video Instance Segmentation|multi-class classification||296k high-quality instance masks from 25 semantic categories|mAP|mAP, AP, AR|True|False|True|False||[0.0]
Criteo Privacy Preserving ML Competition @ AdKDD|Criteo Privacy Preserving ML Competition @ AdKDD|198|2572.0|2021|2.0|20000|20000|86.0|https://competitions.codalab.org/competitions/31485||https://arxiv.org/pdf/2201.13123.pdf|||online advertising, privacy|NLP|click and sales prediction models on noisy, aggregated data|regression|||Click Log Loss|logistic loss|False|True|True|False||[0.0]
Cowboy Outfit Detection Competition|Can you train a model to locate cowboy suits? This is an imbalanced cowboy suits detection problem.|128|517.0|2021|2.0|None|None|26.0|https://competitions.codalab.org/competitions/33573|||||multimedia|computer vision|detect cowboy outfits in wild images|object detection|||mAP|mAP|False|False|True|False||[1.2605434115399496]
VSPW2021|This competition is for video scene parsing in the wild.|127|781.0|2021|2.0|None|None|81.0|https://competitions.codalab.org/competitions/30712|||||multimedia|computer vision|assigning pre-defined semantic labels to pixels of all frames in a given video|multi-class classification|||mIoU|mIoU|False|False|True|True||[1.2363074337504878]
Outdoor Semantic Segmentation Challenge (DAGM GCPR 2021)|Develop a semantic segmentation algorithm for autonomous driving in unstructured environments and benchmark your results on the novel TAS500 dataset.The best results are presented at the workshop on Scene Understanding in Unstructured Environments @ DAGM GCPR 2021 on September 28, 2021.|55|210.0|2021|1.0|None|None|110.0|https://competitions.codalab.org/competitions/31086|||||multimedia|computer vision|classifying each pixel of the input image to one of the 23 semantic labels available|multi-class classification|||mIoU Score|mIoU, mean Boundary Jaccard|True|False|True|False||[1.2010269963145326]
Fincausal 2021 (Eval)|This is the eval competition for FinCausal-2021 Shared Task - Financial Document Causality Detection.|36|96.0|2021|1.0|None|None|67.0|https://competitions.codalab.org/competitions/33102|||||finance|NLP|experiment causality detection, and focuses on determining causality associated to an event|binary classification, relation extraction|||Exact match (Task 2 only)|Precision, Recall, F1, exact match|True|False|True|False||[1.0332101279531036]
Eval4NLP 2021 - Explainable Quality Estimation|Explaining quality estimation scores of translated sentences by identifying word-level translation errors|33|388.0|2021|6.0|None|None|77.0|https://competitions.codalab.org/competitions/33038||||||NLP|predicts the quality score for an input pair of source text and MT hypothesis and (ii) provides word-level evidence for its predictions as explanations|regression|||Target AUC|AUC, AP, recall|True|False|True|False||[1.3856502242152466]
Recognizing Families In the Wild Data Challenge (5th Edition) in conjunction with FG 2021|Large-scale visual kinship recognition challenge (T3 - Search & Retrieval of Missing Children).|71|195.0|2021|3.0|None|None|64.0|https://competitions.codalab.org/competitions/22152|||||multimedia|computer vision|1) Kinship verification aims to determine whether a pair of facial images are blood relatives of a certain type 2) Tri-Subject Verification focuses on a slightly different view of kinship verification– the goal is to decide whether a child is related to a pair of parents 3) Large-Scale Search and Retrieval of family members of missing children|binary classification, multi-class classification|||Average |mAP, CMC curves, rank@K|True|True|True|False||[2.033898305084746]
Multi-View Partial (MVP) Point Cloud Challenge 2021|ICCV2021 Workshop|133|540.0|2021|2.0|5000|5000|62.0|https://competitions.codalab.org/competitions/33430|||||multimedia|computer vision|1) predicting the complete 3D shape from a partially observed point cloud 2) estimating a rigid transformation to align a source point cloud to the target one|regression||100,000 high-quality scans, which renders partial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD model|CD_Loss@Com|Chamfer Distance, MSE|True|True|True|False||[1.0]
ICCV DeeperAction Challenge - FineAction Track on Temporal Action Localization|The challenge is Track 1 at ICCV DeeperAction Challenge. This track is to detect and recognise all action instances within an untrimmed video. The challenge will be carried out on the FineAction dataset. More information on the dataset can be found at https://deeperaction.github.io/fineaction.|253|153.0|2021|2.0|None|None|103.0|https://competitions.codalab.org/competitions/32363|||||multimedia|computer vision|predict the set of all actions instances|action recognition, multi-class classification|||Avg.mAP|average mAP over all classes|False|False|True|False||[2.223789398360894]
ICCV DeeperAction Challenge - MultiSports Track on Spatiotemporal Action Detection (Test Version)|The challenge is Track 2 at ICCV DeeperAction Challenge. This track is for multi-person spatiotemporal action localization in sports videos.|69|76.0|2021|2.0|None|None|103.0|https://competitions.codalab.org/competitions/33355|||||multimedia|computer vision|spatio-temporal action detection. Hence, participants should find the frames that contains actions, and where these actions occur|action recognition, regression, multi-class classification|||V@0.10:0.90|mIoU|False|False|True|False||[3.1801957851087836]
ICCV DeeperAction Challenge - Kinetics-TPS Track on Part-level Action Parsing and Action Recognition|The challenge is Track 3 at ICCV DeeperAction Challenge. This track is to recognize a human action by compositional learning of body part state in videos. The challenge will be carried out on the Kinetics-TPS dataset. More information on the dataset and downloads can be found at https://github.com/Hypnosx/Kinetics-TPS.|160|224.0|2021|2.0|None|None|103.0|https://competitions.codalab.org/competitions/32360|||||multimedia|computer vision|predict human location, body part location, part state in the frame level, and then integrate these results together to predict human action in the video level|action recognition, multi-class classification|||Score||False|False|True|False||[1.4264635950125855]
ASVspoof 2021 - Speech Deepfake track|Automatic Speaker Verification Spoofing and Countermeasures Challenge|116|1573.0|2021|4.0|None|None|109.0|https://competitions.codalab.org/competitions/32345|||||speech|computer vision|Train, develop, and tune your spoofing counter-measure (CM) system|spoofing countermeasures|||EER|equal error rate|False|False|True|False||[0.0]
ICCV'2021 ChaLearn Challenge - Behavior forecasting track.|Understanding Social Behavior in Dyadic and Small Group Interactions Challenge.|46|65.0|2021|2.0|None|None|109.0|https://competitions.codalab.org/competitions/31584|||||multimedia|computer vision|Automatic self-reported personality recognition, Behavior forecasting|action recognition, multi-class classification|||Avg. Rank|AMSE, average rank|True|True|True|False||[0.0]
ICCV'2021 ChaLearn Challenge - Automatic self-reported personality recognition track.|Understanding Social Behavior in Dyadic and Small Group Interactions Challenge.|75|207.0|2021|2.0|None|None|109.0|https://competitions.codalab.org/competitions/31326|||||multimedia|computer vision|Automatic self-reported personality recognition, Behavior forecasting|action recognition, multi-class classification|||MSE|AMSE, average rank|True|True|True|False||[0.0]
MultiLexNorm Multilingual Lexical Normalization - Dev|For this task, participants are asked to develop a system that performs lexical normalization, the conversion of non-canonical texts to their canonical equivalent form. In particular, this task includes data from 12 languages.|13|36.0|2021|1.0|None|None|28.0|https://competitions.codalab.org/competitions/34399|||||linguistics|NLP|lexical normalization, which means that replacements are done on the word level|lexical normalisation|||err|ERR|False|False|True|False||[0.0]
IEEE VIP Cup 2021: SLP Human Pose Estimation|Privacy-preserving in-bed human pose estimation in natural settings|79|766.0|2021|3.0|None|None|125.0|https://competitions.codalab.org/competitions/31489|||||multimedia|computer vision|developing robust in-bed human pose estimation algorithms|pose recognition|||pckAUC|pckAUC|False|False|True|False||[1.0775405572291974]
ForgeryNet: Face Forgery Analysis Challenge 2021|ICCV 2021 Workshop|192|671.0|2021|4.0|None|None|69.0|https://competitions.codalab.org/competitions/33386|||||multimedia|computer vision|1) Image Forgery Classification, including two-way (real / fake), three-way (real / fake with identity-replaced forgery approaches / fake with identity-remained forgery approaches), and n-way (real and 15 respective forgery approaches) classification. 2) Spatial Forgery Localization, which segments the manipulated area of fake images compared to their corresponding source real images. 3) Video Forgery Classification, which re-defines the video-level forgery classification with manipulated frames in random positions. This task is important because attackers in real world are free to manipulate any target frame. and 4) Temporal Forgery Localization, to localize the temporal segments which are manipulated|multi-class classification, binary classification||2.9 million images, 221,247 videos|AUC|AUC|False|True|True|False||[nan]
Learning By Doing NeurIPS 2021 Competition – ROBO|Optimally controlling a robot trajectory. See https://learningbydoingcompetition.github.io for more. Prizes: $3000, $2000, $1000.|129|1084.0|2021|5.0|6000|6000|73.0|https://competitions.codalab.org/competitions/33622|https://learningbydoingcompetition.github.io/||||robotics|RL|closed loop/online RL track (ROBO)|reinforcement learning|||Loss|loss|False|False|True|True||[0.0]
Learning By Doing NeurIPS 2021 Competition – CHEM|Optimally controlling a chemical reaction. See https://learningbydoingcompetition.github.io for more. Prizes: $3000, $2000, $1000.|146|783.0|2021|4.0|6000|6000|82.0|https://competitions.codalab.org/competitions/33378|https://learningbydoingcompetition.github.io/||||robotics|RL|find controls/policies to optimally interact with a target process: an open loop/bandit track (CHEM)|reinforcement learning|||Loss|loss|False|False|True|False||[0.0]
2021 VIPriors Re-identification Challenge|Saving data by adding visual knowledge priors to Deep Learning.|49|465.0|2021|2.0|None|None|88.0|https://competitions.codalab.org/competitions/33216|||||multimedia|computer vision|obtain the highest Mean Average Precision score for person re-identification|image recognition|||Mean AP|mAP|False|False|True|False||[1.3013415892672862]
2021 VIPriors Image Classification Challenge|Saving data by adding visual knowledge priors to Deep Learning.|91|171.0|2021|2.0|None|None|84.0|https://competitions.codalab.org/competitions/33214|||||multimedia|computer vision|obtain the highest Top-1 Accuracy on Imagenet dataset|multi-class classification|||Top1 accuracy|accuracy|False|False|True|False||[1.200640341515475]
2021 VIPriors Action Recognition Challenge|Saving data by adding visual knowledge priors to Deep Learning.|45|94.0|2021|2.0|None|None|81.0|https://competitions.codalab.org/competitions/33420|||||multimedia|computer vision|action recognition|action recognition|||Top1 accuracy|accuracy|False|False|True|False||[0.0]
2021 VIPriors Instance Segmentation Challenge|Saving data by adding visual knowledge priors to Deep Learning.|115|242.0|2021|2.0|None|None|89.0|https://competitions.codalab.org/competitions/33340|||||multimedia, sports|computer vision|segment basketball players and the ball on images recorded of a basketball court|instance segmentation|||AP @ 0.50:0.95|AP|False|False|True|False||[1.8389679715302492]
2021 VIPriors Object Detection Challenge|Saving data by adding visual knowledge priors to Deep Learning.|171|674.0|2021|2.0|None|None|84.0|https://competitions.codalab.org/competitions/33222|||||multimedia|computer vision|detect bike parts|object detection|||AP @ 0.50:0.95|AP|False|False|True|False||[1.5210435137063079]
The 4th Look Into Person (LIP) Challenge - Track 2 Video Multi-Person Human Parsing Challenge|The video multi-person human parsing track of the 4th LIP Challenge|47|208.0|2020|1.0|None|None|588.0|https://competitions.codalab.org/competitions/23433|||||multimedia|computer vision|video instance human parsing|multi-class classification||404 videos covering various scenarios|rank|mIoU, APrvol, mAP|True|False|True|False||[1.2323691907702308]
Multi-camera Multiple People Tracking (MMP-Tracking) Challenge|ICCV2021 Multi-camera Multiple People Tracking (MMP-Tracking) Challenge evaluation.|86|834.0|2021|2.0|None|None|60.0|https://competitions.codalab.org/competitions/33729|||||multimedia|computer vision|Multiple object tracking|object detection|||MOTA|IDF1, MOTA|True|True|True|False||[1.0909090909090908]
ICCV 2021 Workshop SSLAD Track 2 - 3D Object Detection|ICCV 2021 Workshop: Self-supervised Learning for Next-Generation Industry-level Autonomous Driving|139|319.0|2021|2.0|20000|20000|93.0|https://competitions.codalab.org/competitions/33236|||||autonomous driving|computer vision|object detection|object detection||1 million point clouds and 7 million images||mAPH|False|False|True|False||[1.2136546453201422]
WebFace260M Track of ICCV21-MFR|The Masked Face Recognition Challenge & Workshop(MFR), International Conference on Computer Vision (ICCV) 2021.|224|1883.0|2021|2.0|None|None|117.0|https://competitions.codalab.org/competitions/32478|||||multimedia, covid|computer vision|Masked Face Recognition|face recognition|||All-Masked (MFR)|MFR, SFR|True|False|True|False||[0.0]
The Second DiCOVA Challenge|This is a competition to test the DiCOVA competition bundle system. It should be able to create a competition from this bundle.|47|441.0|2021|4.0|None|None|27.0|https://competitions.codalab.org/competitions/34801|||||speech, covid|signal processing|Can COVID-19 be detected from the cough, breathing and speech sound signals of an individual|regression|||TEST AUC|AUC|False|False|True|False||[1.0514427020793455]
ICCV 2021 Workshop SSLAD Track 3B - Continual Object Detection|ICCV 2021 Workshop: Continual Learning for Next-Generation Industry-level Autonomous Driving|58|80.0|2021|2.0|10000|10000|73.0|https://competitions.codalab.org/competitions/33993|||||autonomous driving|computer vision|continual object detection|object detection||||mAP|False|False|True|False||[1.1883263815054113]
ICCV 2021 Workshop SSLAD Track 3A - Continual Object Classification|ICCV 2021 Workshop: Continual Learning for Next-Generation Industry-level Autonomous Driving|51|83.0|2021|2.0|10000|10000|73.0|https://competitions.codalab.org/competitions/33830|||||autonomous driving|computer vision|continual object classification|multi-class classification||||AMCA|False|False|True|False||[1.48832082836168]
NeurIPS 2021 Visual Domain Adaptation (VisDA-21) UniDA Challenge|In addition to input distribution shift, the target data may also have missing and/or novel classes.|150|326.0|2021|2.0|None|None|109.0|https://competitions.codalab.org/competitions/33396|||||multimedia|computer vision|adapt to novel test distributions in an open-world setting||||< Rank >|accuracy, AUROC|True|False|True|False||[0.0]
ALTA Shared Task 2021|Automatic Grading of Evidence - 10 years later|24|33.0|2021|2.0|None|None|103.0|https://competitions.codalab.org/competitions/33739|||||medicine|NLP|automatic evidence grading system for evidence-based medicine|multi-class classification|||Accuracy |accuracy|False|False|True|False||[1.0868682768418916]
Thailand Machine Learning for Chemistry Competition 2021|Predicting gas capture ability of metal-organic frameworks (MOFs).|433|6066.0|2021|3.0|None|None|28.0|https://competitions.codalab.org/competitions/34540|||||chemistry|NLP|Predicting The Gas Capture Ability|regression|||Log of MAE score|log MAE|False|False|True|False||[0.0]
E-challenge|A supervised learning competition for predictive maintenance of Electrical submersible pumps.|34|389.0|2021|2.0|None|None|60.0|https://competitions.codalab.org/competitions/34379|||||oil&gas|NLP|Failure or Reconditioning event will occur based on the telemetry and production information of Electrical Submersible Pumps|binary classification|||Score|accuracy|False|False|True|False|https://gitlab.com/spe.ec/e-challenge-prediction-models-for-historical-data-of-esp-patterns|[-0.0]
Real DSR 2021 Challenge|Real World Depth Map Super-Resolution on the RGB-D-D Dataset|61|134.0|2021|3.0|None|None|79.0|https://competitions.codalab.org/competitions/34235|||||multimedia|computer vision|design depth map SR models that can suit the real-world depth map SR task|image transformation||1586 portraits, 380 plants, 249 models from RGB-D-D dataset|RMSE |size, RMSE|True|False|True|True||[0.0]
The 5th Whole Brain Architecture Hackathon|The 5th Whole Brain Architecture Hackathon Hackathon focussing on the topic of working memory (WM).|6|11.0|2020|1.0|None|None|625.0|https://competitions.codalab.org/competitions/29849|||||multimedia|RL|develop biologically inspired models of Working Memory and demonstrate their capabilities in a set of psychology tests routinely used on humans and animals|reinforcement learning|||Total Score|DM2S, M2S|True|False|True|False||[1.038379539100115]
ICON2021 Shared Task on Multilingual Gender Biased and Communal Language Identification (ComMA@ICON)|Given a comment/text, determine the level of aggression, presence/absence of gender bias and presence/absence of communal bias.|70|295.0|2021|2.0|None|None|37.0|https://competitions.codalab.org/competitions/35482|||||linguistics|NLP|Multilingual Gender Biased and Communal Language Identification: each sample will be required to be classified as aggressive, gender biased or communally charged|multi-class classification|||<Rank>|instance-F1, micro-F1|True|False|True|False||[0.0]
SDU@AAAI-22 - Shared Task 1, Acronym Extraction|"Acronym Extraction competition is one of the shared tasks of ""the AAAI-22 Workshop on Scientific Document Understanding"". This task aims to identify acronyms and their long-forms in English, French, Spanish, Danish, Persian, or Vietnamese documents. The participants are invited to present their work at SDU@AAAI-22 workshop."|59|549.0|2021|14.0|None|None|62.0|https://competitions.codalab.org/competitions/34925|||||linguistics, science, legal|NLP|identify acronyms and their meanings (i.e.,long-forms) from the documents|binary classification||4,000 English, 1,000 Persian, and 800 Vietnamese paragraphs in the scientific domain and 4,000 English, 8,000 French, 6,400 Spanish, and 3,000 Danish paragraphs in the legal domai|F1|macro-averaged precision, recall, and F1|True|False|True|False||[1.2242152466367713]
SDU@AAAI-22 - Shared Task 2, Acronym Disambiguation|"Acronym Disambiguation competition is one of the shared tasks of ""the AAAI-22 Workshop on Scientific Document Understanding"". This task aims to find the correct long-form of an ambiguous acronym with multiple potential long-forms in English, French and Spanish text. The participants are invited to present their work at SDU@AAAI-22 workshop."|44|490.0|2021|8.0|None|None|62.0|https://competitions.codalab.org/competitions/34899|||||linguistics, science, legal|NLP|find the correct meaning of an ambiguous acronym in a given sentence|multi-class classification||457 English Scientific, 273 English legal, 493 Spanish, and 609 French acronyms|F1|macro-averaged precision, recall, and F1|True|False|True|False||[1.558219178082192]
Approximate Inference in Bayesian Deep Learning|Provide the best approximate inference for Bayesian Neural Networks.|34|341.0|2021|3.0|None|None|128.0|https://competitions.codalab.org/competitions/35838|||||multimedia|computer vision|evaluate the fidelity of approximate inference procedures across a range of tasks, including image recognition, regression, covariate shift, and medical applications, such as diagnosing diabetic retinopathy|multi-class classification, regression|||Avg Score|agreement, total variation distance, Wasserstein-2 distance|True|False|True|True||[0.0]
IGLU NeurIPS 2021 Competition: Silent Builder|IGLU Silent Builder|38|67.0|2021|1.0|7000|7000|127.0|https://competitions.codalab.org/competitions/33828||||||RL|Interactive Grounded Language Understanding in a Collaborative Environment. The primary goal of the competition is to approach the problem of how to build interactive agents that learn to solve a task while provided with grounded natural language instructions in a collaborative environment|reinforcement learning|||Completion Rate|completion rate, success rate, episode reward|True|False|True|True||[1.3213822894168465]
IGLU NeurIPS 2021 Competition: Architect|In this task, our goal is to develop an Architect that can generate appropriate step instructions based on the observations of the environment and the Builder’s behavior.|37|22.0|2021|1.0|7000|7000|127.0|https://competitions.codalab.org/competitions/33998||||||RL|develop an Architect that can generate appropriate step instructions based on the observations of environment and the Builder’s behavior|reinforcement learning|||BLEU (test)|BLEU|False|False|True|True||[0.0]
MultiLexNorm Multilingual Lexical Normalization|For this task, participants are asked to develop a system that performs lexical normalization, the conversion of non-canonical texts to their canonical equivalent form. In particular, this task includes data from 12 languages.|18|83.0|2021|1.0|None|None|103.0|https://competitions.codalab.org/competitions/34355|||||linguistics|NLP|lexical normalization, which means that replacements are done on the word level|lexical normalisation|||err|ERR|False|False|True|False||[1.2058564359791228]
Short-duration Speaker Verification (SdSV) Challenge 2021 - Task 2 : Text-Independent|Evaluate New Technologies in Short Duration Scenarios|69|1139.0|2021|2.0|None|None|352.0|https://competitions.codalab.org/competitions/28269|||||speech|signal processing|speaker verification in text-independent mode: given a test segment of speech and the target speaker enrollment data, automatically determine whether the test segment was spoken by the target speaker|binary classification||text-independent Persian utterances from 588 speakers|MinDCF|normalized minimum Detection Cost Function, ERR|True|False|True|False||[0.0]
Short-duration Speaker Verification (SdSV) Challenge 2021 - Task 1 : Text-Dependent|Evaluate New Technologies in Short Duration Scenarios|52|590.0|2021|2.0|0|0|352.0|https://competitions.codalab.org/competitions/28190|||||speech|signal processing, NLP|speaker verification in text-dependent mode: given a test segment of speech and the target speaker's enrollment data, automatically determine whether a specific phrase and the test segment was spoken by the target speaker. In contrast to text-independent speaker verification, the lexical content of the utterance is also taken into consideration. As such, Task 1 is a twofold verification task in which both the speaker and phrase are verified.|binary classification||text-independent Persian utterances from 588 speakers|MinDCF|normalized minimum Detection Cost Function, ERR|True|False|True|False||[0.0]
HASOC-Dravidian-CodeMix-FIRE 2021|Offensive Language Identificaiton for Davidian Languages in Code-Mixed Text|122|15.0|2020|1.0|None|None|559.0|https://competitions.codalab.org/competitions/31146|||||linguistics|NLP|offensive language detection of code-mixed text in Dravidian languages (Malayalam-English and Tamil-English)|binary classification||||F1|False|False|True|False||[0.0]
ADD 2022 - Audio fake game (FG)|including an audio generation task and an audio fake detection task.|76|2696.0|2021|2.0|None|None|19.0|https://competitions.codalab.org/competitions/36190|http://addchallenge.cn/||||speech|signal processing|fake generation and detection|signal generation, binary classification|||EER|DSR, ERR|True|True|True|False||[0.0]
ADD 2022 - Partially fake audio detection(PF)|comprising bona fide and partially fake utterances generated by manipulated the original bona fide utterances with real or synthesized audio.|92|796.0|2022|1.0|None|None|8.0|https://competitions.codalab.org/competitions/36189|http://addchallenge.cn/||||speech|signal processing|fake detection|binary classification|||EER|ERR|False|False|True|False||[0.0]
ADD 2022 - Low-quality fake audio detection(LF)|The well known dataset.|111|2098.0|2022|1.0|None|None|8.0|https://competitions.codalab.org/competitions/36183|http://addchallenge.cn/||||speech|signal processing|fake detection|binary classification|||EER|ERR|False|False|True|False||[0.0]
XMRec WSDM Cup|WSDM Cup 2022 on Cross-Market Product Recommendation.E-commerce companies often operate across markets, for instance, Amazon has expanded their operations and sales to 18 markets (i.e. countries) around the globe. The cross-market recommendation concerns the problem of recommending relevant products to users in a target market (e.g., a resource-scarce market) by leveraging data from similar high-resource markets, e.g. using data from the U.S. market to improve recommendations in a target market.    The key challenge, however, is that data, such as user interaction data with products (clicks, purchases, reviews), convey certain biases of the individual markets. Therefore, the algorithms trained on a source market are not necessarily effective in a different target market. Despite its significance, small progress has been made in cross-market recommendation, mainly due to a lack of experimental data for the researchers. In this WSDM Cup challenge, we provide user purchase and rating data on various markets, enriched with review data in different languages, with a considerable number of shared item subsets. The goal is to improve individual recommendation systems in these target markets by leveraging data from similar auxiliary markets.|334|1809.0|2021|1.0|3500|3500|109.0|https://competitions.codalab.org/competitions/36050|||||ecommerce|NLP|improve individual recommendation systems in these target markets by leveraging data from similar auxiliary markets|multi-class classification|||T1+T2 nDCG@10 (test)|nDCG@10|False|False|True|False||[1.2752955123958298]
L2RPN ICAPS 2021 - new entrant league - closed|Train controllers to conduct a power grid for as long as possible while and alert the operator in case of planned incidents.|64|159.0|2021|4.0|0|0|212.0|https://competitions.codalab.org/competitions/33122|||||electricity|RL|develop your agent to be robust to unexpected network events and maintain reliable electricity everywhere on the network, especially when the network is under stress from external events|reinforcement learning|||score||||True|True||[0.4216002344665885]
SemEval-2022 Task 7 Identifying Plausible Clarifications of Implicit and Underspecified Phrases|SemEval-2022 Shared task on identifying plausible clarifications in WikiHow instructions.|72|259.0|2021|3.0|None|None|150.0|https://competitions.codalab.org/competitions/35210|||||linguistics|NLP|The goal is to predict a class label (IMPLAUSIBLE, NEUTRAL, PLAUSIBLE) given the clarification and its context. The goal is to predict the plausibility score on a scale from 1 to 5 given the clarification and its context.|multi-class classification, ranking|||Ranking Score, Accuracy Score|accuracy, Spearman's rank correlation coefficient|True|True|True|False||[2.817924842035251]
CMCL2022|CMCL 2022 Shared Task: Multilingual and crosslingual prediction of human reading behavior|45|27.0|2021|4.0|None|None|59.0|https://competitions.codalab.org/competitions/36415|||||linguistics|NLP|predicting eye-tracking features recorded during sentence processing of multiple languages. The shared task if formulated as a regression task to predict 2 eye-tracking features and the corresponding standard deviation across readers|regression|||MAE|MAE|False|True|True|False||[0.0]
Zero Resource Speech Challenge 2021|ZeroSpeech 2021|36|37.0|2020|1.0|None|None|813.0|https://competitions.codalab.org/competitions/27711|https://zerospeech.com/2021/track_s.html||||speech|signal processing|build language models without using any textual input|text generation||||Phonetics (Libri-light ABX), Lexicon (sWUGGY spot-the-word), Syntactic level (sBLIMP syntactic acceptability),  Semantic level (sSIMI similarity score)|True|True|True|False||[0.0]
Image SkelNetOn - CVPR 2022|Image SkelNetOn|72|68.0|2022|2.0|None|None|39.0|https://competitions.codalab.org/competitions/24536|||||biology, nature|computer vision|detect skeletons from natural images: binary classification problem to detect the skeleton pixels for an object in a given natural RGB image|binary classification|||Prediction score|F1|False|False|True|True||[nan]
Parametric SkelNetOn - CVPR 2022|Parametric SkelNetOn|67|47.0|2022|2.0|None|None|39.0|https://competitions.codalab.org/competitions/21175|||||biology, nature|computer vision|output the skeleton of the shape defined by its parametric curves, together with a radius function|object recognition|||Prediction score|combined MSD and MBE|False|False|True|True||[0.0]
Point SkelNetOn - CVPR 2022|Point SkelNetOn|73|97.0|2022|2.0|None|None|39.0|https://competitions.codalab.org/competitions/21172|||||biology, nature|computer vision|binary classification problem to assign a skeleton/non-skeleton class to all points in the given point cloud|binary classification|||Prediction score|Chamfer Distance|False|False|True|True||[nan]
Pixel SkelNetOn - CVPR 2022|Pixel SkelNetOn|140|1013.0|2022|2.0|None|None|39.0|https://competitions.codalab.org/competitions/21169|||||biology, nature|computer vision|binary classification problem to detect the skeleton pixels for a given shape image|binary classification|||Prediction score|F1|False|False|True|True||[1.0178272313950705]
2th CSEDM Challenge - Task 1: Knowledge Tracing|Tracking student programming traces and predicting their struggles in late problems.|95|1811.0|2021|3.0|None|None|307.0|https://competitions.codalab.org/competitions/33156|https://sites.google.com/ncsu.edu/csedm-dc-2021/||||education|NLP|predict students' learning outcomes in CS/programming classrooms, based on their submissions to and performance on past programming problems|regression|||AUC|AUC, F1, accuracy|True|False|True|False||[1.045752125223005]
2th CSEDM Challenge -- Task 2: Student Grade Prediction|Using student programming traces to predict their course performance.|95|2111.0|2021|3.0|None|None|307.0|https://competitions.codalab.org/competitions/33164|||||education|NLP|Student Grade Prediction|regression|||MSE|MSE|False|False|True|False||[0.0]
TinyAction Challenge [CVPR 2021]|Focused on recognizing tiny actions in videos, in conjunction with ActivityNet Workshop at CVPR 2021.|47|441.0|2021|1.0|None|None|364.0|https://competitions.codalab.org/competitions/31843|||||multimedia|computer vision|tiny action recognition|action recognition|||F1|class-wise precision, recall, and F1|True|False|True|False||[1.4029435767778276]
Detecting Signs of Depression from Social Media Text-LT-EDI@ACL 2022|Shared task on Detecting Signs of Depression from Social Media Text at Second Workshop on Language Technology for Equality, Diversity, Inclusion (LT-EDI-2022) @ACL 2022|174|16.0|2021|1.0|None|None|206.0|https://competitions.codalab.org/competitions/36410|||||linguistics, multimedia|NLP|detect the signs of depression of a person from their social media postings wherein people share their feelings and emotion|multi-class classification|||correct|macro averaged Precision, macro averaged Recall and macro averaged F-Score across all the classes|True|False|True|True||[0.0]
Emotion Analysis in Tamil-ACL 2022|Shared Task on Emotion Analysis in Tamil-DravidianLangTech@ACL 2022|121|15.0|2021|1.0|None|None|206.0|https://competitions.codalab.org/competitions/36396|||||linguistics, multimedia|NLP|identify whether a given comment contains which emotions|multi-class classification|||correct|macro averaged Precision, macro averaged Recall and macro averaged F-Score across all the classes|True|False|True|True||[0.0]
Hope Speech Detection for Equality, Diversity, and Inclusion -ACL 2022|Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion (English, Tamil, Spanish, Kannada, and Malayalam) (LT-EDI-2022) @ACL 2022|134|20.0|2021|1.0|None|None|206.0|https://competitions.codalab.org/competitions/36393|||||speech|signal processing|Given a Youtube comment, the systems submitted by the participants should classify it into 'Hope speech' and 'Not hope speech'|binary classification||||macro averaged Precision, macro averaged Recall and macro averaged F-Score across all the classes|True|False|True|True||[0.0]
ICCV 2021 Workshop SSLAD Track 1 - 2D object detection|ICCV 2021 Workshop: Self-supervised Learning for Next-Generation Industry-level Autonomous Driving|219|1100.0|2021|2.0|None|None|457.0|https://competitions.codalab.org/competitions/33288|||||autonomous driving|computer vision|2D object detection task with limited annotations and massive unlabeled images|object detection|||meanAP|mAP|False|False|True|False||[1.2478344270570418]
L2RPN NEURIPS 2020 - Robustness Track|Train controllers to conduct a power grid for as long as possible while avoiding incidents.|273|3150.0|2020|4.0|15000|15000|875.0|https://competitions.codalab.org/competitions/25426|||||electricity|RL|develop your agent to be robust to unexpected events and keep delivering reliable electricity everywhere even in difficult circumstances|reinforcement learning|||score||||True|True||[2.274561317517891]
L2RPN NEURIPS 2020 - Adaptability Track|Train controllers to conduct a power grid with various energy mixes.|169|1468.0|2020|4.0|15000|15000|875.0|https://competitions.codalab.org/competitions/25427|||||electricity|RL|develop your agent to adapt to new energy productions in the grid with an increasing share of less controllable renewable energies over years|reinforcement learning|||Score||||True|True||[6.059188420780563]
EPIC-KITCHENS-55 Object Detection|Detect objects in given frames from the EPIC-KITCHENS-55 dataset.|85|384.0|2020|1.0|None|None|800.0|https://competitions.codalab.org/competitions/20111||||||computer vision|Detect and classify bounding boxes of objects in video frames from seen and unseen kitchens|regression, multi-class classification|||IoU > 0.5|mAP|False|False|True|False||[0.0]
3D Poses in the Wild (3DPW) Challenge|This is the first challenge of 3D human pose estimation in the wild, including people recorded with a moving camera performing activities in challenging natural scenes.Given a single image, the task is to estimate the 3D pose of the person.|98|222.0|2020|2.0|None|None|974.0|https://competitions.codalab.org/competitions/24938||||||computer vision|track the person and estimate the 3D pose of the person|object recognition, multi-class classification|||Rank, MPJPE |Rank, MPJPE, MPJPE_PA, PCK, AUC, MPJAE, MPJAE_PA|True|False|True|False||[0.0]
The Algonauts Project 2021 Challenge - Full Track (Whole Brain)|How the Human Brain Makes Sense of a World in Motion|54|370.0|2021|1.0|None|None|610.0|https://competitions.codalab.org/competitions/30937|||||brain|computer vision|predict human brain responses to a set of 3-second videos in selected voxels across the whole brain using computational models|regression|||Score|Avg Noise-Normalized Correlation Across All Voxels|False|False|True|True||[0.0]
UG2+ Track1.2 - DarkFace Detection (Still in testing to provide testing results)|Face Detection in the Low-Light Condition for track1.2 challenge, UG2+ workshop, CVPR 2021|49|508.0|2021|2.0|None|None|850.0|https://competitions.codalab.org/competitions/32499||||||computer vision||face detection|||mAP|mAP|False|False|True|True||[1.4862772695285011]
SIW 2021: Official ICDAR Competition on Script Identification in the Wild|ICDAR 2021 Official Competition|24|202.0|2020|2.0|None|None|1095.0|https://competitions.codalab.org/competitions/30594|||||linguistics|NLP|script identification|multi-class classification|||Task 1 - Handwritten (Accuracy %), Task 2 - Printed (Accuracy %), Task 3 - Mixed (Accuracy %)|accuracy|False|False|True|True||[0.0]
L2RPN Sandbox, on the case14 file|Train controlers to conduct a power grid for as long as possible while avoiding incidents.|190|304.0|2020|1.0|None|None|1806.0|https://competitions.codalab.org/competitions/24493|||||electricity|RL|controlling existing grids is becoming increasingly difficult, forcing grid operators to do “more with less”|reinforcement learning|||score||||True|True||[-2.626508995124138]
HANDS19 Challenge: Task 1 - Depth-Based 3D Hand Pose Estimation|This task builds on BigHand2.2M dataset in a similar format to HANDS 2017 challenge. Some hand shapes, articulations and viewpoints are strategically excluded from the training set in order to measure interpolation and extrapolation capabilities of submissions. No objects appear in this task. Hands appear in both 3rd person and egocentric viewpoints.|94|3261.0|2019|1.0|None|None|2271.0|https://competitions.codalab.org/competitions/20913||||||computer vision|assess the performance of state of the art approaches in terms of interpolation-extrapolation capabilities of hand variations in their main four axes (shapes, articulations, viewpoints, objects), and the use of synthetic data to fill the gaps of current datasets on these axes: predictthe 21 joints’ 3D locations for each given image|regression|||EXTRAP. |mean joint error|True|True|True|False||[0.0]
AIM 2019 Demoireing Challenge - Track 2: Perceptual|Based on a novel dataset we propose a image demoireing challenge to promote research on this topic.|103|65.0|2019|2.0|None|None|2356.0|https://competitions.codalab.org/competitions/20166||||||computer vision|image demoireing|image transformation|||PSNR|Peak Signal To Noise Ratio (PSNR), Structural Similarity (SSIM) index|True|True|True|False||[1.132734836873221]
PuzzLing Machines|"Automatically solving linguistic translation puzzles - a ""learning from small data"" challenge"|16|23.0|2020|2.0|None|None|2100.0|https://competitions.codalab.org/competitions/24414|||||linguistics|NLP|systems that can provide state-of-the-art results by exploiting big data but can also learn from small data|machine translation, meta-learning|||EM|BLEU-2, CharacTER, ChrF-3 and exact match|True|False|True|False||[0.0]
AIM 2019 Demoireing Challenge - Track 1: Fidelity|Based on a novel dataset we propose a image demoireing challenge to promote research on this topic.|216|147.0|2019|2.0|None|None|2356.0|https://competitions.codalab.org/competitions/20165|||||multimedia|computer vision|removing the moire effect from an input image based on a set of examples of images with and without the moire effect|image transformation|||PSNR|Peak Signal To Noise Ratio (PSNR), Structural Similarity (SSIM) index|True|True|True|False||[1.1665307073767626]
SVC-onGoing (based on ICDAR 2021 Competition on On-Line Signature Verification)|Organized by Biometrics and Data Pattern Analytics - BiDA Lab (UAM)|112|253.0|2021|2.0|None|None|3289.0|https://competitions.codalab.org/competitions/27295|||||biometrics|computer vision|carry out a benchmark evaluation of the latest on-line signature verification technology using large-scale public databases and both traditional office-like scenarios (pen stylus), but also the challenging mobile scenarios with signatures performed using the finger over a touch screen||||ERR(%)|ERR|False|False|True|False||[0.0]
COCO Stuff Segmentation Challenge|This challenge is designed to push the state of the art in semantic segmentation of stuff classes.|76|100.0|2018|3.0|None|None|11508.0|https://competitions.codalab.org/competitions/19472|||||multimedia|computer vision|stuff detection|multi-class classification|||FIOU |Mean Intersection-Over-Union, Frequency Weighted Intersection-Over-Union, Mean Accuracy, Pixel Accuracy|True|True|True|False||[1.2285866288348999]
SensatUrban Semantic Segmentation|Organized by Qingyong Hu (University of Oxford) and Meida Chen (University of Southern California - ICT)|215|332.0|2021|1.0|3000|3000|10491.0|https://competitions.codalab.org/competitions/31519|||||urbanisme|NLP|Therefore, the input to all evaluated methods is a list of coordinates of the three-dimensional points along with their appearance, i.e., the RGB value of each point. Each method should then output a label for each point, this is used for the final performance evaluation.|multi-class classification|||mIoU |Jaccard Index, accuracy, mean intersection-over-union (mIoU) metric over all classes|True|False|True|False||[1.1964792883553936]
SemEval-2017 Task 7, Subtask 1|Detection of English Puns|42|45.0|2016|3.0|None|None|12094.0|https://competitions.codalab.org/competitions/15705|||||linguistics|NLP|classify each context according to whether or not it contains a pun|binary classification|||precision|precision, recall, accuracy, F1|True|False|True|False||[1.1040641331371455]
The 2017 Hands in the Million Challenge on 3D Hand Pose Estimation|This is the submission page of the 2017 Hands in the Million Challenge on 3D Hand Pose Estimation, instructions can be found in the webpage http://icvl.ee.ic.ac.uk/hands17/challenge/.Hand-Object task available here: https://competitions.codalab.org/competitions/17452|128|2208.0|2017|3.0|None|None|11917.0|https://competitions.codalab.org/competitions/17356|http://icvl.ee.ic.ac.uk/hands17/challenge/|https://arxiv.org/abs/1707.02237; https://arxiv.org/abs/1712.03917||https://arxiv.org/abs/1704.02612; https://guiggh.github.io/publications/first-person-hands/|multimedia|computer vision|assess how far is the state of the art in terms of solving the problem of 3D hand pose estimation as well as detect major failure and strength modes of both systems and evaluation metrics that can help to identify future research directions: The system should be able to predict the 21 joints’ 3D locations for each image|regression|||AVG|mean error|False|True|True|False||[0.0]
SemEval-2020 Task5: Modelling Causal Reasoning in Language: Detecting Counterfactuals||224|1437.0|2019|6.0|None|None|11336.0|https://competitions.codalab.org/competitions/21691|||||linguistics|NLP|determine whether a given statement is counterfactual or not, locate antecedent and consequent in counterfactuals|binary classification, regression|||F1|Exact Match, Precision, Recall, and F1|True|True|True|False||[1.30859208847299]
BioRelEx Benchmark 1.0|Biological relation extraction from raw text.|44|225.0|2019|2.0|None|None|11506.0|https://competitions.codalab.org/competitions/20468|||||biology|NLP|learn end-to-end relation extraction from raw sentences|||1405 sentences are for training, another 201 sentences are for validation|Relation Extraction (all) F-score|F1|False|False|True|True||[1.5016701101928374]
Video-guided Machine Translation Challenge 2020|Benchmark for video-guided machine translation, aiming to translate source language into target language with video information as the additional context.|47|131.0|2020|1.0|None|None|28753.0|https://competitions.codalab.org/competitions/24384||||||computer vision, NLP||machine translation||41,250 videos and 825,000 captions in both English and Chinese. Among the captions, there are over 206,000 English-Chinese parallel translation pairs|Corpus Bleu-4|different Bleu-k|True|False|True|False||[1.1394769613947697]
DE2021: Russian News Clustering and Headline Selection - Clustering|Evaluation of different text clustering algorithms|50|429.0|2021|1.0|None|None|28453.0|https://competitions.codalab.org/competitions/28830|||||news|NLP|clustering, headline selection, and headline generation|text generation|||F-score |F1|False|True|True|False||[1.049298985523766]
VATEX Video Captioning Challenge|Benchmark for (multilingual) video captioning, aiming to automatically describe the activities in videos with various languages.|63|260.0|2020|4.0|None|None|28754.0|https://competitions.codalab.org/competitions/24360|||||multimedia|computer vision, NLP|describing the video content with natural language|multi-class classification, text generation|||CIDEr|BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L, METEOR, CIDEr|True|True|True|False||[1.4920634920634919]
TabFact Competition|TabFact Competition requires the model to verify whether a textual statement is supported by the evidence in the semi-structured tables.|35|25.0|2019|1.0|None|None|29365.0|https://competitions.codalab.org/competitions/21611|https://tabfact.github.io/||||linguistics|NLP|understand a table and verify whether a given textual statement is entailed by the given table|binary classification|||Accuracy |accuracy|False|False|True|False||[1.1414634146341465]
HybridQA Competition|Hybrid Question Answering with both Tabular and Textual Information as Evidence|24|116.0|2020|1.0|None|None|29113.0|https://competitions.codalab.org/competitions/24420|||||linguistics|NLP|read a table along with its hyperlinked passages to answer a multi-hop question|multi-class classification, text generation|||EM|EM, F1|True|False|True|False||[1.1683623150791589]
OTT-QA Competition|The real-world data is distributed in different forms like unstructured text and semi-structured tables. In this dataset, we challenge the model's capability to integrate heterogeneous information (table + text) from the web to answer open-domain questions.|16|33.0|2020|1.0|None|None|28923.0|https://competitions.codalab.org/competitions/27324|https://ott-qa.github.io/||||linguistics|NLP|retrieve tables and text from web to answer questions|multi-class classification, text generation|||EM|EM, F1|True|False|True|False||[1.2844983234459635]
TweetQA Competition|TweetQA task requires model to read a short tweet and a question and outputs a text phrase (does not need to be in the tweet) as the answer.|69|419.0|2019|1.0|None|None|29384.0|https://competitions.codalab.org/competitions/20307|https://tweetqa.github.io/||||linguistics, tweets|NLP|read a short tweet and a question and outputs a text phrase (does not need to be in the tweet) as the answer|multi-class classification, text generation|||BLEU-1|BLEU-1, METEOR, and ROUGE-L|True|False|True|False||[1.1194029850746272]
DE2021: Russian News Clustering and Headline Generation - Headline generation|Evaluation of different text clustering and summarization algorithms|7|15.0|2021|1.0|None|None|28783.0|https://competitions.codalab.org/competitions/29905|||||news|NLP|Headline generation|text generation|||ROUGE |(ROUGE-1 + ROUGE-2 + ROUGE-L) / 3|False|False|True|False||[1.2317708333333335]
Task 10 - SemEval-2020|Emphasis Selection for Written Text in Visual Media|225|777.0|2019|4.0|None|None|226.0|https://competitions.codalab.org/competitions/20815|||||linguistics|NLP|model emphasis: design automatic methods for emphasis selection, i.e. choosing candidates for emphasis in short written text, to enable automated design assistance in authoring|binary classification|||Rank|MatchM|False|False|True|False||[1.0751683247914783]
Classification of Normal vs Malignant Cells in B-ALL White Blood Cancer Microscopic Image: ISBI 2019|Classification of leukemic B-lymphoblast cells from normal B-lymphoid precursors from blood smear microscopic images.|228|1299.0|2018|3.0|None|None|125.0|https://competitions.codalab.org/competitions/20395|||||medicine|computer vision|Normal vs Malignant Cells in B-ALL White Blood Cancer Microscopic Image|binary classification|||Prediction score|weighted-precision, weighted-recall and weighted-f1|True|False|True|False||[1.183906814490381]
Word-level QE shared task 2018|WMT'18 Quality Estimation shared task, submission platform for predictions at word-level (task2).|15|183.0|2018|6.0|None|None|0.0|https://competitions.codalab.org/competitions/19306|||||linguistics|NLP|distinguishing between 'OK' and 'BAD' tokens|binary classification, machine translation|||F1-Multi |F1|True|False|True|False||[0.0]
VLEP Evaluation|UNC VLEP test set evaluation.|22|28.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/26881|https://arxiv.org/abs/2010.07999||||multimedia|computer vision, NLP|multimodal commonsense next-event predictions|multi-class classification, text generation|||test||||True|False||[1.0839422103616454]
SMM4H'18 - Shared Task|The SMM4H shared task proposed four NLP challenges on social media mining for health monitoring and surveillance, 1. Automatic detection of posts mentioning a drug name, 2. Automatic classification of posts describing medication intake|17|124.0|2020|2.0|None|None|0.0|https://competitions.codalab.org/competitions/27081|||||medicine|NLP|Task 1: Automatic detection of posts mentioning a drug name—binary classification. Task 2: Automatic classification of posts describing medication intake—three-class classification|binary classification, multi-class classificaiton|||F1|F1|False|True|True|False||[0.0]
SemEval 2021 Task 11: NLPContributionGraph|"Structuring the ""contributions"" information from scholarly articles to facilitate contributions-centered scientific knowledge graph building."|127|627.0|2020|5.0|None|None|169.0|https://competitions.codalab.org/competitions/25680|||||linguistics|NLP|information retrieval|information retrieval|||Rank-Avg F1|precision, recall and F1|True|False|True|False||[0.0]
DanceTrack - Tracking Multiple Objects in Uniform Appearance and Diverse Motion|DanceTrack is a benchmark for tracking multiple objects in uniform appearance and diverse motion.|38|132.0|2021|1.0|None|None|0.0|https://competitions.codalab.org/competitions/35786|https://github.com/DanceTrack/DanceTrack||||multimedia|computer vision|tracking multiple objects in uniform appearance and diverse motion|action recognition, regression||100 videos|HOTA|HOTA, CLEAR and Identity to DanceTrack|True|False|True|False||[1.2645007921327325]
Second NADI Shared Task (Subtask 1.1)|Second Nuanced Arabic Dialect Identification Shared Task (Subtask 1.1)|31|67.0|2020|3.0|None|None|54.0|https://competitions.codalab.org/competitions/27768|||||linguistics, tweets|signal processing, NLP|Dialect identification is the task of automatically detecting the source variety of a given text or speech segment|multi-class classification||21,000 tweets|F1|precision, recall, accuracy, F1|True|False|True|False||[1.2835512732278045]
Reliable Intelligence Identification on Vietnamese SNSs (ReINTEL)|This challenge aims to identify a piece of information shared on social network sites (SNSs), is reliable or unreliable. With the blazing-fast spurt of SNSs, e.g., Facebook, Zalo, or Lotus, there are approximately 65 million Vietnamese users on board with the annual growth of 2.7 million in the recent year, as reported by the Digital 2020 [6]. SNSs become essential means for users to not only connect friends but also freely create, share diverse information [2, 5], i.e., news. Within freedom, a number of users tend to spread unreliable information for their personal purposes affecting the online society. Detecting whether news spreading in SNSs is reliable or unreliable has gained significant attention recently [1, 3, 4]. Therefore, this shared task targets identifying shared information in Vietnamese SNSs. It provides an opportunity for participants who are interested in the problem, to contribute their knowledge to improve the online society for social good.|84|788.0|2020|3.0|None|None|38.0|https://competitions.codalab.org/competitions/27232|||||multimedia, linguistics|NLP|identify a piece of information shared on social network sites (SNSs), is reliable or unreliable|binary classification|||ROC|ROC-AUC|False|False|True|False||[1.0203123186400123]
Feature Selection Over Urban Growth Prediction Challenge|Challenge organized by ABEONA team of M1 AI students 2020/2021|20|93.0|2021|2.0|None|None|453.0|https://competitions.codalab.org/competitions/29119|||||urbanisme|NLP|feature selection (NP-Hard) on geographical, social and economical indicators for urban growth modelling + predict the urban growth of a country for a year having a huge amount of data about the previous year of this country|regression|||Prediction score|accuracy|False|False|True|True||[1.2609345597997916]
eHealth KD @ IberLEF 2021 (Training)|Automated knowledge extraction from electronic health documents.|14|48.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/30333|||||medicine|NLP|Entity recognition, Relation extraction||||F1|F1, precision, recall|True|True|True|False||[0.0]
FewRel 2.0 None-of-the-above|A few-shot relation classification benchmark|36|142.0|2018|1.0|None|None|0.0|https://competitions.codalab.org/competitions/27982||https://aclanthology.org/D18-1514.pdf, https://aclanthology.org/D19-1649.pdf|||linguistics|NLP||multi-class classification|||avg |N-way K-shot|True|True|True|False||[1.15813894630278]
H2O|This is a competition for action recognition and pose estimation using the H2O dataset.|9|60.0|2021|1.0|None|None|0.0|https://competitions.codalab.org/competitions/34473|https://taeinkwon.com/projects/h2o/||||multimedia|computer vision, NLP|Two Hands Manipulating Objects for First Person Interaction Recognition|action recognition, hand pose recognition|||Accuracy |accuracy, mean joint error and Percentage of Correct Keypoints (PCK)|True|True|True|False||[0.0]
Mowjaz Multi-Topic Labelling Task||48|517.0|2021|3.0|None|None|60.0|https://competitions.codalab.org/competitions/29220|||||linguistics|NLP|classify articles based on their topics|multi-class classification|||Accuracy |F1, average testing time|True|True|True|False||[1.0327028602440151]
Word-level Post-Editing Effort QE shared task 2021|WMT'21 Quality Estimation shared task, submission platform for predictions at word-level *Post-Editing Effort* (Task 2 word-level)|13|191.0|2021|11.0|None|None|0.0|https://competitions.codalab.org/competitions/33413|https://statmt.org/wmt21/quality-estimation-task.html||||linguistics|NLP|quality estimation, error detection|machine translation, multi-class classification|||MCC|MCC (Matthews correlation coefficient), F1|True|True|True|False||[0.0]
SwissText Shared Task on Hierarchical Patent Classification|Hierarchical Patent Classification with the IPC (international patent classification).|34|62.0|2020|3.0|None|None|77.0|https://competitions.codalab.org/competitions/22533|||||linguistics|NLP|Multi-lingual Hierarchical Classification of Patents|multi-class classification|||Correct Submission|F1-score over micro- and macro-averaged F1-score|True|True|True|False||[0.0]
CMCL2021|CMCL 2021 Shared Task on Predicting Human Reading Behavior|55|57.0|2021|4.0|None|None|46.0|https://competitions.codalab.org/competitions/28176|||||linguistics|computer vision, NLP|predicting eye tracking-based metrics recorded during English sentence processing|regression|||F1 Macro|MAE|False|True|True|False||[0.0]
OffensEval 2020 - English|This is the competition site for the development set of the second edition of OffensEval organized at SemEval 2020 (Task 12). SemEval 2020 will take place on September 13 and 13, 2020 co-located with COLING in Barcelona, Spain.|146|579.0|2020|3.0|None|None|2.0|https://competitions.codalab.org/competitions/22917|||||linguistics, tweets|NLP|Multilingual Offensive Language Identification in Social Media|binary classification|||F1 Macro|macro-averaged F1|False|False|True|False||[1.1949842719869006]
SemSketches competition|This is a competition that allows you to try working with semantic sketches and evaluate their illustrativeness.|6|21.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/29992|https://github.com/dialogue-evaluation/SemSketches||||linguistics|NLP|evaluate how representative and illustrative the sketches are by trying to find the necessary sketches in the given set using the context of a word without seeing the word itself|multi-class classification|||Score on the first task|accuracy|False|True|True|True||[0.0]
To be, or not to be?|Prediction of mortality given medical records.|72|130.0|2018|1.0|None|None|0.0|https://competitions.codalab.org/competitions/19365|||||medicine|NLP|predict the survival of a patient given his or her medical record|binary classification|||Accuracy |accuracy|False|False|True|False||[2.093348891481914]
The 2020 Climate Informatics Hackathon - Public Challenge|Generating nighttime satellite imagery from IR observations|23|12.0|2020|2.0|None|None|0.0|https://competitions.codalab.org/competitions/26644|https://docs.google.com/presentation/d/1Cbh5fFsd4yUcFs7DBeemqyOd7k4Ph4n6BgYjW_PMg-Y/edit#slide=id.p||||climate|computer vision|prediction of visible imagery at night using thermal infra-red (IR) observations|image transformation|||RMSE|SSIM|False|False|True|False||[1.2]
NSURL 2021: Semantic Relation Extraction in Persian||18|53.0|2021|3.0|None|None|120.0|https://competitions.codalab.org/competitions/31979|||||linguistics|NLP||relation extraction|||MACRO-averaged-F1|accuracy, macro-averaged F|True|True|True|True||[1.0]
To be, or not to be?|Prediction of mortality given medical records.|91|332.0|2018|1.0|None|None|0.0|https://competitions.codalab.org/competitions/22873|||||medicine|NLP|predict the survival of a patient given his or her medical record|binary classification|||Balanced accuracy|accuracy|False|False|True|False||[1.3839541547277936]
The VoxCeleb Speaker Recognition Challenge 2020 - Track 3 (Verification self supervised, closed)|The goal of the VoxSRC challenge is to probe how well current methods can recognize speakers from speech obtained 'in the wild'. The task of speaker verification aims to determine whether two samples of speech are from the same person.|41|39.0|2020|2.0|None|None|47.0|https://competitions.codalab.org/competitions/26215|https://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2020.html||||speech|signal processing|recognize speakers from speech obtained 'in the wild'|binary classification|||DCF|DCF, ERR|True|False|True|False||[0.0]
Linking mathematical symbols to their descriptions|Linking mathematical symbols to their descriptions|244|132.0|2021|3.0|None|None|183.0|https://competitions.codalab.org/competitions/34011|||||mathematics/science|NLP|Linking mathematical symbols to their descriptions|binary classification|||correct|precision, recall, F|True|True|True|True||[0.0]
Aerial Image Recognition|Image recognition on landscape nature images|75|401.0|2020|2.0|None|None|44.0|https://competitions.codalab.org/competitions/27749|||||aerial|computer vision|Aerial image recognition|multi-class classification, image recognition|||Prediction score|accuracy|False|False|True|True||[nan]
Document-level MQM (score) QE shared task 2020|WMT'20 Quality Estimation shared task, submission platform for **MQM score** predictions at document-level (Task 3)|17|126.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/24762|||||linguistics|NLP|predict document-level quality according to MQM scores|machine translation, regression|||Pearson r|Pearson, MAE, RMSE|True|True|True|False||[1.2078414839797638]
The VoxCeleb Speaker Recognition Challenge 2021 - Track 3 (Verification self supervised, closed)|The goal of the VoxSRC challenge is to probe how well current methods can recognize speakers from speech obtained 'in the wild'. The task of speaker verification aims to determine whether two samples of speech are from the same person.|47|62.0|2021|2.0|None|None|33.0|https://competitions.codalab.org/competitions/34069|||||speech|signal processing|recognize speakers from speech obtained 'in the wild'|binary classification|||EER|DCF, ERR|True|False|True|False||[0.0]
BioCreative'21, Task 3 - Automatic extraction of medication names in tweets|The BioCreative 2021 task 3 is a sequence labeling task where competing systems will extract the spans of drug names or dietary supplements in tweets. The dataset will consist of all 112 timelines annotated for|57|234.0|2020|3.0|None|None|627.0|https://competitions.codalab.org/competitions/23925|||||medicine, tweets|NLP|extraction of medication names in tweets|binary classification|||overlapping F1|Overlapping and Strict Precision, Recall and the balanced F1-score|True|False|True|False||[1.138617200674536]
FIRE 2020 - Authorship Identification of SOurce COde (AI-SOCO)|Identify the source codes authors using the pre-defined list of 1,000 authors.|69|147.0|2020|3.0|None|None|92.0|https://competitions.codalab.org/competitions/25148||||||NLP|detecting the authors|multi-class classification|||Accuracy |accuracy|False|False|True|False||[1.0303604799176664]
Reliable Intelligence Identification on Vietnamese SNSs (ReINTEL) - Legacy|"This challenge aims to identify a piece of information shared on social network sites (SNSs), is reliable or unreliable. With the blazing-fast spurt of SNSs, e.g., Facebook, Zalo, or Lotus, there are approximately 65 million Vietnamese users on board with the annual growth of 2.7 million in the recent year, as reported by the Digital 2020 [6]. SNSs become essential means for users to not only connect friends but also freely create, share diverse information [2, 5], i.e., news. Within freedom, a number of users tend to spread unreliable information for their personal purposes affecting the online society. Detecting whether news spreading in SNSs is reliable or unreliable has gained significant attention recently [1, 3, 4]. Therefore, this shared task targets identifying shared information in Vietnamese SNSs. It provides an opportunity for participants who are interested in the problem, to contribute their knowledge to improve the online society for social good. This challenge has reopened as a ""Legacy"" phase for previous or new participants to test out their methods. There will be no end date and no limit of the number of submissions per day."|18|479.0|2020|1.0|None|None||https://competitions.codalab.org/competitions/28114|||||multimedia, linguistics|NLP|identify a piece of information shared on social network sites (SNSs), is reliable or unreliable|binary classification|||ROC|ROC-AUC|False|False|True|False||[1.0158730158730158]
Regression Over Urban Growth Prediction Challenge|Challenge organized by ABEONA team of M1 AI students 2020/2021|25|70.0|2021|2.0|None|None|53.0|https://competitions.codalab.org/competitions/28673|||||urbanisme|NLP|predict the urban growth of a year having a huge amount of data about the previous year|regression|||Prediction score|accuracy|False|False|True|True||[1.5423374363148956]
SMM4H 2021 - Social Media Mining for Health Shared Task|The Social Media Mining for Health Applications (#SMM4H) Shared Task involves natural language processing (NLP) challenges of using social media data for health research, including informal, colloquial expressions and misspellings of clinical concepts, noise, data sparsity, ambiguity, and multilingual posts. For each of the eight tasks below, participating teams will be provided with a set of annotated tweets for developing systems, followed by a three-day window during which they will run their systems on unlabeled test data and upload the predictions of their systems to CodaLab. Information about registration, data access, paper submissions, and presentations can be found in the individual task pages.|145|1240.0|2021|36.0|None|None|63.0|https://competitions.codalab.org/competitions/28766|||||medicine, health, tweets|NLP|Task 1: Classification, Extraction and Normalization of Adverse Effect mentions in English tweets. Task 2: Classification of Russian tweets for detecting presence of Adverse Effect mentions. Task 3: Classification of change in medications regimen in tweets. Task 4: Classification of tweets self-reporting adverse pregnancy outcomes. Task 5: Classification of tweets self-reporting potential cases of COVID-19. Task 6: Classification of COVID19 tweets containing symptoms. Task 7: Identification of professions and occupations (ProfNER) in Spanish tweets. Task 8: Classification of self-reported breast cancer posts on Twitter|multi-class classificaiton|||F1|precision, recall, accuracy, F1|False|True|True|False||[0.0]
ActivityNet-Entities Object Localization Task (General Purpose, NOT for Challenge 2020)|This is the official evaluation server for ActivityNet-Entities dataset on the object localization task. More info: https://github.com/facebookresearch/ActivityNet-Entities Note that this is NOT the evaluation server for the 2020 ANet-Entities challenge (they are here instead: https://github.com/facebookresearch/ActivityNet-Entities#evaluation-servers).|17|91.0|2019|2.0|None|None|297.0|https://competitions.codalab.org/competitions/20537|https://github.com/facebookresearch/ActivityNet-Entities|https://arxiv.org/pdf/1812.06587.pdf|||multimedia|computer vision|object localization|regression|||F1_all|precision, recall, accuracy, F1|False|False|True|False||[0.0]
LOVEU@CVPR'21 GEBD Sub-track 1.2|This competition is for generic event boundary detection with constraints of training data and pre-trained models.|44|159.0|2021|2.0|1000|1000|37.0|https://competitions.codalab.org/competitions/30452|||||multimedia|computer vision|event boundary detection|regression|||F1 Score|F1|False|False|True|True||[1.1473850572638902]
SemEval-2020 Task 5: Modelling Causal Reasoning in Language: Detecting Counterfactuals||161|13.0|2019|3.0|None|None|152.0|https://competitions.codalab.org/competitions/20972|||||linguistics|NLP|determine whether a given statement is counterfactual or not, locate antecedent and consequent in counterfactuals|binary classification, regression|||f1_task1|Exact Match, Precision, Recall, and F1|True|True|True|False||[0.0]
Low Resource ASR|This is a low resource ASR competition based on the Lingvodoc data.|8|18.0|2020|1.0|None|None||https://competitions.codalab.org/competitions/30008|||||speech|signal processing, NLP|transcribe utterances or spell them||||Total by language|accuracy, ERR|True|True|True|True||[0.0]
FewRel 1.0|A few-shot relation classification benchmark|135|1966.0|2018|1.0|None|None||https://competitions.codalab.org/competitions/27980||https://aclanthology.org/D18-1514.pdf|||linguistics|NLP||multi-class classification|||avg|N-way K-shot|True|True|True|False||[1.1707287861548596]
LaySumm (The 1st Computational Linguistics Lay Summary Challenge Shared Task)|A Shared Task at EMNLP 2020 that focuses on generating lay summaries for scientific documents. LaySumm is one of three shared tasks conducted as part of the 1st Workshop on Scholarly Document Processing|30|469.0|2020|1.0|None|None||https://competitions.codalab.org/competitions/25516|||||science|NLP|generate summaries that are representative of the content, comprehensible, and interesting to a lay audience|text generation|||Rouge1-F1|Recall, F1, ROUGE-1, -2, -L|True|False|True|False||[1.1018293960897032]
Word Segmentation and Morphological Parsing for Sanskrit|Hackathon on word segmentation and morphological parsing for Sanskrit, in association with FIRE '21 (Forum for Information Retrieval Evaluation)|21|139.0|2021|2.0|None|None|41.0|https://competitions.codalab.org/competitions/35744|||||linguistics|NLP|predict a list of triplets (string, string, string) where each item has the analysis for a word (segmented word, stem, morph category)|text generation|||T1 Task Score|recall|False|True|True|False||[0.0]
XL-WiC|This is a competition based on the XL-WiC (Cross-Lingual Word-in-Context) evaluation datasets|12|21.0|2020|1.0|None|None||https://competitions.codalab.org/competitions/27071|||||linguistics|NLP|recognising whether a target word is used with the same meaning or not in two different contexts|binary classification||||accuracy|False|False|True|False||[0.0]
2019-Video-Caption-Competition|Automatically generate a natural description based on the video content|46|122.0|2019|1.0|None|None||https://competitions.codalab.org/competitions/22018|||||multimedia|computer vision, NLP|generate a natural description based on the video content|text generation|||CIDEr |CIDEr |False|False|True|False||[2.244419294411489]
Zero Resource Speech Challenge 2020|ZeroSpeech 2020|34|22.0|2020|1.0|None|None||https://competitions.codalab.org/competitions/22999|https://zerospeech.com/2020/||||speech|signal processing, NLP|construct a system that learn an end-to-end Spoken Dialog (SD) system|dialogue generation||||precision, recall and F1|True|False|True|False||[0.0]
2020 IEEE GRSS Data Fusion Contest|2020 IEEE GRSS Data Fusion Contest: Global Land Cover Mapping with Weak Supervision|235|3606.0|2020|4.0|None|None|67.0|https://competitions.codalab.org/competitions/22289|||||aerial|computer vision|Land cover classification|multi-class classification|||Prediction |average accuracy|False|False|True|False||[1.1464193097945343]
SemEval-2018 Task 12 - The Argument Reasoning Comprehension Task|Given a natural language argument consisting of a claim, premise, and context, the goal is to choose the correct warrant from two contradicting options.|128|187.0|2017|3.0|None|None|244.0|https://competitions.codalab.org/competitions/17327|||||linguistics|NLP|Given a natural language argument consisting of a claim, premise, and context, the goal is to choose the correct warrant from two contradicting options.|binary classification|||Accuracy |accuracy|False|False|True|False||[1.0697191866094893]
Assessing the Funniness of Edited News Headlines (SemEval-2020)|This is a humor competition that requires participants to estimate the funniness of edited news headlines.|587|5566.0|2019|6.0|None|None|286.0|https://competitions.codalab.org/competitions/20970|||||multimedia, news|NLP|estimate the funniness of edited news headlines|ordinal regression|||RMSE, Accuracy|RMSE, accuracy|True|False|True|False||[0.0]
Document-level MQM (annotations) QE shared task 2020|WMT'20 Quality Estimation shared task, submission platform for **MQM annotations** predictions at document-level (Task 3)|23|140.0|2020|1.0|None|None||https://competitions.codalab.org/competitions/24763|||||linguistics|NLP|estimating the quality of neural machine translation output at run-time, without relying on reference translations|machine translation, regression|||F1 score|F1|False|False|True|False||[1.0841379310344827]
CLEF 2019 Lab ProtestNews|Extracting Protests from News Using Automated Methods|9|80.0|2019|4.0|None|None|49.0|https://competitions.codalab.org/competitions/20318|||||linguistics, news|NLP|extracting event information from news articles across multiple countries|multi-class classification, information retrieval||||F1|False|False|True|False||[0.0]
SemEval 2020 Task 4 - Commonsense Validation and Explanation (ComVE)|Does the statement Make Sense? And Why?|634|2916.0|2019|5.0|None|None|209.0|https://competitions.codalab.org/competitions/21080|||||linguistics|NLP|The first task is to choose from two natural language statements with similar wordings which one makes sense and which one does not make sense; The second task is to find the key reason from three options why a given statement does not make sense; The third task asks machine to generate the reasons and we use BLEU to evaluate them.|binary classification, multi-class classification, text generation|||Subtask A Accuracy, Subtask B Accuracy, Subtask C BLEU|accuracy, BLEU|True|True|True|False||[1.5443002200375477]
AutoML :: ROUND 0 PRACTICE|"This is a ""clone"", this is NOT the AutoML main competition page. To enter the challenge, go to yellow icon."|63|594.0|2015|2.0|None|None|1839.0|https://competitions.codalab.org/competitions/7471||||||AutoML||binary classification|||<Rank>|time, accuracy|True|True|True|True||[1.3061451625725415]
MIND News Recommendation Competition|This is a competition for news recommendation on the Microsoft News Dataset (MIND).|772|5239.0|2020|2.0|None|None|32.0|https://competitions.codalab.org/competitions/24122|||||linguistics, news|NLP|personalized news services|ordinal regression|||AUC|AUC, MRR and nDCG@K|True|False|True|False||[1.1041167336300048]
Deep Pollination Raw|Deep Pollination M1 [AI] 2020-2021|24|168.0|2018|2.0|None|None|11489.0|https://competitions.codalab.org/competitions/28996|||||biology|computer vision|recognize the type of insect in the image|image recognition|||Prediction score|accuracy|False|False|True|True||[1.206285563858008]
HAHA@IberLEF2021: Humor Analysis based on Human Annotation|Detecting and scoring humor, mechanisms and targets in Spanish tweets|108|487.0|2021|3.0|None|None|63.0|https://competitions.codalab.org/competitions/30090|||||linguistics, tweets|NLP|classify tweets in Spanish as humorous or not, and to further our analysis of humor by determining which characteristics of the tweets contribute to their humor|binary classification|||Task 1 Score|F1, RMSE|True|True|True|False||[0.0]
SemEval 2020 Task 2: Predicting Multilingual and Cross-Lingual (Graded) Lexical Entailment|This is a competition set for the Shared Task 2 of the SemEval 2020. The competition includes tasks for predicting graded and binary lexical entailment (i.e., hyponym-hypernym relation between words) for several different languages and cross-lingual language pairs.|162|115.0|2019|3.0|None|None|191.0|https://competitions.codalab.org/competitions/20865|||||linguistics|NLP|predicting binary and graded Lexical Entailment (i.e., is-a or hyponym-hypernym relation) for several different languages (multilingual component) and across languages for several language pairs (cross-lingual component)|multi-class classification|||EN (Gr)|Spearman correlation, F1|True|False|True|False||[0.0]
OpenMonkeyChallenge|Non-human Primate Landmarks detection competition|32|71.0|2021|1.0|None|None|0.0|https://competitions.codalab.org/competitions/34342|||||biology, multimedia|computer vision|detect the 2D coordinates of the landmarks on the non-human primate (NHP) images in the wild|object detection|||MPJPE, <Rank>|MPJPE, PCK@0.2, PCK@0.5, and mAP|True|False|True|False||[4.300044583147572]
IndoNLU Benchmark|12 Natural Language Understanding Tasks|98|1880.0|2020|12.0|None|None|0.0|https://competitions.codalab.org/competitions/26537|||||linguistics|NLP|sentence classification tasks and sequence labeling tasks, with different levels of difficulty, domains, and styles|multi-class classification|||f1|accuracy, macro-precision, macro-recall, and macro-F1|True|True|True|False||[1.0913421126365932]
SemEval-2021 Task 12 - Learning with Disagreements|A SemEval-2021 shared task on learning to classify with datasets containing disagreements.|135|152.0|2020|3.0|0|0|200.0|https://competitions.codalab.org/competitions/25748|||||linguistics|NLP|provide a unified testing framework for learning from disagreements using the best-known datasets containing information about disagreements for interpreting language and classifying images|multi-class classification|||Average|F1, enthropy|True|True|True|False||[0.0]
RSNA Pneumonia Crop|RSNA Pneumonia Crop|20|103.0|2018|2.0|None|None|11489.0|https://competitions.codalab.org/competitions/29560|||||medicine|computer vision|binary classification of normal vs pneumonia|binary classification|||Prediction score|accuracy|False|False|True|True||[1.1096597993231607]
SALICON Saliency Prediction Challenge (LSUN 2017)|Understanding and predicting visual attention in natural scene images.|264|1003.0|2017|1.0|None|None|0.0|https://competitions.codalab.org/competitions/17136|||||biology, multimedia|computer vision|predicting visual saliency in natural scene images|regression|||SAUC|SAUC, IG, NSS, and CC|True|False|True|False||[1.0486328125]
SegTHOR|Segmentation of THoracic Organs at Risk in CT images|510|839.0|2019|2.0|None|None|54.0|https://competitions.codalab.org/competitions/21145|||||medicine|computer vision|segment 4 OAR: heart, aorta, trachea, esophagus|multi-class classification, regression|||Rank <All>|overlap Dice metric (DM), Hausdorff distance (HD)|True|False|True|False||[0.0]
BEA 2019 Shared Task - Grammatical Error Correction - Restricted Track|This is the Codalab competition corresponding to the BEA 2019 Shared Task on Grammatical Error Correction - Restricted Track.|292|18149.0|2019|3.0|None|None|33.0|https://competitions.codalab.org/competitions/20228|||||linguistics|NLP|Grammatical Error Correction|binary classification, text generation|||F_0.5|span-based correction, precision, recall|True|False|True|False||[1.2144286463713405]
MetaDL few-shot learning self-service|Here you can submit datasets to Proto-Networks|12|101.0|2021|1.0|None|None||https://competitions.codalab.org/competitions/31280|||||multimedia|computer vision|meta-learner on a meta-train set and produce a learner (a machine learning algorithm), which will be used to train on classification tasks generated from the meta-test set and evaluated|meta-learning, multi-class classification|||score|accuracy, 5-way 1-shot|False|False|True|True||[1.165687898408874]
Multi-FC|MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims|284|124.0|2019|1.0|None|None|0.0|https://competitions.codalab.org/competitions/21163|||||claims|NLP|automatic claim verification|multi-class classification|||Average|micro-F1, macro-F1|True|False|True|False||[nan]
SemEval-2018 Task 1: Affect in Tweets (AIT-2018)|We present an array of tasks where systems have to automatically determine the intensity of emotions (E) and intensity of sentiment (aka valence V) of the tweeters from their tweets. We also include a multi-label emotion classification task for tweets. For each task, we provide separate training and test datasets for English, Arabic, and Spanish tweets.|300|1492.0|2017|3.0|None|None|167.0|https://competitions.codalab.org/competitions/17751|||||linguistics, tweets|NLP|determine the intensity of emotions (E) and intensity of sentiment (aka valence V) of the tweeters. We also include a multi-label emotion classification task for tweets. |multi-class classification, regression|||Pearson (all instances) macro-avg|Pearson Correlation Coefficient, Kappa, accuracy|True|True|True|False||[0.0]
ASVspoof 2021 - Post-Challenge - Physical Access track|Automatic Speaker Verification Spoofing and Countermeasures Challenge|25|437.0|2021|1.0|None|None|0.0|https://competitions.codalab.org/competitions/35160|||||speech|signal processing||regression|||min t-DCF|t-DCF, EER|True|False|True|False||[0.0]
Constraint@AAAI2021 - Hostile Post Detection in Hindi|Multi-label Classification of a given Hindi text into various Hostile classes.|215|69.0|2020|3.0|None|None|71.0|https://competitions.codalab.org/competitions/26654|||||linguistics|NLP|The set of valid categories are fake news, hate speech, offensive, defamation, and non-hostile posts. It is a multi-label multi-class classification problem where each post can belong to one or more of these hostile classes. |mult-class classificaition||||weighted F1|False|False|True|False||[0.0]
SemanticKITTI: Moving Object Segmentation|Moving Object Segmentation of automotive LiDAR point cloud sequences.|79|182.0|2021|1.0|None|None|0.0|https://competitions.codalab.org/competitions/28894|||||urbanisme|computer vision|provide motion labels for each point of the test sequences|mult-class classificaition|||IoU (moving)|Jaccard Index or intersection-over-union (IoU) metric|True|False|True|False||[0.0]
SemEval 2021 Task 10 - Source-Free Domain Adaptation for Semantic Processing|Given a trained model, distill and transfer the knowledge to a new domain.|87|98.0|2020|3.0|None|None|237.0|https://competitions.codalab.org/competitions/26152|||||linguistics|NLP|negation detection and time expression recognition|binary classification|||F1|precision, recall and F1|True|False|True|False||[0.0]
SemEval 2018 Task 4: Character Identification on Multiparty Dialogues|An entity linking task where mentions are personal nouns and entities are characters in the TV show, <i>Friends</i>.|95|73.0|2017|2.0|None|None|140.0|https://competitions.codalab.org/competitions/17310|||||linguistics, movies|NLP|assign each mention to its entity, who may or may not participate in the dialogue|||two seasons of the TV show Friends are annotated for this task.|accuracy |accuracy, F1|True|False|True|False||[0.0]
Document-level QE shared task 2018|WMT'18 Quality Estimation shared task, submission platform for predictions at document-level (task4).|7|33.0|2018|1.0|None|None|0.0|https://competitions.codalab.org/competitions/19309|||||linguistics|NLP|identification of an error and its classification according to this typology (by applying a specific tag), the errors will receive a severity scale that will show the impact of each error on the overall meaning, style, and fluency of the translation|machine translation, multi-class classification, regression|||Pearson r|Pearson's correlation, MAE, RMSE|True|True|True|False||[1.3387683431581587]
Transliteration task|Transliteration task|74|594.0|2021|2.0|None|None|364.0|https://competitions.codalab.org/competitions/30932|||||linguistics|NLP|transliteration problem of names from English on Russian. Transliteration of a string means writing this string using the alphabet of another language with the preservation of pronunciation, although not always|machine traslation, text generation|||Accuracy@1|accuracy|False|False|True|False||[1.0714019712505276]
GermEval 2020 Shared Task on the Classification and Regression of Cognitive and Motivational style|The validity of high school grades as a predictor of academic success iscontroversial. During an aptitude test (Operant Motive Test, OMT),participants are asked to write freely associated texts to providedquestions and images. Psychologists can predict subsequentsuccess from these texts.For our task, we provide textual data from the OMT, paired with motives,tests on cognitive and motivational style and school grades, extending sole text classification toprediction of psychometrics.|37|38.0|2019|3.0|None|None|159.0|https://competitions.codalab.org/competitions/22006|||||linguistics, psychology|NLP|classification and regression of cognitive and motivational style from a text: build systems to restore an artificial 'rank' as well as performing classification on an image description that psychologists can investigate on implicit motives|multi-class classification, ordinal regression|||Prediction score|Pearson rank correlation coefficient, precision, recall and F1|True|True|True|True||[0.0]
(Additional scoring) Shared Task on Multilingual protest news detection CASE 2021 @ ACL-IJCNLP 2021|(Additional scoring) Extracting Protests from News Using Automated Methods: Multilingual protest news detection|23|305.0|2021|13.0|None|None|0.0|https://competitions.codalab.org/competitions/31639|||||news|NLP|protest news detection|binary classification, multi-class classification|||score|macro-F1|False|False|True|True||[1.012537286686553]
Sentence-level Direct Assessment QE Shared Task 2020|WMT'20 Quality Estimation shared task, submission platform for predictions at sentence-level *Direct Assessment* (Task 1)|60|1261.0|2020|8.0|None|None|0.0|https://competitions.codalab.org/competitions/24447|||||linguistics|NLP|automatic methods for estimating the quality of neural machine translation output at run-time, without relying on reference translations|machine translation|||Pearson r|z-standardised DA|False|False|True|False||[1.2304185420630134]
Covid-19 Infection Percentage Estimation|Covid-19|179|2401.0|2022|1.0|None|None||https://competitions.codalab.org/competitions/35575|||||covid|computer vision|not only limited to the detection of COVID-19 cases, but they can also be used for other important tasks such quantifying the infection and monitoring the evolution of the disease|multi-class classification, regression|||MAE|MAE, PC, RMSE|True|False|True|False||[0.0]
DocRED|Document-level relation extration|589|6752.0|2019|1.0|None|None|0.0|https://competitions.codalab.org/competitions/20717|https://github.com/thunlp/DocRED||||linguistics|NLP||relation extraction|||RE_ignore_annotated_F1|relation extration F1 and evidence F1|True|True|True|False||[1.247263318516553]
Sentence-level Direct Assessment QE shared task 2021|WMT'21 Quality Estimation shared task, submission platform for predictions at sentence-level *Direct Assessment* (Task 1).|25|837.0|2021|12.0|None|None|15.0|https://competitions.codalab.org/competitions/33411|||||linguistics|NLP|(i) a zero-shot sentence-level prediction task to encourage language independent and unsupervised approaches; (ii) a task on predicting catastrophic, i.e. critical translation errors, in other words, errors that make the translation convey a completely different meaning, which could lead to negative effects such as safety risks|machine translation, multi-class classification, regression|||Pearson r|z-standardised DA|False|True|True|False||[1.1007492944954425]
Multi-Hop Inference Explanation Regeneration (TextGraphs-15)|Extract and rank explanation sentences from a provided structured knowledge base such that the top-ranked sentences provide a complete explanation for the given answer.|29|85.0|2021|3.0|None|None|38.0|https://competitions.codalab.org/competitions/29228|||||linguistics|NLP|determining relevance versus completeness in large multi-hop explanations|text generation, ordinal regression, QA|||NDCG |Normalized Discounted Cumulative Gain (NDCG), accuracy|True|False|True|False||[1.0]
2019 IEEE GRSS Data Fusion Contest 3D Point Cloud Classification Challenge|2019 IEEE GRSS Data Fusion Contest 3D Point Cloud Classification Challenge|47|567.0|2019|3.0|None|None|44.0|https://competitions.codalab.org/competitions/20217|||||aerial|computer vision|semantic 3D reconstruction and stereo using machine intelligence and deep learning applied to satellite images|image recognition, multi-class classification|||Prediction score|mIoU|False|False|True|False||[1.2454244210688552]
WiC-TSV: Word-in-Context Target Sense Verification|This is a competition based on the WiC-TSV (Word-in-Context Target Sense Verification) evaluation dataset. This competition extends the SuperGLUE WiC task but it is self-contained. WiC-TSV is used for a shared task at the IJCAI-20 SemDeep workshop. For questions about WiC-TSV, you can write in our Google Group.|35|113.0|2020|3.0|None|None|198.0|https://competitions.codalab.org/competitions/23683|||||linguistics|NLP|identifying the correct meaning of a word in context, in a retrieval setting|binary classification, multi-class classification|||all|accuracy, F1|True|True|True|True||[nan]
MAVEN Event Detection Challenge|A massive general domain textual event detection dataset.|156|2826.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/27320|https://github.com/THU-KEG/MAVEN-dataset|https://arxiv.org/abs/2004.13590|||linguistics|NLP|event detection|binary classification, multi-class classification||4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types|Micro_F1|precision, recall, and F1 (micro and macro averaged)|True|False|True|False||[1.0712180601809553]
Sentiment classification tasks|"An example competition where submissions should output ""Hello World!"""|57|614.0|2021|8.0|None|None|0.0|https://competitions.codalab.org/competitions/30517|||||linguistics|NLP|sentiment classification|binary classification|||Accuracy |accuracy|False|False|True|False||[1.1690864710643203]
VALUE Leaderboard Evaluation|Evaluation portal for The Video And Language Understanding Evaluation (VALUE) benchmark.|53|80.0|2021|1.0|None|None|0.0|https://competitions.codalab.org/competitions/34470|https://value-benchmark.github.io/|||||computer vision, NLP|text-based video retrieval, video question answering and video captioning|QA, text generation, multi-class classification|||TVR|Mean-Rank, the average of model ranks over 11 tasks|False|True|True|False||[0.0]
CIEC-CTC 2021|Chinese text Correction (CTC 2021).|227|993.0|2021|3.0|30000 CNY|4500|42.0|https://competitions.codalab.org/competitions/32702|||||linguistics|NLP|text correction|binary classifcation, multi-class classifcation, text generation|||Prediction score|F1|False|False|True|True||[1.0]
The VoxCeleb Speaker Recognition Challenge 2020 - Track 1 (Verification fully supervised, closed)|The goal of the VoxSRC challenge is to probe how well current methods can recognize speakers from speech obtained 'in the wild'. The task of speaker verification aims to determine whether two samples of speech are from the same person.|133|422.0|2020|2.0|None|None|59.0|https://competitions.codalab.org/competitions/26120|||||speech|signal processing|recognize speakers from speech obtained 'in the wild'|binary classification|||DCF|DCF, ERR|True|False|True|False||[0.0]
Word-level Post-Editing Effort QE shared task 2020|WMT'20 Quality Estimation shared task, submission platform for predictions at word-level *Post-Editing Effort* (Task 2)|28|177.0|2020|2.0|None|None|0.0|https://competitions.codalab.org/competitions/25377|||||linguistics|NLP|predicting DA scores and their relationship with models trained for predicting post-editing effort|machine translation, multi-class classification, regression|||MCC|MCC (Matthews correlation coefficient)|False|True|True|False||[0.0]
SemKITTI-DVPS Challenge|The test set for SemKITTI-DVPS ranked by DSTQ.|25|117.0|2021|2.0|None|None|86.0|https://competitions.codalab.org/competitions/33634|||||multimedia|computer vision|provide for each frame a depth prediction, a semantic prediction, and a prediction of temporally consistent instance IDs|multi-class classifcation, regression|||DSTQ |accuracy, Depth-aware Segmentation and Tracking Quality (DSTQ)|True|True|True|False||[1.0]
TVQA Test Public Evaluation (w/o timestamp at inference) Beta|UNC TVQA test_public set evaluation for models that did not use ground-truth timestamp at inference.|43|86.0|2018|1.0|None|None|0.0|https://competitions.codalab.org/competitions/20415|||||multimedia|computer vision, NLP|localize relevant moments within a clip, comprehend subtitles-based dialogue, and recognize relevant visual concepts|QA, text generation, multi-class classification||152.5K QA pairs from 21.8K video clips, spanning over 460 hours of video|bbt|accuracy|False|True|True|False||[0.0]
UAVid semantic segmentation|The UAVid dataset is an UAV video dataset for semantic segmentation task focusing on urban scenes. 4K resolution UAV videos are captured with 8 object categories labelled.|72|974.0|2020|2.0|None|None|121.0|https://competitions.codalab.org/competitions/25224|||||urbanisme|computer vision|predict per-pixel semantic labelling for the UAV video sequences|multi-class classification|||meanIoU|IoU |False|False|True|False||[1.1573965125228918]
SentiMix Hindi-English|Sentiment analysis of code-mixed tweets. SemEval 2020 task 9. Hindi sub task.|894|4338.0|2019|1.0|None|None|0.0|https://competitions.codalab.org/competitions/20654||https://arxiv.org/abs/2008.04277|||linguistics, tweets|NLP|sentiment analysis in code-mixed social media text: predict the sentiment of a given code-mixed tweet|machine translation, multi-class classification|||correct|Precision, Recall, and F1 (micro and macro averaged)|True|False|True|False||[1.4785473728403624]
MinneApple Fruit Counting Challenge|We present a new dataset to advance the state-of-the-art in fruit detection, segmentation, and counting in orchard environments. While there has been significant recent interest in solving these problems, the lack of a unified dataset has made it difficult to compare results. We hope to enable direct comparisons by providing a large variety of high-resolution images acquired in orchards, together with human annotations of the fruit on trees.|21|28.0|2019|1.0|None|None|0.0|https://competitions.codalab.org/competitions/21719|||||agriculture|computer vision|mapping in orchard environments from RGB images|regression||1000 annotated images for fruit detection and segmentation and 60000 images for patch-based fruit counting|Accuracy |accuracy|False|False|True|False||[0.0]
2019 IEEE GRSS Data Fusion Contest Single-view Semantic 3D Challenge|2019 IEEE GRSS Data Fusion Contest Single-view Semantic 3D Challenge|77|65.0|2019|3.0|None|None|44.0|https://competitions.codalab.org/competitions/20208|||||aerial, urbanisme|computer vision|predict both 3D geometric models and segmentation maps of semantic classes for urban scenes from single-view satellite images|object recognition|||Prediction|mIoU-3|False|False|True|False||[1.129083412622899]
SMM4H'20 - Shared Task|The SMM4H shared task proposed four NLP challenges on social media mining for health monitoring and surveillance, 1. Automatic classification of tweets that mention medications, 2. Automatic classification of multilingual tweets that report adverse effects, 3.    Automatic extraction and normalization of adverse effects in English tweets, 4. Automatic characterization of chatter related to prescription medication abuse in tweets, 5. Automatic classification of tweets reporting a birth defect pregnancy outcome|123|1068.0|2020|21.0|None|None|156.0|https://competitions.codalab.org/competitions/23705|||||medicine, tweets|NLP|social media mining for health monitoring and surveillance|binary classification|||F1|F1|False|False|True|False||[0.0]
2021 IEEE GRSS Data Fusion Contest Track DSE|2021 IEEE GRSS Data Fusion Contest Track DSE|191|3751.0|2021|3.0|10000|10000|60.0|https://competitions.codalab.org/competitions/27943|||||multimedia, urbanisme|computer vision|detection of human settlements deprived of access to electricity using multimodal and multitemporal remote sensing data: detect human settlements that do not have access to electricity using multimodal and multitemporal remote sensing data|binary classification|||Prediction|F1|False|False|True|False||[1.0922726597693186]
SemEval-2022 Task 10: Structured Sentiment competition|SemEval-2022 Task 10: Structured Sentiment competition|258|763.0|2021|3.0|None|None|236.0|https://competitions.codalab.org/competitions/33556|||||linguistics|NLP|predict the sentiment graphs|sentiment graphs|||Average score|F1|False|False|True|False||[0.0]
Webvision - Video track - FEATURES ONLY!|Action recognition from video subtitles with precomputed features|13|22.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/23719|||||multimedia|computer vision, NLP|Action recognition from video subtitles with precomputed features|action recognition|||IoU|IoU |False|False|True|False||[1.2877632947298188]
SemanticKITTI: Semantic Segmentation|Semantic Segmentation of automotive LiDAR point cloud sequences|681|2345.0|2019|2.0|None|None|0.0|https://competitions.codalab.org/competitions/20331|||||linguistics|NLP|provide labels for each point of the test sequences 11-21: method should then output a label for each point of a scan, i.e., one full turn of the rotating LiDAR sensor|multi-class classification|||mIoU|mIoU|False|False|True|False||[1.5854763026863103]
SemanticKITTI: Semantic Scene Completion|Semantic Scene Completion of automotive LiDAR point cloud sequences|89|125.0|2019|1.0|None|None|0.0|https://competitions.codalab.org/competitions/22037|||||linguistics|NLP|predict the completed voxel grid. Here, the approach needs to provide labels for each voxel of a pre-defined dimension|multi-class classification|||mIoU|mIoU|False|False|True|False||[1.6668381785438642]
ICDAR 2021 Competition on Scientific Table Image Recognition to LaTeX||166|423.0|2020|6.0|None|None|169.0|https://competitions.codalab.org/competitions/26979|||||linguistics|NLP|table recognition: Subtask I: Table structure reconstruction (S): Reconstructing the structure of a table in the form of LaTeX symbols and code. Subtask II: Table content reconstruction (C): Reconstructing and recognizing the content of a table in the form of LaTeX symbols and code|binary classification, multi-class classification|||Exact Match|accuracy, accuracy@95%|True|True|True|False||[0.0]
SemEval2021 Task 8 - MeasEval - Counts and Measurements|Extracting Quantities and their Contexts from Scientific Texts.|251|485.0|2020|3.0|None|None|123.0|https://competitions.codalab.org/competitions/25770|||||linguistics|NLP|relation extraction task focused on finding counts and measurements, attributes of these quantities, and additional information including measured entities, properties, and measurement contexts|relation extraction, binary classification, multi-class classification|||correct|F1|False|True|True|False||[0.0]
2021 Video Captioning Competition|Automatically generate a natural description based on the video content|75|789.0|2021|1.0|None|None|0.0|https://competitions.codalab.org/competitions/32106|||||multimedia|computer vision, NLP|video captioning competition! All participants need to generate a caption sentences for each videos in test set|multi-class classification, text generation|||Avg_Score|Bleu、METEOR、ROUGE_L、CIDEr|True|False|True|False||[1.2864979983555511]
SemEval-2018 Task 9: Hypernym Discovery|This is the CodaLab Competition for the SemEval-2018 Task 9: Hypernym Discovery.|166|78.0|2018|3.0|None|None|22.0|https://competitions.codalab.org/competitions/17119|||||linguistics|NLP|identifying hypernymic relations from text corpora|binary classification||||MRR, P@k, MAP|True|False|True|False||[0.0]
Occluded Video Instance Segmentation (OVIS)|OVIS (short for Occluded Video Instance Segmentation) is a new large scale benchmark dataset for video instance segmentation task. It is designed with the philosophy of perceiving object occlusions in videos, which could reveal the complexity and the diversity of real-world scenes.|31|736.0|2021|1.0|None|None|0.0|https://competitions.codalab.org/competitions/36203|||||multimedia|computer vision|Occluded Video Instance Segmentation: detect, segment, and track instances in occluded scenes|video recognition|||mAP|AP|False|False|True|False||[1.9790794979079496]
SemEval 2020 Task 1: Unsupervised Lexical Semantic Change Detection|Shared task addressing the unsupervised detection of lexical semantic change, i.e., word sense changes over time, in text corpora of German, English, Latin and Swedish. The task is organized by Barbara McGillivray, Dominik Schlechtweg, Simon Hengchen, Haim Dubossarsky and Nina Tahmasebi.|371|1118.0|2019|3.0|None|None|256.0|https://competitions.codalab.org/competitions/20948|||||linguistics|NLP|detection of lexical semantic change, i.e., word sense changes over time|multi-class classification, ordinal regression|||All ACC All SPR|accuracy, Spearman's rank-order correlation coefficient|True|True|True|False||[0.0]
TVQAplus Evaluation|UNC TVQAplus test set evaluation.|39|69.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/22705|||||linguistics|computer vision, NLP|video-based QA|QA, text generation, multi-class classification|||Grd mAP|Grd mAP, Temp mIoU,  ASA, QA Acc |True|True|True|False||[0.0]
TriviaQA|TriviaQA Leaderboard|144|421.0|2017|2.0|None|None|0.0|https://competitions.codalab.org/competitions/17208|http://nlp.cs.washington.edu/triviaqa/|https://arxiv.org/pdf/1705.03551.pdf|||linguistics|NLP|Reading Comprehension and Question Answering|QA, text generation, multi-class classification|||full-em|F1|False|False|True|False||[1.3593181842998938]
AutoML :: ROUND 3 PRACTICE|"This is a ""clone"", this is NOT the AutoML main competition page. To enter the challenge, go to yellow icon."|41|248.0|2015|2.0|None|None|1851.0|https://competitions.codalab.org/competitions/7451||||||AutoML||multi-class classification|||<Rank>, Set 1||||True|True||[1.4495957186900121]
IUI 2022-Dyadic IMPRESSION Recognition challenge|The IMPRESSION challenge 2022, to be held in March 2022 in conjunction with IUI 2022 in Helsinki, Finland, will be devoted to all aspects of computer science and behavioural science for the analysis of human-human interaction from multimodal data. To advance and motivate the research on human bodily responses in dyadic interactions, we organise the challenge which uses an open and accessible multimodal IMPRESSION dataset. It addresses multimodal recognition as well as dynamic multi-user recognition, where both interlocutors’ information can be exploited.|43|27.0|2021|2.0|None|None|70.0|https://competitions.codalab.org/competitions/34190|||||multimedia|computer vision|automatic impression recognition|impression recognition|||Prediction score|mean concordance correlation coefficient (CCC)|False|False|True|False||[0.0]
User behavior prediction|This competition is a supplement to the Trumid Data Scientist interview process. If you are up to the challenge, please contact us and we will grant access.|50|11.0|2016|1.0|None|None|0.0|https://competitions.codalab.org/competitions/15261|||||finance|NLP|Predict which traders respond to firm-up requests|binary classification|||F1|precision, recall, F1|True|False|True|False||[0.0]
Constraint@AAAI2021 - COVID19 Fake News Detection in English|Binary Classification of a given covid19 related post into Real News vs Fake News.|1297|1187.0|2020|3.0|None|None|71.0|https://competitions.codalab.org/competitions/26655|||||covid, news|NLP|COVID19 Fake News Detection in English|binary classification|||weighted_f1|F1|False|False|True|False||[1.1048526863084924]
Photo Triage Benchmark|The goal of the competition is to use machine learning methods to to automatically learn human preferences within a series of photos taken of the same scene.|19|41.0|2016|1.0|None|None|0.0|https://competitions.codalab.org/competitions/9621|||||multimedia|computer vision|learn human preferences within a series of photos taken of the same scene|multi-class classification||15,545 unedited photos|LogLikelihood , Accuracy|accuracy|False|False|True|False||[0.0]
FewRel 2.0 Domain Adaptation|A few-shot relation classification benchmark|82|1579.0|2018|1.0|None|None|0.0|https://competitions.codalab.org/competitions/27981|||||linguistics|NLP||multi-class classification|||MAP|N-way K-shot|True|True|True|False||[1.1897076814951715]
Depth From Focus Competition on the DDFF 12-Scene Dataset|This is a competition to predict a depth map given a focal stack. Results are evaluated on the DDFF 12-Scene dataset.|34|47.0|2017|1.0|None|None|0.0|https://competitions.codalab.org/competitions/17807||https://hazirbas.com/projects/ddff/||https://vision.in.tum.de/data/datasets/ddff12scene|multimedia|computer vision|predict a depth map given a focal stack|regression|||mse (× 10e-4)|ERR, MSE, RMSE|True|False|True|False||[0.0]
Dialogue Evaluation 2020: Taxonomy Enrichment for the Russian Language|Dialogue Evaluation 2020: Taxonomy Enrichment for the Russian Language|95|5220.0|2019|6.0|None|None|351.0|https://competitions.codalab.org/competitions/22168|||||linguistics|NLP|taxonomy enrichment task: Given words that are not yet included in the taxonomy, we need to associate each word with the appropriate hypernyms from an existing taxonomy|ordinal regression|||MAP|MAP, MRR|True|False|True|False||[1.3336461755044582]
Deep Pollination Raw - Explainability|Deep Pollination M1 [AI] 2020-2021|11|22.0|2018|2.0|None|None|11489.0|https://competitions.codalab.org/competitions/29425|||||biology|computer vision|recognize the type of insect in the image|image recognition|||Prediction score|accuracy|False|False|True|True||[1.0091374269005848]
BEA 2019 Shared Task - Grammatical Error Correction - Low Resource Track|This is the Codalab competition corresponding to the BEA 2019 Shared Task on Grammatical Error Correction - Low Resource Track.|54|524.0|2019|3.0|None|None|33.0|https://competitions.codalab.org/competitions/20230|||||linguistics|NLP|error correction|binary classification, text generation|||F_0.5|F0.5|False|False|True|False||[1.0845921450151055]
Predicting Emphasis in Presentation Slides Shared Task|CAD21@AAAI21 Shared Task on Predicting Emphasis in Presentation Slides|36|155.0|2020|3.0|None|None|29.0|https://competitions.codalab.org/competitions/27419|||||linguistics|NLP|Predicting Emphasis in Presentation Slides|binary classification, multi-class classification|||Rank|matchM|False|False|True|False||[1.0540765391014975]
MultiCoNER: SemEval-2022 Task 11 (Post Evaluation Phase)|SemEval-2022 task to develop NER systems for complex entities across 11 languages.|261|2737.0|2022|13.0|None|None|0.0|https://competitions.codalab.org/competitions/36425|||||linguistics|NLP|develop complex Named Entity Recognition systems for 11 languages. The task focuses on detecting semantically ambiguous and complex entities in short and low-context settings.|named entity recognition|||F1|precision, recall, F1|True|False|True|False||[0.0]
FreiHAND|A dataset for hand pose and shape estimation from single color image.|304|10773.0|2019|1.0|None|None|0.0|https://competitions.codalab.org/competitions/21238|||||multimedia|computer vision|hand pose and shape estimation|object recognition|||KP mean error (aligned)|Mesh error, F1|True|False|True|False||[0.0]
CODWOE - Comparing Dictionaries and Word Embeddings|SemEval 2022 Task 1 - Are dictionary glosses and word embedding representations semantically equivalent? Can we generate one from the other?|188|230.0|2022|2.0|None|None|22.0|https://competitions.codalab.org/competitions/34022|||||linguistics|NLP|generate glosses from vectors, generate vectors from glosses|text generation|||MvSc., MSE|MoverScore, BLEU, lemma-level BLEU|True|False|True|False||[0.0]
MinneApple Fruit Segmentation Challenge|We present a new dataset to advance the state-of-the-art in fruit detection, segmentation, and counting in orchard environments. While there has been significant recent interest in solving these problems, the lack of a unified dataset has made it difficult to compare results. We hope to enable direct comparisons by providing a large variety of high-resolution images acquired in orchards, together with human annotations of the fruit on trees.|28|143.0|2019|1.0|None|None|0.0|https://competitions.codalab.org/competitions/21694|||||agriculture|computer vision|mapping in orchard environments from RGB images||||Mean IoU|mIoU, accuracy|True|False|True|False||[1.080797481636936]
Non Parametric Density Estimation|<p>Assignment 3 for Statistical Pattern Recognition and Learning Course. &nbsp,Apply non-parametric density estimation techniques to ecology dataset.</p><p>&nbsp,</p><p>&nbsp,</p>|11|52.0|2015|1.0|None|None|0.0|https://competitions.codalab.org/competitions/6351|||||ecology|NLP||density estimation|||Sylvester |AUC|False|False|True|False||[1.2537670968240475]
SemEval 2021 Task 1 - Lexical Complexity Prediction (LCP)|This is the competition web page of the LCP shared task of SemEval 2021.|191|1167.0|2020|4.0|None|None|74.0|https://competitions.codalab.org/competitions/27420|||||linguistics|NLP|Lexical Complexity Prediction|regression|||Pearson |Spearman correlation (Rho), Mean absolute error (MAE), Mean squared error (MSE), R-squared (R2)|True|False|True|False||[1.1399584533368632]
ETH-XGaze Competition|This is the official competition website for ETH-XGaze dataset. The competition includes the cross-dataset, within-dataset and person-specific competition. Details can be found on individual competition pages.|126|2858.0|2020|4.0|None|None|92.0|https://competitions.codalab.org/competitions/28930|||||multimedia|computer vision|Gaze estimation|regression|||Gaze Error|gaze estimation error|False|False|True|False||[0.0]
Lives :: Learning with Multiple Views|Predict proteins activation during embryo development|11|41.0|2016|1.0|None|None|0.0|https://competitions.codalab.org/competitions/21342|||||biology|computer vision|predict such missing data, a first step in this direction is to predict one view from the others. We then formalize the given task as predicting view 1 given views 0, 2, 3, 4 and 5. This is a multiview regression task|multi-class classification|||MSE|MSE|False|False|True|False||[0.0]
Multi-Hop Inference Explanation Regeneration (TextGraphs-14)|Given every row in the knowledge base (the tablestore), for a given question, can you rerank them so that the rows with the highest ranks are the same as the rows from the gold explanation graph for that question in the WorldTree corpus?|35|171.0|2020|3.0|None|None|205.0|https://competitions.codalab.org/competitions/23615|||||linguistics|NLP|reconstruct gold explanations for elementary science questions, using a new corpus of gold explanations that provides supervision and instrumentation for this multi-hop inference task|ordinal regression|||MAP|MAP, accuracy|True|False|True|False||[1.1649093647743207]
AutoML :: ROUND 4 PRACTICE|"This is a ""clone"", this is NOT the AutoML main competition page. To enter the challenge, go to yellow icon."|38|282.0|2015|2.0|None|None|1850.0|https://competitions.codalab.org/competitions/9031||||||AutoML||multi-class classification, regression|||<Rank>, Set 1||||True|True||[1.456023351509132]
The VoxCeleb Speaker Recognition Challenge 2019 - Audio speaker verification - FIXED training data|The goal of the VoxSRC challenge is to probe how well current methods can recognize speakers from speech obtained 'in the wild'. The task of speaker verification aims to determine whether two samples of speech are from the same person.|223|669.0|2019|2.0|None|None|46.0|https://competitions.codalab.org/competitions/20200|||||speech|signal processing|recognize speakers from speech obtained 'in the wild'|binary classification|||EER|DCF, ERR|True|False|True|False||[0.0]
ACRV Probabilistic Object Detection (PrOD) Challenge - Continuous|An ACRV Robotic Vision Challenge evaluation server for continuous research on probabilistic object detection. Validation and Test-dev servers available for use.|29|105.0|2019|2.0|None|None|205.0|https://competitions.codalab.org/competitions/20595|||||multimedia, robotics|computer vision|provide how certain they are about the detection|object recognition, regression|||PDQ Score|probability-based detection quality (PDQ)|False|False|True|False||[1.0]
Global Wheat Challenge 2020 - Localization and counting|The Global Wheat Challenge 2020 took place from 4th May to 4th August. We propose to centralize the benchmark on the dataset here.|18|27.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/27449|||||agriculture|computer vision|image detection: detect wheat heads from outdoor images of wheat plants, including wheat datasets from around the globe. Using worldwide data, you will focus on a generalized solution to estimate the number and size of wheat heads|image detection, regression|||mAP@0.5|mAP@0.5, accuracy|True|False|True|False||[1.0]
Question Paraphrase Task|This task is about the evaluation of paraphrase of questions|30|265.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/28529|||||linguistics|NLP|generate paraphrased questions, that is questions that have the same meaning but are different in terms of the vocabulary and grammar of the sentence|text generation|||Harmonic Mean|BLEU, precision, mean, PINC|True|False|True|False||[1.9254691689008043]
The Condensed Movies Challenge 2021|The focus of this challenge is text-to-video retrieval. Specifically, we target movie scenes described by high-level semantic caption.|22|39.0|2021|2.0|None|None|70.0|https://competitions.codalab.org/competitions/34124|||||multimedia, movies|computer vision, NLP|text-to-video retrieval:  retrieve 2-3 minute video clips from movies using corresponding high-level natural language descriptions|QA|||GEOMETRIC_MEAN|geometric_mean, R1, R5, R10|True|False|True|False||[1.7218639031813898]
SemEval-2022 Task 09: R2VQ - Competence-based Multimodal Question Answering|The Recipe Reading and Video Question Answering (R2VQ) task is structured as question answering pairs, querying how well a system understands the semantics of recipes, derived from a collection of cooking recipes and videos.|158|108.0|2021|4.0|None|None|178.0|https://competitions.codalab.org/competitions/34056|||||linguistics|computer vision, NLP||QA|||correct|exact match, word-level F1|True|False|True|False||[0.0]
Semantic Segmentation of LArTPC tracks|Why segmenting pixels?In the first step of this challenge, we ask you to classify non-zero pixels into two basic categories of particles: energy deposited by electron/positron, referred to as EM-particle, vs all other particles.  Accurate identification of EM-particle pixels is a crucial task to identify electron neutrino interaction for neutrino oscillation experiments using LArTPC detectors. In a traditional data reconstruction process of LArTPC experiments, this distinction is made after pixels are clustered into individual particles and analysing the topological feature of clustered pixels. However, this is proven to be difficult. Instead, having a pixel-level distinction of EM-particles beforehand can improve the performance of clustering and simplify the rest of the data reconstruction chain. At the second step of the challenge, we will add another distinct label to those pixels that contain energy deposited by protons. Two most basic yet essential neutrino interaction final states contain electron+proton from electron neutrino interaction or muon+proton from muon neutrino interaction. Adding the proton label, therefore, improve the separation power between two interaction channels.Finally, at the third step of the challenge, we will make the simulation sample more realistic by introducing gaps in the data sample which represents an unresponsive part of the detector. Your algorithm needs to overcome this lack of information to be proven useful for an application to real data.|58|178.0|2018|6.0|None|None|57.0|https://competitions.codalab.org/competitions/19818|||||multimedia|computer vision|"pixel classifcation: predict the following classes for each cell (""pixel""): no signal (0), electron/positron (1) [, proton (3)], another particles (2)"|binary classification, multi-class classification|||Accuracy (80% percentile)|accuracy|False|False|True|True||[1.5581567978073432]
SemEval 2022 Task 4: Patronizing and Condescending Language Detection|Somebody is patronizing or condescending when their language denotes a superior attitude towards others, talks down to them, or describes them or their situation in a charitable way, raising a feeling of pity and compassion.Patronizing and Condescending Language (PCL) is often involuntary and unconscious, and the authors using such language are usually trying to help communities in need by e.g., raising awareness, moving the audience to action or standing for the rights of the under-represented. On the other hand, due to its subtlety, subjectivity and the (generally) good intentions behind its use, the audience is often unaware of this diminishing treatment. But PCL can potentially be very harmful, as it feeds stereotypes, routinizes discrimination and drives to greater exclusion.PCL detection is difficult both for humans and NLP systems, due to its subtle nature, its subjectivity and the fair amount of world knowledge and commonsense reasoning required to understand this kind of language. With this task, we expect to push the boundaries of this new challenge in the NLP community.|411|3061.0|2021|3.0|None|None|185.0|https://competitions.codalab.org/competitions/34344|||||linguistics|NLP|Given a paragraph, a system must predict whether or not it contains any form of PCL. Given a paragraph, a system must identify which PCL categories express the condescension.|binary classification, multi-class classification|||F1_Score, Average|F1|False|True|True|False||[0.0]
JEC-QA|A competetion of JEC-QA, the National Judicial Examination of China.|20|89.0|2019|1.0|None|None|0.0|https://competitions.codalab.org/competitions/22173|||||linguistics, legal|NLP|predict the answer using the questions and relevant articles|QA, text generation|||aa|accuracy|False|False|True|False||[0.0]
VISIOCITY Benchmark|Create a model that produces good summaries of videos|5|12.0|2020|1.0|None|None|366.0|https://competitions.codalab.org/competitions/28349|||||multimedia|computer vision, NLP|Create a model that produces good summaries of videos|video recognition, text generation|||Average F1|Average F1, Max F1, Importance, Mega Event Continuity, Diversity Time, Diversity Similarity, Diversity Concept|True|False|True|True||[0.0]
ELEXIS Monolingual Word Sense Alignment Task|This is a competition to develop systems to predict alignment between senses of two monolingual dictionaries in 15 languages|33|248.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/22163|||||linguistics|NLP|monolingual word sense alignment across dictionaries: finding matching senses between two dictionary entries, predicting the relationship between two senses in one of five categories: “exact”, “broader”, “narrower”, “related” or “none”|multi-class classification|||5-Class Accuracy|accuracy, precision, recall, F1|True|False|True|False||[0.0]
TBX11K Tuberculosis Classification and Detection Challenge|The TBX11K challenge for tuberculosis classification and detection|134|920.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/25848|||||medicine|computer vision|Computer-aided tuberculosis diagnosis|multi-class classificaiton|||Accuracy (80% percentile)|accuracy, AUC, AP, AR|True|False|True|False||[0.0]
AutoML :: ROUND 1 PRACTICE|"This is a ""clone"", this is NOT the AutoML main competition page. To enter the challenge, go to yellow icon."|39|263.0|2015|2.0|None|None|1575.0|https://competitions.codalab.org/competitions/7462||||||AutoML|||||<Rank>, Set 1||||True|True||[3.3910443183560806]
The End-of-End-to-End - A Video Understanding Pentathlon (2020)|Searching for content within a corpus of videos using natural language queries.|57|191.0|2020|4.0|None|None|56.0|https://competitions.codalab.org/competitions/24292|||||multimedia|computer vision, NLP|"build a system to retrieve videos from natural language queries. The data for the challenge consists of videos and queries from a ""pentathlon"" of five video retrieval benchmarks. Each dataset comprises a collection of videos with natural language descriptions (some statistics are given below). The final score of a system will be a weighted combination of its performance on the test set of each of the five datasets (more details on the metrics are given below). This can be done by training a single shared model across all the datasets, or training separate models per-dataset."|multi-class classificaiton|||SCORE |score|False|False|True|False||[1.4044016342736114]
TVQA Test Public Evaluation (w/ timestamp at inference) Beta|UNC TVQA test_public set evaluation for models that used ground-truth timestamp annotation at inference|35|65.0|2018|1.0|None|None|0.0|https://competitions.codalab.org/competitions/20414|||||linguistics|NLP|The questions are designed to be compositional, requiring systems to jointly localize relevant moments within a clip, comprehend subtitles-based dialogue, and recognize relevant visual concepts|QA||152.5K QA pairs from 21.8K video clips, spanning over 460 hours of video|bbt|accuracy|False|False|True|False||[0.0]
Grass Clover Dataset Semantic Segmentation and Biomass Composition Challenges|In this competition, the participants must perform pixel-wise classification in grass clover images and predict the biomass composition of the mixed crops.|62|301.0|2019|1.0|None|None|0.0|https://competitions.codalab.org/competitions/21122|||||agriculture|computer vision|pixel-wise classifiction on hand-labeled collected images, predict the biomass composition (in fractions) of a subset of the biomass-labeled images|multi-class classification, regression|||IoU [%] mean|IoU|False|False|True|False||[0.0]
WASSA 2021 Shared Task|Shared Task on Empathy Detection and Emotion Classification, organized as part of WASSA 2021|31|223.0|2020|3.0|None|None|65.0|https://competitions.codalab.org/competitions/28713|||||linguistics|NLP|Empathy Detection and Emotion Classification: predict empathy and emotion based on essays written in reaction to news articles where there is harm to a person, group, or other|binary classification, multi-class classification, ordinal regression|||Averaged Pearson Correlations, Macro F1-Score|average of the two Pearson correlations, F1|True|False|True|False||[0.0]
Memotion 2.0|A Hate Speech Detection dataset released as part of the De-Factify workshop in AAAI-21|99|233.0|2021|2.0|None|None|44.0|https://competitions.codalab.org/competitions/35688|||||linguistics|NLP|Hate Speech Detection. Task A- Sentiment Analysis: Given an Internet meme, the first task is to classify it as a positive, negative or neutral meme.  Task B- Emotion Classification: Given an Internet meme, the system has to identify the type of emotion expressed. The categories are humorous, sarcastic, offensive and motivational.  A meme can have more than one category. Task C- Scales/Intensity of Emotion Classes: The third task is to quantify the extent to which a particular emotion is being expressed. |multi-class classification, ordinal regression|||Correct|F1|False|False|True|False||[1.2]
SemEval19 Task 3 : EmoContext|In this task, you are given a textual dialogue, you have to classify the emotion of last user utterance.|284|325.0|2018|2.0|None|None|142.0|https://competitions.codalab.org/competitions/19790|||||linguistics|NLP|Contextual Emotion Detection|multi-class classification|||F1Score|F1|False|False|True|True||[0.0]
The VoxCeleb Speaker Recognition Challenge 2019 - Audio speaker verification - OPEN training data|The goal of the VoxSRC challenge is to probe how well current methods can recognize speakers from speech obtained 'in the wild'. The task of speaker verification aims to determine whether two samples of speech are from the same person.|120|356.0|2019|2.0|None|None|46.0|https://competitions.codalab.org/competitions/20199|||||speech|signal processing|determine whether two samples of speech are from the same person|binary classification|||EER|ERR|False|False|True|False||[0.0]
The VoxCeleb Speaker Recognition Challenge 2021 - Track 4 (Diarisation, open)|The goal of the VoxSRC challenge is to probe how well current methods can recognize speakers from speech obtained 'in the wild'. The task of speaker diarisation is to break up multi-speaker audio into homogenous single speaker segments, effectively solving ‘who spoke when’.|76|252.0|2021|2.0|None|None|33.0|https://competitions.codalab.org/competitions/34113|||||speech|signal processing|break up multi-speaker audio into homogenous single speaker segments, effectively solving ‘who spoke when’|multi-class classification, ordinal regression|||DER|DER, JER|True|False|True|False||[0.0]
Benchmarking - Malaria|benchmarking of Malaria Detection Algorithms|9|14.0|2020|3.0|None|None|0.0|https://competitions.codalab.org/competitions/29097|||||medicine|NLP|detection of Malaria|binary classification|||This benchmark only allow code submission. Metrics are: ROC AUC, Precision, Recall, MAP|ROC AUC, Precision, Recall, MAP|True|False|True|True||[0.0]
The VoxCeleb Speaker Recognition Challenge 2020 - Track 4 (Diarisation, open)|The goal of the VoxSRC challenge is to probe how well current methods can recognize speakers from speech obtained 'in the wild'. The task of speaker diarisation is to break up multi-speaker audio into homogenous single speaker segments, effectively solving ‘who spoke when’.|81|139.0|2020|2.0|None|None|37.0|https://competitions.codalab.org/competitions/26357|||||speech|signal processing|break up multi-speaker audio into homogenous single speaker segments, effectively solving ‘who spoke when’|multi-class classification, ordinal regression|||DER|DER, JER|True|False|True|False||[0.0]
SemEval 2019 Task 5 - Shared Task on Multilingual Detection of Hate|Given a tweet and a target (Woman or Immigrant), determine the presence of hate speech and whether it is against an individual or a group.|503|35.0|2018|9.0|None|None|143.0|https://competitions.codalab.org/competitions/19935|||||linguistics, tweets|NLP|Given a tweet and a target (Woman or Immigrant), determine the presence of hate speech and whether it is against an individual or a group.|binary classification||||accuracy, precision, recall, F1, partial match, exact match|True|True|True|True||[0.0]
eHealth KD @ IberLEF 2021 (Official Competition)|Automated knowledge extraction from electronic health documents.|16|69.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/30830|||||linguistics, health|NLP|Entity recognition, Relation extraction|||||F1, precision, recall|True|True|True|True||[0.0]
Multi-Hop Inference Explanation Regeneration (TextGraphs-13)|Given every row in the knowledge base (the tablestore), for a given question, can you rerank them so that the rows with the highest ranks are the same as the rows from the gold explanation graph for that question in the WorldTree corpus?|33|150.0|2019|2.0|None|None|85.0|https://competitions.codalab.org/competitions/20150|||||linguistics|NLP|reconstruct gold explanations for elementary science questions, using a new corpus of gold explanations that provides supervision and instrumentation for this multi-hop inference task|ordinal regression|||MAP|MAP, accuracy|True|False|True|False||[1.304412641621944]
HaHackathon: Detecting and Rating Humor and Offense|Detecting if a text is humorous or offensive on a demographically diverse corpus|405|4337.0|2020|3.0|None|None|123.0|https://competitions.codalab.org/competitions/27446|||||linguistics|NLP|Detecting if a text is humorous or offensive on a demographically diverse corpus: Is the intention of this text to be humorous? (0 or 1) [If it is intended to be humorous] How humorous do you find it? (1-5)|binary classification, ordinal regression|||F-Score, RMSE|F1, RMSE|True|True|True|False||[0.0]
2019 IEEE GRSS Data Fusion Contest Pairwise Semantic Stereo Challenge|2019 IEEE GRSS Data Fusion Contest Pairwise Semantic Stereo Challenge|41|224.0|2018|3.0|None|None|117.0|https://competitions.codalab.org/competitions/20212|||||aerial, urbanisme|computer vision|predict both 3D geometric models and segmentation maps of semantic classes for urban scenes from pairs of epipolar rectified satellite images|multi-class classification|||Prediction |mIoU|False|False|True|False||[1.2063351842503787]
Automated Deep Learning Self-Service|Here you can submit datasets to Deep Wisdom (winner of AutoDL challenge)|15|66.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/27082||||||AutoML|produce Automated Deep Learning solutions which were evaluated on datasets covering a wide range of domains (image, video, speech, text and tabular) for multilabel (or multiclass) classification tasks|multi-class classification|||Prediction score||||True|True||[0.0]
EmotionGIF 2020, Round 2 (Main Track)|Predict the category for an animated GIF reaction|96|1108.0|2020|3.0|None|None|22.0|https://competitions.codalab.org/competitions/25174|||||multimedia|computer vision|Predict the category for an animated GIF reaction|multi-class classification|||P (all)|P, P1, P2|True|False|True|False||[1.2802045894387615]
2021 IEEE GRSS Data Fusion Contest Track MSD|2021 IEEE GRSS Data Fusion Contest Track MSD|166|2205.0|2021|3.0|20000|20000|67.0|https://competitions.codalab.org/competitions/27956|||||multimedia|computer vision|Multitemporal Semantic Change Detection: land cover change detection and classification from multitemporal, multiresolution, and multispectral imagery, create bitemporal high resolution land cover maps using only low-resolution and noisy land cover labels for training|multi-class classification, image generation|||Prediction |IoU|False|False|True|False||[1.1781938023527259]
Multimedia Automatic Misogyny Identification (MAMI)|SemEval 2022 Task 5 - identification of misogyny in internet memes.|426|2339.0|2021|3.0|None|None|186.0|https://competitions.codalab.org/competitions/34175|||||linguistics|NLP|identification of misogyny in internet memes|binary classification|||Task A, Task B|F1|False|False|True|False||[0.0]
Sentence-level Post-Editing Effort QE shared task 2021|WMT'21 Quality Estimation shared task, submission platform for predictions at sentence-level *Post-Editing Effort* (Task 2 sent-level).|15|272.0|2021|12.0|None|None|15.0|https://competitions.codalab.org/competitions/33412|||||linguistics|NLP|(i) a zero-shot sentence-level prediction task to encourage language independent and unsupervised approaches; (ii) a task on predicting catastrophic, i.e. critical translation errors, in other words, errors that make the translation convey a completely different meaning, which could lead to negative effects such as safety risks|machine translation, multi-class classification, regression|||Pearson r|HTER, Pearson, RMSE, MAE|True|True|True|False||[1.197995386206348]
BBO (GINA+MLP+SGD)|Black Box Optimization (BBO) challenge (GINA+MLP+SGD)|32|246.0|2021|1.0|None|None|0.0|https://competitions.codalab.org/competitions/28609|||||multimedia|computer vision|find the best black-box optimizer for machine learning|hyper-parameter selection/tuning|||Score|accuracy|False|False|True|True||[1.058854384070396]
Holistic 3D Vision Challenge - HoliCity Plane Detection Track @ ECCV 2020|Learning to detect planes with HoliCity CAD dataset|49|45.0|2020|3.0|None|None|70.0|https://competitions.codalab.org/competitions/24942|||||multimedia|computer vision|Learning to detect planes with HoliCity CAD dataset: detect all the planes along with their positions and normals given a perspective RGB image|multi-class classification, ordinal regression|||APDR |APDR, mAP|True|False|True|False||[nan]
SemEval 2017 Task 12: Clinical TempEval||106|213.0|2016|4.0|None|None|244.0|https://competitions.codalab.org/competitions/15621|||||medicine|NLP|make predictions on brain cancer patients|multi-class classification, binary classification, ordinal regression|||All spans + all properties F1|precision, recall, F1|True|False|True|False||[0.0]
Sentence-level Critical Error Detection shared task 2021|WMT'21 Quality Estimation shared task, submission platform for predictions at sentence-level *Critical Error Detection* (Task 3).|17|220.0|2021|4.0|None|None|0.0|https://competitions.codalab.org/competitions/33414|||||linguistics|NLP|error detection:  (i) a zero-shot sentence-level prediction task to encourage language independent and unsupervised approaches; (ii) a task on predicting catastrophic, i.e. critical translation errors, in other words, errors that make the translation convey a completely different meaning, which could lead to negative effects such as safety risks|machine translation, binary classification|||MCC|MCC, F1, Critical Error Detection binary scores|True|False|True|False||[0.0]
HO3D (version 3)|This is a competition to evaluate different methods for hand pose estimation when interacting with objects under severe occlusions using HO3D (version 3) dataset.|46|180.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/33267|||||multimedia|computer vision|hand pose estimation|object detection|regression||Mean joint error (procrustes alignment), Mean joint error|Mesh error, Mean joint error, F1, AUC|True|True|True|False||[0.0]
WiC competition|This is a competition based on the WiC (Word-in-Context) evaluation dataset.|126|541.0|2018|3.0|None|None|176.0|https://competitions.codalab.org/competitions/20010|https://pilehvar.github.io/wic/||||linguistics|NLP|identify the intended meaning of words. WiC is framed as a binary classification task. Each instance in WiC has a target word w, either a verb or a noun, for which two contexts are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in the two contexts correspond to the same meaning or not. |binary classification|||accuracy |accuracy|False|False|True|False||[1.3225593356296892]
TVR Evaluation|UNC TVR test-public set evaluation.|54|156.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/22780||https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660443.pdf|||multimedia|computer vision, NLP|video-subtitle moment retrieval||||R@1|R@1, R@10,R@100|True|False|True|False||[0.0]
MultiCoNER: SemEval-2022 Task 11 (Practice Phase)|SemEval-2022 task to develop NER systems for complex entities across 11 languages.|408|2497.0|2021|13.0|None|None|5.0|https://competitions.codalab.org/competitions/36044|||||linguistics|NLP|Named Entity Recognition systems for 11 languages|named entity recognition|||F1|F1, precision, recall|True|False|True|False||[1.130592152888374]
DeftEval 2020 (SemEval 2020 - Task 6)|Identify and extract term and definition pairs in free text.|185|1009.0|2019|6.0|None|None|211.0|https://competitions.codalab.org/competitions/22759|||||health, tweets|NLP|Subtask 1: Sentence Classification, Subtask 2: Sequence Labeling, Subtask 3: Relation Classification|multi-class classification, binary classification|||SUBTASK 1 Accuracy|F1, precision, recall|True|True|True|False||[0.0]
SMM4H'19 - Shared Task|The SMM4H shared task proposed four NLP challenges on social media mining for health monitoring and surveillance, 1. Automatic classifications of adverse effects mentions in tweets, 2. Extraction of adverse effect mentions, 3.  Normalization of adverse drug reaction mentions, 4. Identification of a first-person mention of a health concern or condition in tweets|109|1153.0|2019|12.0|None|None|108.0|https://competitions.codalab.org/competitions/20798|||||multimedia, tweets, health|NLP|Task 1: Automatic classifications of adverse effects mentions in tweets. Task 2: Extraction of adverse effect mentions. Task 3: Normalization of adverse drug reaction mentions. Task 4: Generalizable identification of personal health experience mentions|multi-class classification, binary classification|||F1|F1, precision, recall, accuracy|True|True|True|False||[0.0]
NCSU USDA Hackathon|In this competition, the participants must perform image classification of weeds.|27|29.0|2021|1.0|None|None|0.0|https://competitions.codalab.org/competitions/35694|||||agriculture|computer vision|implement and train deep learning models to classify 43 weed species from images|multi-class classification|||Category-level accuracy [%] mean|accuracy|False|False|True|False||[0.0]
SemEval-2018 task 7 Semantic Relation Extraction and Classification in Scientific Papers|This is the CodaLab competition page for the 2018 SemEval task on Semantic Relation Extraction and Classification in Scientific Papers.|195|756.0|2017|7.0|None|None|606.0|https://competitions.codalab.org/competitions/17422|||||linguistics, science|NLP|semantic relation extraction and classification into 6 categories|multi-class classification|||2_extraction_F1|F1, precision, recall|True|True|True|False||[0.0]
SemEval 2022 Task 2|Competition Website for SemEval 2022 Task 2 - Multilingual Idiomaticity Detection and Sentence Embedding|103|1543.0|2021|3.0|None|None|151.0|https://competitions.codalab.org/competitions/34710|||||linguistics|NLP|Multilingual Idiomaticity Detection and Sentence Embedding|multi-class classification|||F1 Score|F1, Pearson correlation|True|True|True|False||[0.0]
Headstart IMDB Competition|Can you predict if a review is positive or negative using only its text?|28|65.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/25363|||||movies|NLP|sentiment analysis: given the text of a review, you should classify it as positive or negative|binary classification|||correct |accuracy|False|False|True|False||[1.194220908036082]
SemEval 2019, Task 10 -- Math Question Answering|Evaluates question answering systems on their ability to solve Math SAT questions.|53|24.0|2018|4.0|None|None|212.0|https://competitions.codalab.org/competitions/20013|||||linguistics, maths|NLP|Evaluates question answering systems on their ability to solve Math SAT questions.|QA, multi-class classification|||Accuracy (overall)|accuracy|False|False|True|False||[1.0]
Legacy version - Multimodal Emotion Recognition on Comics scenes (EmoRecCom)|The emotions of comic characters are described by the Visual information, the Text in speech Balloons or Captions and the Onomatopoeia (Comic drawings of words that phonetically imitates, resembles, or suggests the sound that it describes). The task hence is a multi-modal analysis task which can take advantages from both fields computer vision and natural language processing which are two of the main interests of the ICDAR community.|16|125.0|2020|3.0|None|None|98.0|https://competitions.codalab.org/competitions/30954|||||multimedia, comics|computer vision, NLP|Multimodal Emotion Recognition on Comics scenes|multi-class classification|||ROC-AUC|ROC-AUC|False|False|True|False||[1.1033447098976108]
SemEval 2020 Task 3 - Predicting the (Graded) Effect of Context in Word Similarity|We ask participants to try to predict the effect that context has in human perception of similarity of words.|244|2054.0|2019|6.0|None|None|216.0|https://competitions.codalab.org/competitions/20905|||||linguistics|NLP|predict the effect that context has in human perception of similarity of words.|ordinal regression|||score|average score of similarity|False|False|True|False||[1.215873015873016]
Statistical NLP - Assignment 1 - Word Sense Induction and Disambiguation|Word Sense Induction and Disambiguation for the Russian Language|24|1347.0|2021|9.0|None|None|35.0|https://competitions.codalab.org/competitions/36019|||||linguistics|NLP|cluster these contexts in the (unknown in advance) number of clusters which correspond to various senses of the word|cluterisation|||ARI|Adjusted Rand Index (ARI)|False|False|True|True||[1.8608521970705727]
SemanticKITTI: 4D Panoptic Segmentation|4D Panoptic Segmentation for automotive LiDAR point cloud sequences|39|33.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/26369|||||multimedia|computer vision|provide instance IDs and semantic labels for each point of the test sequences 11-21|multi-class classification, ordinal regression|||PQ4D |LiDAR Segmentation and Tracking Quality (LSTQ)|False|False|True|False||[1.0499175888862728]
SemEval-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC)|The task aims at determining whether two occurrences of the same word/translation are used with same meaning in two different sentences, within and across languages.|193|239.0|2020|3.0|None|None|123.0|https://competitions.codalab.org/competitions/27054|||||linguistics|NLP|determining whether two occurrences of the same word/translation are used with same meaning in two different sentences, within and across languages: perform a binary classification task in which they indicate whether the target word is used in the same meaning (tagged as T for true) or in a different meaning (F for false) in the same language (multilingual dataset) or across different languages (cross-lingual dataset).|binary classification||||accuracy|False|False|True|False||[0.0]
SemEval-2018 Task 5: Counting Events and Participants in the Long Tail|Competition for the SemEval-2018 Task 5 entitled Counting Events and Participants within Highly Ambiguous Data covering a very long tail.|68|177.0|2017|3.0|None|None|169.0|https://competitions.codalab.org/competitions/17285|||||linguistics|NLP|Counting Events and Participants: provide a numeric answer together with the supporting documents that directly relate to and support the answer, provide answers to questions about the number of incidents of an event type (subtasks S1 and S2) or participants in roles (subtask S3)|regression|||S1 DOC F1|RMSE, F1, precision, recall, BCUB, BLANC, CEAF_E, CEAF_M, and MUC|True|True|True|False||[1.6971849375881338]
Factify|A Multi-Modal Fact Verification dataset released as part of the De-Factify workshop in AAAI-21|118|60.0|2021|2.0|None|None|50.0|https://competitions.codalab.org/competitions/35153|||||fake news|computer vision, NLP|fact/claim verification: classify the claims into support, not-enough-evidence and refute categories|multi-class classification||50,000 claims accompanied with 100,000 images|Correct |F1|False|False|True|False||[1.6666666666666667]
SemanticKITTI: Panoptic Segmentation|Panoptic segmentation of automotive LiDAR point cloud sequences|147|175.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/24025|||||multimedia, urbanisme|computer vision|provide instances and labels for each point of the test sequences 11-21. Each method should then output a label and instance for each point of a scan, i.e., one full turn of the rotating LiDAR sensor|multi-class classification, regression|||PQ|panoptic quality (PQ)|False|False|True|False||[1.2072357981593143]
HO3D (version 2)|This is competition to evaluate different methods for hand pose estimation when interacting with objects under severe occlusions using HO3D dataset.|128|3736.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/22485|||||multimedia|computer vision|hand pose estimation|object detection|regression|3D annotations for more than 65 sequences captured with 10 different subjects and 10 objects. approximately 80K images|Mean joint error (procrustes alignment), Mean joint error |Mesh error, Mean joint error, F1, AUC|True|False|True|False||[0.0]
YouCook2 Dense Video Captioning|YouCook2 is the largest task-oriented, instructional video dataset in the vision community.|12|13.0|2018|1.0|None|None|0.0|https://competitions.codalab.org/competitions/20594||https://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0037.pdf|||multimedia|computer vision, NLP|Dense video captioning aims to generate text descriptions for all events in an untrimmed video|text generation, multi-class classification||2000 long untrimmed videos from 89 cooking recipes|METEOR |METEOR, Bleu 1, Bleu 2, Bleu 3, Bleu 4, CIDEr, ROUGE L,  Precision, Recall|True|False|True|False||[1.0154211150652432]
LOVEU@CVPR'21 GEBD Sub-track 1.1|This competition is for generic event boundary detection with no constraint of training data and pre-trained models.|50|103.0|2021|2.0|None|None|37.0|https://competitions.codalab.org/competitions/30309|||||multimedia|computer vision|event boundary detection: detecting generic, taxonomy-free event boundaries that segment a whole video into chunks|regression|||F1 Score |precision, recall and F1|True|False|True|True||[1.0801002221418607]
The 1st Video Panoptic Segmentation Challenge|This challenge evaluates algorithms for video panoptic segmentation on Cityscapes-VPS test set.|72|213.0|2020|1.0|None|None|0.0|https://competitions.codalab.org/competitions/26183||https://arxiv.org/pdf/2006.11339.pdf|||multimedia|computer vision|simultaneous prediction of object classes, bounding boxes, masks, instance id associations, and semantic segmentation, while assigning unique answers to each pixel in a video|multi-class classification, regression, visual recognition|||vpq_all|video panoptic quality (VPQ)|False|True|True|False||[0.0]
ASVspoof 2021 - Post-Challenge - Speech Deepfake track|Automatic Speaker Verification Spoofing and Countermeasures Challenge|35|416.0|2021|1.0|None|None|0.0|https://competitions.codalab.org/competitions/35159|||||speech|signal processing|Automatic Speaker Verification Spoofing and Countermeasures: spoofing counter-measure (CM) system|spoofing counter-measure (CM)|||EER|ERR|False|False|True|False||[0.0]
GermEval 2019 Task 1 -- Shared task on hierarchical classification of blurbs|GermEval is a series of shared task evaluation campaigns that focus on Natural Language Processing for the German language. This shared task is about classifying blurbs (short descriptions / teasers for books) into hierarchichal categories. This can be seen as a multi-label classification task.|57|99.0|2019|4.0|None|None|198.0|https://competitions.codalab.org/competitions/20139|||||linguistics|NLP|classifying blurbs (short descriptions / teasers for books) into hierarchichal categories. This can be seen as a multi-label classification task.|multi-class classification, ordinal regression|||< Rank >|F1|False|True|True|False||[1.0]
Sentence-level Post-Editing Effort QE shared task 2020|WMT'20 Quality Estimation shared task, submission platform for predictions at sentence-level *Post-Editing Effort* (Task 2)|78|720.0|2020|2.0|None|None|0.0|https://competitions.codalab.org/competitions/24515|||||linguistics|NLP|automatic methods for estimating the quality of neural machine translation output at run-time, without relying on reference translations, score sentences according to HTER scores|machine translation|||Pearson r|HTER, Pearson, RMSE, MAE|True|False|True|False||[1.1395087683747478]
KPA 2021 shared task|Quantitive Summarisation - Key Point Analysis Shared Task|44|376.0|2021|5.0|None|None|61.0|https://competitions.codalab.org/competitions/31166|||||linguistics|NLP|produce a succinct list of the most prominent key-points in the input corpus, along with their relative prevalence. Thus, the output of KPA is a bullet-like summary, with an important quantitative angle and an associated well-defined evaluation framework. Track 1 - Key-Point Matching. Track 2 - Key Points Generation and Matching|multi-class classification, ordinal regression, text generation|||mAP Strict |AP, Key points matching|True|True|True|False||[1.0393613070924619]
2019 IEEE GRSS Data Fusion Contest Multi-view Semantic Stereo Challenge|2019 IEEE GRSS Data Fusion Contest Multi-view Semantic Stereo Challenge|27|16.0|2019|3.0|None|None|44.0|https://competitions.codalab.org/competitions/20216|||||multimedia, urbanisme|computer vision|predict both 3D geometric models and segmentation maps of semantic classes for urban scenes from sets of unrectified multi-view satellite images|multi-class classification, image recognition|||Prediction |IoU|False|False|True|False||[1.1344762503932055]
SemEval-2018 Task 2, Multilingual Emoji Prediction|This is a SemEval shared task on emoji prediction in English and Spanish|397|315.0|2017|3.0|None|None|243.0|https://competitions.codalab.org/competitions/17344|||||linguistics, tweets|NLP|emoji prediction: predict, given a tweet in English or Spanish, its most likely associated emoji|multi-class classification||500k tweets in English and 100K tweets in Spanish|english , spanish|precision, recall, F1|True|False|True|False||[2.196856838831137]
CLEF 2019 Lab ProtestNews Extended|COPE 2019 - Extracting Protests from News Using Automated Methods|17|170.0|2019|4.0|None|None|35.0|https://competitions.codalab.org/competitions/20288|||||linguistics, news|NLP|develop text classification and information extraction tools on one country and test them on data from different countries. Task 1 : The task is to classify news documents as protest (1) or non-protest (0), given the raw document.  Task 2 : The task is to classify sentences as a sentence containing an event-trigger (1) or not (0), given the sentence and the news article containing that sentnece. Task 3 : The task is to extract various information from a given event sentence such as location, time and participant of an event. |binary classification|||Task1, Task2, Task 3|F1|False|True|True|False||[0.0]
EVE: End-to-end Video-based Eye Tracking|A video-based gaze estimation challenge using the EVE public dataset.|49|132.0|2021|2.0|None|None|105.0|https://competitions.codalab.org/competitions/28954||https://ait.ethz.ch/projects/2020/EVE/, https://arxiv.org/abs/2007.13120|||multimedia|computer vision|gaze estimation|regression, image recognition, eye-tracking|||Gaze Direction Angular Error / deg|mean angular error|False|False|True|False||[0.0]
Multimodal translation shared task|WMT'18 Multimodal MT shared task submission platform|28|129.0|2018|4.0|None|None|0.0|https://competitions.codalab.org/competitions/19917|https://www.statmt.org/wmt18/multimodal-task.html|||||computer vision, NLP|generation of image descriptions in a target language|machine translation, text generation, image recognition|||Meteor |Meteor, Bleu|True|True|True|False||[1.0205396208070006]
SemEval 2018 Task 8: SecureNLP (Semantic Extraction from CybersecUrity REports using NLP)|Extract entities, relations, and malware attributes from a cybersecurity report.|211|538.0|2017|3.0|None|None|243.0|https://competitions.codalab.org/competitions/17262||https://aclanthology.org/P17-1143/|||linguistics, cybersecurity |NLP|SubTask1: Classify relevant sentences to malware, SubTask2: Predict token labels, SubTask3: Predict relation labels, SubTask4: Predict attribute labels|binary classification, multi-class classification|||SubTask1|F1|False|True|True|False||[1.3212435233160622]
To be, or not to be?|Prediction of mortality given medical records.|73|530.0|2018|1.0|None|None|0.0|https://competitions.codalab.org/competitions/27605|||||medicine|NLP|predict mortality|binary classification|||Balanced accuracy|accuracy|False|False|True|False||[1.4249336870026528]
The VoxCeleb Speaker Recognition Challenge 2021 - Track 2 (Verification fully supervised, open)|The goal of the VoxSRC challenge is to probe how well current methods can recognize speakers from speech obtained 'in the wild'. The task of speaker verification aims to determine whether two samples of speech are from the same person.|101|352.0|2021|2.0|None|None|33.0|https://competitions.codalab.org/competitions/34066|||||speech|signal processing|determine whether two samples of speech are from the same person|binary classification|||DCF|DCF, ERR|True|False|True|False||[0.0]
SemEval 2018 Task 10 Capturing Discriminative Attributes|Capturing Discriminative Attributes is a SemEval 2018 competition.|223|129.0|2017|3.0|None|None|244.0|https://competitions.codalab.org/competitions/17326|||||linguistics|NLP|Capturing Discriminative Attributes: semantic difference detection, predict whether a word is a discriminative attribute between two other words|binary classification|||correct |F1|False|False|True|False||[1.1595006934812762]
SemEval-2018 Task 6: Parsing Time Normalizations|Parsing Time Normalization|57|13.0|2017|3.0|None|None|169.0|https://competitions.codalab.org/competitions/17286|||||linguistics|NLP|time normalization based on recognizing semantically compositional time operators. Track 1: Parse text to time operators. Systems must identify time operators in text and link them correctly to signal how they have to be composed. Track 2: Produce time intervals. Systems can participate through Track 1 or by providing a TimeML annotations. In both cases, the intervals are inferred by our interpreter.|regression|||Track 1 - Parsing, Track 2 - Intervals, F1|precision, recall, F1|True|True|True|False||[0.0]
Russian sQuAD (sberQuAD)|sQuAD competition based on the data from sberQuAD 2017|75|292.0|2021|1.0|None|None|0.0|https://competitions.codalab.org/competitions/34843|||||linguistics|NLP|reading comprehension|QuAD|||f1|EM, F1|True|False|True|False||[1.0201845444059974]
MOCS: Detecting Moving Objects in Construction Sites|MOCS dataset is a large-scale image dataset for detecting Moving Objects in Construction Sites. All the images in MOCS dataset are collected from real construction sites. There are 41,668 images in dataset and 13 categories of objects are annotated. For details of the dataset, readers are refer to our research article.|20|51.0|2021|2.0|None|None|0.0|https://competitions.codalab.org/competitions/32605|||||multimedia, urbanisme|computer vision|detecting Moving Objects in Construction Sites|object detection|||AP|mAP, AP|True|False|True|False||[1.0]
SemEval 2022 Task 8: Multilingual News Article Similarity|Consider the following question: Given a pair of news articles, are they covering the same news story? Answering this question likely requires knowing specific aspects of the events covered:  what happened, where and when it happened, who was involved, and why and how it happened. This SemEval task aims to develop systems that identify multilingual news articles that provide similar information. This is a document-level similarity task in the applied domain of news articles, rating them on a continuous scale.|237|1238.0|2021|3.0|None|None|182.0|https://competitions.codalab.org/competitions/33835|||||linguistics, news|NLP|develop systems that identify multilingual news articles that provide similar information|binary classification, similarity detection|||correlation |Overall Similarity|False|False|True|False||[0.0]
UIoU Dark Zurich at Vision for All Seasons Workshop, CVPR 2020|UIoU Dark Zurich benchmarks semantic segmentation performance with the Uncertainty-aware IoU metric on the Dark Zurich dataset, presented in the paper Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation, ICCV 2019.|217|1380.0|2020|2.0|None|None|86.0|https://competitions.codalab.org/competitions/23553|||||multimedia|computer vision|Semantic Nighttime Image Segmentation: uncertainty-aware semantic nighttime image segmentation|image segmentation|||UIoU |UIoU, mIoU, IoU|True|False|True|False||[2.124870920129972]
RuSimpleSentEval (RSSE)|sentence simplification: get a simple sentence from a difficilt/complex one|40|433.0|2021|2.0|None|None|32.0|https://competitions.codalab.org/competitions/29037|||||linguistics|NLP|Sentence Simplification|text generation|||SARI|SARI (System output Against References and against the Input sentence)|False|False|True|False||[1.0330742215889084]
SemEval-2018 Task 11: Machine Comprehension using Commonsense Knowledge|This is the CodaLab competition for the SemEval 2018 Task 11: Machine Comprehension using Commonsense Knowledge.|286|327.0|2017|3.0|None|None|169.0|https://competitions.codalab.org/competitions/17184|||||linguistics|NLP|answer multiple-choice questions based on this text|QA, multi-class classification|||correct |accuracy|False|False|True|False||[1.227547968191435]
MinneApple Fruit Detection Challenge|We present a new dataset to advance the state-of-the-art in fruit detection, segmentation, and counting in orchard environments. While there has been significant recent interest in solving these problems, the lack of a unified dataset has made it difficult to compare results. We hope to enable direct comparisons by providing a large variety of high-resolution images acquired in orchards, together with human annotations of the fruit on trees.|46|696.0|2019|1.0|None|None|0.0|https://competitions.codalab.org/competitions/21718|||||agriculture|computer vision|mapping in orchard environments from RGB images||||AP at IoU=.50:.05:.95|APs|True|False|True|False||[1.255441860465116]
Million-AID Multi-class Scene Classification|An evaluation platform for Multi-class aerial scene parsing with Million-AID.|9|142.0|2021|1.0|None|None|0.0|https://competitions.codalab.org/competitions/35945|||||multimedia, aerial|computer vision|multi-class aerial scene classification|multi-class classification, image recognition|||Kappa |overall accuracy (OA), average accuracy (AA), and Kappa coefficient (Kappa)|True|False|True|False||[1.2127426424546024]
Leaf Segmentation Challenge (LSC)|This is a rebuild of the LSC competition which has been run as challenge with CVPPP2017.|197|9174.0|2018|2.0|0|0|1.0|https://competitions.codalab.org/competitions/18405|||||biology|computer vision|leaf segmentation|multi-class classification, image recognition|||FgBgDice |bestDice, absDiffFG, diffFG, FgBgDice|True|False|True|False||[0.0]
SemEval-2019 Task 12 - Toponym Resolution in Scientific Papers|This challenge on toponym resolution, also known as geoparsing, geo-grounding or place name resolution, aims to assign geographic coordinates to all location names mentioned in documents.|36|61.0|2018|9.0|None|None|382.0|https://competitions.codalab.org/competitions/19948|||||science|NLP|assign geographic coordinates to all location names mentioned in documents: First, toponym detection or geotagging, where the span of place names mentioned in a document is noted. Second, toponym disambiguation or geocoding, where each name found is mapped to latitude and longitude coordinates corresponding to the centroid of its physical location.|binary classification, geocoding||||precision, recall, F1|True|False|True|False||[0.0]
AutoML :: ROUND 2 PRACTICE|"This is a ""clone"", this is NOT the AutoML main competition page. To enter the challenge, go to yellow icon."|43|242.0|2015|2.0|None|None|1851.0|https://competitions.codalab.org/competitions/7461||||||AutoML|||||<Rank>, Set 1||||True|True||[-9.352522935779817]
Visual Domain Decathlon|The goal of this challenge is to solve simultaneously ten image classification problems representative of very different visual domains.|135|585.0|2017|2.0|None|None|70.0|https://competitions.codalab.org/competitions/17001|https://www.robots.ox.ac.uk/~vgg/decathlon/#overview|https://arxiv.org/abs/1705.04228|||multimedia|computer vision|solve simultaneously ten image classification problems representative of very different visual domains|image recognition|||total|top-1 classification error|False|False|True|False||[1.7037779101678387]
2017 Visual Domain Adaptation (VisDA2017) Segmentation Challenge|The VisDA challenge aims to test domain adaptation methods’ ability to transfer source knowledge and adapt it to novel target domains.|88|238.0|2017|2.0|None|None|81.0|https://competitions.codalab.org/competitions/17054|https://ai.bu.edu/visda-2017/||||multimedia|computer vision|test domain adaptation methods’ ability to transfer source knowledge and adapt it to novel target domains|visual recognition|||MeanIoU|IoU, mIoU|True|False|True|False||[0.0]
WebVision Challenge on Visual Understanding by Learning from Web Data - Image Classification Track|The recent success of deep learning has shown that a deep architecture in conjunction with abundant quantities of labeled training data is the most promising approach for most vision tasks.  However, annotating a large-scale dataset for training such deep neural networks is costly and time-consuming, even with the availability of scalable crowdsourcing platforms like Amazon’s Mechanical Turk.  As a result, there are relatively few public large-scale datasets (e.g., ImageNet and Places2) from which it is possible to learn generic visual representations from scratch.|151|49.0|2017|2.0|None|None|100.0|https://competitions.codalab.org/competitions/16439|||||multimedia|computer vision||image recognition|||top-5 accuracy|accuracy|False|False|True|False||[1.0582861107898693]
IGLU NeurIPS 2021 Competition: Silent Builder|IGLU Silent Builder|9|37.0|2021|1.0|7000|7000|127.0|https://codalab.lisn.upsaclay.fr/competitions/387||||||RL|Interactive Grounded Language Understanding in a Collaborative Environment. The primary goal of the competition is to approach the problem of how to build interactive agents that learn to solve a task while provided with grounded natural language instructions in a collaborative environment|reinforcement learning|||Completion Rate|completion rate, success rate, episode reward|True|False|True|True||[1.1505789076264787]
Western Power Distribution Data Challenge (Part 1)|Can you estimate max/min electrical power within a half hour given weather and the average over that half hour?|89|883.0|2021|2.0|None|None|31.0|https://codalab.lisn.upsaclay.fr/competitions/213|||||electricity|NLP|Can you estimate max/min electrical power within a half hour given weather and the average over that half hour?|regression|||skill |RMSE|False|False|True|False||[0.0]
MISP Challenge 2021(Task2)|MISP Challenge 2021 has been accepted as a Signal Processing Grand Challenge (SPGC) of ICASSP 2022！Please refer to https://2022.ieeeicassp.org/call_for_grandchallenges.php for more details of ICASSP 2022 SPGC.The challenge considers the problem of audio-visual distant multi-microphone conversational wake-up and speech recognition in everyday home environments. Both audio and video data are collected in a home TV scenario, where several people are chatting while watching TV in the living room, and they can interact with a smart speaker/TV.|35|13.0|2021|1.0|None|None|38.0|https://codalab.lisn.upsaclay.fr/competitions/436|||||speech|signal processing|distant multi-microphone conversational audio-visual wake-up and audio-visual speech recognition in everyday home environments|speech recognition|||CCER |Chinese Character Error Rate (CCER)|False|False|True|False||[0.0]
ADD 2022 - Low-quality fake audio detection(LF)|The well known dataset.|14|38.0|2022|1.0|None|None|8.0|https://codalab.lisn.upsaclay.fr/competitions/4110|||||speech|signal processing|fake audio detection|binary classification|||EER|EER|False|False|True|False||[0.0]
PreTENS SemEval22 Task3 (SubTask1)||43|67.0|2022|1.0|None|None|6.0|https://codalab.lisn.upsaclay.fr/competitions/1292|https://sites.google.com/view/semeval2022-pretens/||||linguistics|NLP|a binary classification sub-task (SubTask1), which consists in predicting the acceptability label assigned to each sentence of the test set; |binary classification|||Global Rank|F1, precision, recall|True|False|True|False||[1.2378782159978816]
PreTENS SemEval22 Task3 (SubTask2)||39|55.0|2022|1.0|None|None|6.0|https://codalab.lisn.upsaclay.fr/competitions/1290|||||linguistics|NLP|a regression sub-task (SubTask2), which consists in predicting the average score assigned by human annotators on a seven point Likert-scale with respect to the subset of data evaluated via crowdsourcing.|regression|||Global Rank|F1, precision, recall|True|False|True|False||[3.654248190833557]
MISP Challenge 2021(Task2) Eval Phase|MISP Challenge 2021 has been accepted as a Signal Processing Grand Challenge (SPGC) of ICASSP 2022！Please refer to https://2022.ieeeicassp.org/call_for_grandchallenges.php for more details of ICASSP 2022 SPGC. The challenge considers the problem of audio-visual distant multi-microphone conversational wake-up and speech recognition in everyday home environments. Both audio and video data are collected in a home TV scenario, where several people are chatting while watching TV in the living room, and they can interact with a smart speaker/TV.|19|87.0|2022|1.0|None|None|23.0|https://codalab.lisn.upsaclay.fr/competitions/850|https://mispchallenge.github.io/task2_instructions.html||||speech|multi-domain|distant multi-microphone conversational audio-visual wake-up and audio-visual speech recognition in everyday home environments. |speech recognition|||CCER |Chinese Character Error Rate (CCER)|False|False|True|True||[0.0]
MISP Challenge 2021(Task1) Eval Phase|MISP Challenge 2021 has been accepted as a Signal Processing Grand Challenge (SPGC) of ICASSP 2022！Please refer to https://2022.ieeeicassp.org/call_for_grandchallenges.php for more details of ICASSP 2022 SPGC.The challenge considers the problem of audio-visual distant multi-microphone conversational wake-up and speech recognition in everyday home environments. Both audio and video data are collected in a home TV scenario, where several people are chatting while watching TV in the living room, and they can interact with a smart speaker/TV.|28|369.0|2022|1.0|None|None|23.0|https://codalab.lisn.upsaclay.fr/competitions/887|||||speech|multi-domain|classification problem. ‘1’ indicates that the current sample contains the wake-up word, and ‘0’ indicates the opposite|binary classification|||SCORE |false reject rate (FRR) and false alarm rate (FAR)|True|False|True|True||[0.0]
MISP Challenge 2021(Task1)|MISP Challenge 2021 has been accepted as a Signal Processing Grand Challenge (SPGC) of ICASSP 2022！Please refer to https://2022.ieeeicassp.org/call_for_grandchallenges.php for more details of ICASSP 2022 SPGC.The challenge considers the problem of audio-visual distant multi-microphone conversational wake-up and speech recognition in everyday home environments. Both audio and video data are collected in a home TV scenario, where several people are chatting while watching TV in the living room, and they can interact with a smart speaker/TV.|39|94.0|2021|1.0|None|None|60.0|https://codalab.lisn.upsaclay.fr/competitions/454|||||speech|multi-domain|classification problem. ‘1’ indicates that the current sample contains the wake-up word, and ‘0’ indicates the opposite|binary classification|||SCORE |false reject rate (FRR) and false alarm rate (FAR)|True|False|True|True||[0.0]
SemEval 2022 - Task 6 (iSarcasmEval) : Intended Sarcasm Detection In English and Arabic|Intended Sarcasm Detection In English and Arabic|110|535.0|2022|5.0|None|None|14.0|https://codalab.lisn.upsaclay.fr/competitions/1340|https://sites.google.com/view/semeval2022-isarcasmeval/home||||linguistics|NLP|SubTask A: Given a text, determine whether it is sarcastic or non-sarcastic; SubTask B (English only): A binary multi-label classification task. Given a text, determine which ironic speech category it belongs to, if any; SubTask C: Given a sarcastic text and its non-sarcastic rephrase, i.e. two texts that convey the same meaning, determine which is the sarcastic one.|text recognition, binary classification, multi-class classification|||F-1 sarcastic|F1, accuracy|True|False|True|False||[0.0]
Western Power Distribution Data Challenge (Part 2)|Can you estimate the largest daily EV demand from substation data?|78|1749.0|2022|2.0|500 pounds|600|31.0|https://codalab.lisn.upsaclay.fr/competitions/1324|||||electricity|NLP|Estimating EV Charger Demand|regression|||skill |RMSE|False|False|True|False||[0.0]
Соревнование RuNNE (Тестовая фаза)|Детектирование вложенных сущностей в few-shot режиме (Тестовая фаза)|17|158.0|2022|1.0|None|None|52.0|https://codalab.lisn.upsaclay.fr/competitions/1863|||||linguistics|NLP||entity recognition|||F1 score few-shot, F1 score|F1|False|False|True|False||[0.0]
MedVidQA 2022 Shared Task on Medical Visual Answer Localization|BioNLP workshop co-located with ACL 2022: Challenge on medical visual answer localization|36|71.0|2022|2.0|None|None|47.0|https://codalab.lisn.upsaclay.fr/competitions/1078|||||medicine|computer vision, NLP|Given a medical or health-related question and a video, the task aims to locate the temporal segments (start and end timestamps) in the video where the answer to the medical question is being shown, or the explanation is illustrated in the video. This task seeks to find a video segment with a visual answer to the natural language question|regression, VQA|||mIoU |mIoU, IoU |True|False|True|False||[]
MedVidQA 2022 Shared Task on Medical Video Classification|BioNLP workshop co-located with ACL 2022: Challenge on medical video classification|46|49.0|2022|2.0|None|None|47.0|https://codalab.lisn.upsaclay.fr/competitions/1058|||||medicine|computer vision, NLP|categorize the video|multi-class classification|||Med-Inst F1|F1, precision, recall|True|False|True|False||[]
Real-world Reinforcement Learning Challenge|Learning to make fair and incentive coupon decisions for sales promotion from data.|153|658.0|2021|3.0|None|None|64.0|https://codalab.lisn.upsaclay.fr/competitions/823|||||ecommerce|RL|participants are asked to learn from the historical interactive data between the promotion activities and customer behaviors, which are sampled from the simulation environment. A submitted promotion policy is required to be fair to all customers, i.e., giving the same discount rates for everyone. |reinforcement learning|||Total GMV|customers' feed-back, ROI, GMV|True|False|True|True||[1.0208394258493823]
ADAPT - sim2real Object Detection Challenge|ADAPT - sim2real Object Detection Challenge|20|20.0|2021|2.0|None|None|130.0|https://codalab.lisn.upsaclay.fr/competitions/58|||||multimedia|computer vision|detect and recognize different parts of a system|object recognition|||AP|mAP|False|False|True|False||[]
Hero, villain and Victim: Dissecting harmful memes for Semantic role labelling of entities|Given a meme and an entity, determine the role of the entity in the meme: hero vs. villain vs. victim vs. other.|109|111.0|2022|1.0|None|None|68.0|https://codalab.lisn.upsaclay.fr/competitions/906|||||linguistics|NLP|Given a meme and an entity, determine the role of the entity in the meme: hero vs. villain vs. victim vs. other.|multi-class classification|||correct|accuracy|False|False|True|False||[1.3240763494300372]
PBVS 2022 Thermal Image Super-Resolution Challenge (TISR) - EVALUATION|This is a thermal image super-resolution challenge which consist of two kind of evaluation.|78|459.0|2022|2.0|None|None|57.0|https://codalab.lisn.upsaclay.fr/competitions/1990|||||multimedia|computer vision|obtaining super-resolution images at x2 and x4 scales from the given images.|image transformation|||psnr_x4|PSNR, structural similarity (SSIM)|True|False|True|False||[1.093903008272999]
UPF Object Tracking Mini-Challenge|Object tracking challenge|35|334.0|2022|2.0|None|None|16.0|https://codalab.lisn.upsaclay.fr/competitions/2341|||||multimedia|computer vision||object recognition, object tracking|||HOTA |HOTA, DetA|True|False|True|True||[1.6822413218418175]
PBVS 2022 Multi-modal Aerial View Object Classification Challenge - Track 2 (SAR+EO)|Our objectives are to gauge the s-o-t-a in Multi-modal Aerial view Imagery Classification, to promote research on this topic and to introduce a novel benchmark.|81|483.0|2022|2.0|None|None|76.0|https://codalab.lisn.upsaclay.fr/competitions/1392|||||aerial|computer vision|Multi-modal Aerial view Imagery Classification: train a classifier that is maximally accurate on a held-out test set of EO+SAR pair images from the 10 classes|image recognition, multi-class classification|||Accuracy (top-1)[%]|accuracy|False|False|True|False||[1.9852445415305768]
PBVS 2022 Multi-modal Aerial View Object Classification Challenge - Track 1 (SAR)|Our objectives are to gauge the s-o-t-a in Multi-modal Aerial View Object Classification, to promote research on this topic and to introduce a novel benchmark.|88|626.0|2022|2.0|None|None|79.0|https://codalab.lisn.upsaclay.fr/competitions/1388|||||aerial|computer vision|train a classifier that is maximally accurate on a held-out test set of SAR chips from the 10 classes|image recognition, multi-class classification|||Accuracy (top-1)[%]|accuracy|False|False|True|False||[1.6091960175570064]
Qur'an QA 2022 Shared Task|Qur'an QA 2022 is a shared task of answering questions on the Holy Qur'an @ OSACT 2022 Workshop, LREC 2022.|44|106.0|2022|2.0|None|None|22.0|https://codalab.lisn.upsaclay.fr/competitions/2536|||||linguistics|NLP|Reading Comprehension|QA|||pRR|pRR, exact match, F1|True|False|True|False||[1.35782423381368]
NTIRE 2022 Stereo Image Super-Resolution Challenge||244|647.0|2022|2.0|None|None|58.0|https://codalab.lisn.upsaclay.fr/competitions/1598|||||multimedia|computer vision|reconstruct high-resolution (HR) stereo images from their low-resolution (LR) counterparts|image transformation|||PSNR(RGB)|Peak Signal to Noise Ratio (PSNR), the Structural Similarity (SSIM) index|True|False|True|True||[1.0291387123352165]
NTIRE 2022 High Dynamic Range Challenge - Track 1 Fidelity (low-complexity constrain)|Our objectives are to gauge the s-o-t-a in High Dynamic Range, to promote research on this topic and to introduce a novel benchmark.|211|652.0|2022|2.0|None|None|64.0|https://codalab.lisn.upsaclay.fr/competitions/1514|||||multimedia|computer vision|recovering an HDR image from one or multiple input Low Dynamic Range (LDR) images that are affected by noise, quantization errors, and might suffer from  over- and under-exposed regions due to the sensor limitations|image transformation|||muPSNR |muPSNR, PSNR|True|False|True|False||[0.0]
NTIRE 2022 Perceptual Image Quality Assessment Challenge Track 1 Full-Reference|Our objectives are to gauge the s-o-t-a in perceptual image quality assessment, to promote research on this topic and to introduce a novel benchmark.|198|1827.0|2022|2.0|None|None|63.0|https://codalab.lisn.upsaclay.fr/competitions/1567|||||multimedia|computer vision|perceptual image quality assessment, that is, the task of predicting the perceptual quality of an image based on a set of prior examples of images and their perceptual quality labels|regression|||MainScore |Pearson linear correlation coefficient (PLCC), Spearman rank-order correlation coefficients (SROCC)|True|False|True|False||[12.571428571428571]
NTIRE 2022 Challenge on Multi-Spectral Filter Array Demosaicing|To gauge the current state-of-the-art in Multi-Spectral Filter Array Demosaicing, to compare and to promote different solutions we are organizing an NTIRE challenge in conjunction with the CVPR 2022 conference. The largest dataset to date will be introduced with the challenge.|164|619.0|2022|2.0|None|None|58.0|https://codalab.lisn.upsaclay.fr/competitions/722|||||multimedia|computer vision|hyperspectral demosaicing from 16 channel mosaic, that is, the task of restoration of 16 channel hyperspectral images from simulated “RAW” 4X4 MSFA input|image transformation||||PSNR, Spectral Angle Mapper (SAM)|True|False|True|False||[0.0]
NTIRE 2022 Image Inpainting Challenge Track 2 Semantic Guidance|Our objectives are to gauge the s-o-t-a in image inpainting, to promote research on this topic and to introduce a novel benchmark.|91|59.0|2022|2.0|None|None|62.0|https://codalab.lisn.upsaclay.fr/competitions/1608|||||multimedia|computer vision|predicting the values of missing pixels in an image so that the completed result looks realistic and coherent.|image transformation, regression|||MOS|Learned Perceptual Image Patch Similarity (LPIPS) and Frechet Inception Distance (FID), PSNR, SSIM|True|False|True|False||[-0.0]
NTIRE 2022 Perceptual Image Quality Assessment Challenge Track 2 No-Reference|Our objectives are to gauge the s-o-t-a in perceptual image quality assessment, to promote research on this topic and to introduce a novel benchmark.|189|2363.0|2022|2.0|None|None|63.0|https://codalab.lisn.upsaclay.fr/competitions/1568|||||multimedia|computer vision|predicting the perceptual quality of an image based on a set of prior examples of images and their perceptual quality labels|regression|||MainScore|Pearson linear correlation coefficient (PLCC), Spearman rank-order correlation coefficients (SROCC)|True|False|True|False||[17.0]
NTIRE 2022 Challenge on Spectral Reconstruction from RGB|To gauge the current state-of-the-art in spectral reconstruction from RGB images, to compare and to promote different solutions we are organizing an NTIRE challenge in conjunction with the CVPR 2022 conference. The largest dataset to date will be introduced with the challenge.|269|1376.0|2022|2.0|None|None|58.0|https://codalab.lisn.upsaclay.fr/competitions/721|||||multimedia|computer vision|restoration of hyperspectral images (high spectral resolution) for a single RGB input image based on a set of prior examples with hyperspectral and corresponding RGB images|image transformation||||RMSE, MRAE|True|False|True|False||[0.0]
OSACT5 Shared Task on Fine-Grained Hate Speech Detection on Arabic Twitter (Subtask B)|OSACT5 Shared Task on Fine-Grained Hate Speech Detection on Arabic Twitter (Subtask B)|27|70.0|2022|2.0|None|None|52.0|https://codalab.lisn.upsaclay.fr/competitions/2332|||||linguistics, tweets|NLP|Subtask A: Detect whether a tweet is offensive or not., Subtask B: Detect whether a tweet has hate speech or not. Subtask C: Detect the fine-grained type of hate speech.|binary classification|||F1-Score|F1|False|False|True|False||[0.0]
NTIRE 2022 Image Inpainting Challenge Track 1 Unsupervised|Our objectives are to gauge the s-o-t-a in image inpainting, to promote research on this topic and to introduce a novel benchmark.|103|158.0|2022|2.0|None|None|62.0|https://codalab.lisn.upsaclay.fr/competitions/1607|||||multimedia|computer vision|predicting the values of missing pixels in an image so that the completed result looks realistic and coherent.|image transformation, regression||||Learned Perceptual Image Patch Similarity (LPIPS) and Frechet Inception Distance (FID), PSNR, SSIM|True|False|True|False||[-0.0]
OSACT5 Shared Task on Fine-Grained Hate Speech Detection on Arabic Twitter (Subtask A)|OSACT5 Shared Task on Fine-Grained Hate Speech Detection on Arabic Twitter (Subtask A)|41|142.0|2022|2.0|None|None|52.0|https://codalab.lisn.upsaclay.fr/competitions/2324|||||linguistics, tweets|NLP|Subtask A: Detect whether a tweet is offensive or not., Subtask B: Detect whether a tweet has hate speech or not. Subtask C: Detect the fine-grained type of hate speech.|binary classification|||F1-Score|F1|False|False|True|False||[0.0]
NTIRE 2022 Burst Super-Resolution Challenge - Track 1 Synthetic|Our objectives are to gauge the SOTA in burst image super-resolution, to promote research on this topic and to introduce a novel benchmark.|94|311.0|2022|2.0|None|None|59.0|https://codalab.lisn.upsaclay.fr/competitions/1750|https://github.com/goutamgmb/NTIRE22_BURSTSR||||multimedia|computer vision|generate a denoised, demosaicked, higher-resolution image, given a RAW burst as input|image generation|||PSNR|PSNR, SSIM|True|False|True|False||[1.0]
NTIRE 2022 High Dynamic Range Challenge - Track 2 Low-complexity (fidelity constrain)|Our objectives are to gauge the s-o-t-a in High Dynamic Range, to promote research on this topic and to introduce a novel benchmark.|179|589.0|2022|2.0|None|None|64.0|https://codalab.lisn.upsaclay.fr/competitions/1515|||||multimedia|computer vision|recovering an HDR image from one or multiple input Low Dynamic Range (LDR) images that are affected by noise, quantization errors, and might suffer from  over- and under-exposed regions due to the sensor limitations.|image transformation|||muPSNR |the number of operations (MAcc), method's runtime |True|False|True|False||[0.0]
NTIRE 2022 Efficient Super-Resolution Challenge|Our objectives are to gauge the s-o-t-a in example-based efficient single image super-resolution, to promote research on this topic and to introduce a novel benchmark.|319|755.0|2022|2.0|None|None|52.0|https://codalab.lisn.upsaclay.fr/competitions/1865|||||multimedia|computer vision|super-resolving (increasing the resolution) an input image with a magnification factor x4 based on a set of prior examples of low and corresponding high resolution images.|image transformation|||PSNR|PSNR, SSIM, runtime|True|False|True|False||[1.3013698630136987]
OSACT5 Shared Task on Fine-Grained Hate Speech Detection on Arabic Twitter (Subtask C)|OSACT5 Shared Task on Fine-Grained Hate Speech Detection on Arabic Twitter (Subtask C)|25|59.0|2022|2.0|None|None|52.0|https://codalab.lisn.upsaclay.fr/competitions/2334|||||linguistics, tweets|NLP|Subtask A: Detect whether a tweet is offensive or not., Subtask B: Detect whether a tweet has hate speech or not. Subtask C: Detect the fine-grained type of hate speech.|multi-class classification|||F1-Score|F1|False|False|True|False||[0.0]
Lexical Semantic Change Discovery Shared Task|Shared task on semantic change discovery and semantic change over time, in a text corpora of Spain. The task is organized by Frank D. Zamora-Reina, Felipe Bravo-Marquez and Dominik Schlechtweg.|29|156.0|2022|4.0|None|None|31.0|https://codalab.lisn.upsaclay.fr/competitions/2243|||||linguistics|NLP|predicting and evaluating changes for all the words in the corpus vocabulary instead of having a predefined set of target words|regression, binary classification|||SPR|F1, precision, recall|True|True|True|False||[0.0]
NTIRE 2022 Super-Resolution and Quality Enhancement of Compressed Video - Track 2 Enhancement (x2SR)|Our objectives are to gauge the s-o-t-a in enhancement of compressed videos, to promote research on this topic and to introduce a novel benchmark.|215|570.0|2022|2.0|None|None|67.0|https://codalab.lisn.upsaclay.fr/competitions/1496|||||multimedia|computer vision|Super-Resolution and Quality Enhancement of Compressed Video|video transformation|||PSNR|PSNR, SSIM|True|False|True|False||[1.0313064179725826]
NTIRE 2022 Super-Resolution and Quality Enhancement of Compressed Video - Track 1 Enhancement|Our objectives are to gauge the s-o-t-a in enhancement of compressed videos, to promote research on this topic and to introduce a novel benchmark.|236|568.0|2022|2.0|None|None|67.0|https://codalab.lisn.upsaclay.fr/competitions/1495|||||multimedia|computer vision|x2 Super-Resolution and Enhancement |video transformation|||PSNR|PSNR, SSIM|True|False|True|False||[1.02910011623511]
NTIRE 2022 Super-Resolution and Quality Enhancement of Compressed Video - Track 3 Enhancement (x4SR)|Our objectives are to gauge the s-o-t-a in enhancement of compressed videos, to promote research on this topic and to introduce a novel benchmark.|195|909.0|2022|2.0|None|None|67.0|https://codalab.lisn.upsaclay.fr/competitions/1497|||||multimedia|computer vision|x4 Super-Resolution and Enhancement |video transformation|||PSNR|PSNR, SSIM|True|False|True|False||[1.034511680068917]
Western Power Distribution Data Challenge (Part 3)|Can you estimate Missing Data in a Distribution Network Hierarchy?|30|156.0|2022|2.0|500|600|42.0|https://codalab.lisn.upsaclay.fr/competitions/2625|||||electricity|NLP|estimate Missing Data in a Distribution Network Hierarchy|regression|||skill |RMSE|False|False|True|False||[0.0]
DeepFake Game Competition (DFGC) @ IJCB 2022 - Creation Track|A competition to evaluate the status of the adversarial game between Deepfake creation and detection.This is the second edition of DFGC.|50|47.0|2022|3.0|8000|8000|41.0|https://codalab.lisn.upsaclay.fr/competitions/2149|||||multimedia|computer vision|Deepfake creation and detection|deepfake|||Prediction score|prediction score, duration|True|True|True|False||[nan]
The 3rd International Competition on Human Identificaiton at a Distance 2022 (HID2022)|The goal of the competition is to provide an evaluation for state-of-the-arts on human identification at a distance. The features used can be gait, body shape and any other cues on human body.|162|1712.0|2022|2.0|None|None|51.0|https://codalab.lisn.upsaclay.fr/competitions/2542|||||multimedia|computer vision|human identification at a distance|human identification|||correct|accuracy|False|False|True|False||[1.1245878366001099]
ICPR VideoPipe Challenge - Track on Video Defect Classification|The challenge requires to predict the categories of pipe defects in a short QV video. The challenge will be carried out on the QV-Pipe dataset. More information on the dataset and downloads can be found at https://videopipe.github.io/qvpipe.|57|265.0|2022|1.0|None|None|61.0|https://codalab.lisn.upsaclay.fr/competitions/2232|||||multimedia|computer vision|Video Defect Classification|image recognition, multi-class classification|||Avg.mAP (%)|AP|False|False|True|False||[1.5508683974932858]
ICPR VideoPipe Challenge - Track on Temporal Defect Localization|The challenge requires to find the temporal locations of pipe detects and recognize their corresponding categories in a long CCTV video. The challenge will be carried out on the CCTV-Pipe dataset. More information on the dataset and downloads can be found at https://videopipe.github.io/qvpipe.|41|75.0|2022|1.0|None|None|61.0|https://codalab.lisn.upsaclay.fr/competitions/2284|||||multimedia|computer vision|find the temporal locations of pipe detects and recognize their corresponding categories in a long CCTV video|image recognition, multi-class classification, regression|||Avg.mAP (%)|AP|False|False|True|False||[1.8696213981134515]
Paraphrase Identification in Mexican Spanish Competition|Given pairs of sentences, the aim is to indicate whether each pair captures a paraphrase relationship.|15|258.0|2022|2.0|None|None|66.0|https://codalab.lisn.upsaclay.fr/competitions/2345|||||linguistics|NLP|Given pairs of sentences, the aim is to indicate whether each pair captures a paraphrase relationship|binary classification|||F1|F1|False|False|True|False||[1.0583497777991688]
CLEF2022 CheckThat! Lab - Task 1|Identifying Relevant Claims in Tweets (Arabic, Bulgarian, Dutch, English, Spanish, and Turkish). More information:https://sites.google.com/view/clef2022-checkthat|36|281.0|2022|21.0|None|None|14.0|https://codalab.lisn.upsaclay.fr/competitions/4230|||||linguistics, tweets|NLP|Subtask 1A: Check-worthiness of tweets: Given a tweet, predict whether it is worth fact-checking|binary classification||||F1, accuracy|True|False|True|False||[0.0]
CLEF2022 CheckThat! Lab - Task 2|Given a check-worthy claim, and a set of previously fact-checked claims, determine whether the claim has been previously fact-checked|17|41.0|2022|3.0|None|None|14.0|https://codalab.lisn.upsaclay.fr/competitions/4260|||||linguistics|NLP|determine whether the claim has been previously fact-checked|binary classification||||F1, accuracy|True|False|True|False||[0.0]
CLEF2022 CheckThat! Lab - Task 3|Fake News Detection|44|165.0|2022|2.0|None|None|18.0|https://codalab.lisn.upsaclay.fr/competitions/4633|||||linguistics, fake news|NLP|Subtask 1C: Harmful tweet detection : Given a tweet, predict whether it is harmful to the society and why.|binary classification, multi-class classification|||Subtask A F1 Macro average|F1, accuracy|True|False|True|False||[0.0]
SatVideoDT 2022 Challenge on Moving Object Detection and Tracking in Satellite Videos Track 2.||89|849.0|2022|2.0|None|None|57.0|https://codalab.lisn.upsaclay.fr/competitions/3079|||||multimedia, aerial|computer vision|Single object tracking in satellite videos|object detection|||Score |DPR and OSR|True|False|True|True||[1.4830807669328119]
SatVideoDT 2022 Challenge on Moving Object Detection and Tracking in Satellite Videos Track 1.||126|790.0|2022|2.0|None|None|57.0|https://codalab.lisn.upsaclay.fr/competitions/2802|||||multimedia, aerial|computer vision|moving objects detection and tracking in satellite videos|object detection|||bbox_mAP_50|mAP|False|False|True|True||[2.6262899025942716]
SatVideoDT 2022 Challenge on Moving Object Detection and Tracking in Satellite Videos Track 3.||73|730.0|2022|2.0|None|None|57.0|https://codalab.lisn.upsaclay.fr/competitions/3081|||||multimedia, aerial|computer vision|Multi-object tracking in satellite videos|object detection|||MOTA |MOTA|False|False|True|True||[1.887232480659552]
HinglishEval-INLG-2022|Quality Evaluation of the Low-Resource Synthetically Generated Code-Mixed Hinglish Text|49|139.0|2022|2.0|None|None|101.0|https://codalab.lisn.upsaclay.fr/competitions/1688|||||linguistics|NLP|quality rating prediction and annotators’ disagreement prediction of the synthetic Hinglish dataset|ordinal regression, binary classification|||F1 Score|F1, Kappa, MSE|True|True|True|False||[0.0]
LeQua2022 - Official|LeQua2022 is the 1st edition of the CLEF “Learning to Quantify” lab. The aim of this challenge is to allow the comparative evaluation of methods for “learning to quantify”, i.e., methods for training predictors of the relative frequencies of the classes of interest in sets of unlabelled documents. For further details, please visit the official LeQua2022's site.|25|62.0|2022|4.0|None|None|24.0|https://codalab.lisn.upsaclay.fr/competitions/4134|||||linguistics|NLP|Task T1A: This task is concerned with evaluating binary quantifiers, i.e., quantifiers that must only predict the relative frequencies of a class and its complement. Task T1B: This task is concerned with evaluating single-label multi-class quantifiers, i.e., quantifiers that operate on documents that each belong to exactly one among a set of n>2 classes|regression|||Relative Absolute Error|RAE (relative absolute error), AE (absolute error)|True|False|True|False||[0.0]
ICPR2022 Extracting subtitles with both visual and audio annotations|In this subtask, for the training set, we present 50 hours of video content with both the visual and audio supervisions and 200-hour video content with no annotation.   Another about 20 and 5 h of videos will be provided to serve as validation and testing sets, respectively.   For the visual annotation, we will provide characters of all text in key frames, we will present speech transcripts of segment or the audio modal. With these data, participants will be required to produce each VAD subtitle for each video in our testing set, and the submitted results will be ranked with the CER metric.|102|163.0|2022|3.0|5300|5300|66.0|https://codalab.lisn.upsaclay.fr/competitions/2418|||||multimedia|computer vision, NLP|produce each VAD subtitle for each video in our testing set, and the submitted results will be ranked with the CER metric|text generation|||Difference |CER|False|False|True|True||[0.0]
ICPR2022MSR: Extracting subtitles in visual modality with audio annotations|In this subtask, we will present 75h of video content, divided into set of 50, 5, 20 as training, validation, and testing sets, respectively. For the training set, only audio annotations will be provided. The participants are required to design subtitles OCR system with these annotations. To pretrain an OCR system, participants can also use a limited number of open datasets, and fine-tune their model with audio supervision.Under these conditions, will be asked to produce subtitle text for each video in our testing set, and the submitted results will be ranked using the CER metric.|161|543.0|2022|3.0|5300|5300|66.0|https://codalab.lisn.upsaclay.fr/competitions/2415|||||multimedia|computer vision, NLP|design subtitles OCR system|text generation|||Difference |CER|False|False|True|True||[0.0]
ICPR2022MSR: Extracting subtitles in audio modal with visual annotations|In this subtask, the participants will be required to use only visual annotations to build an ASR system for the corresponding videos. To improve the robustness, some public ASR data in the following tables may be used as well. We will also provide a baseline model. The submitted results will be ranked with the CER metric on our testing set.|114|226.0|2022|3.0|5300|5300|66.0|https://codalab.lisn.upsaclay.fr/competitions/2417|||||multimedia|computer vision, NLP, speech|use of visual annotations in videos, especially automatic annotations to assist in building an ASR system|text generation|||Difference |CER|False|False|True|True||[0.0]
DeepFake Game Competition (DFGC) @ IJCB 2022 - Detection Track (NEW)|A competition to evaluate the status of the adversarial game between Deepfake creation and detection.This is the second edition of DFGC.|57|467.0|2022|4.0|8000|8000|61.0|https://codalab.lisn.upsaclay.fr/competitions/3523|||||multimedia|computer vision|Deepfake detection|deepfake|||AUC score|AUC|False|False|True|False||[1.215296913499249]
DeepFake Game Competition (DFGC) @ IJCB 2022 - Detection Track (Migrated)|A competition to evaluate the status of the adversarial game between Deepfake creation and detection.This is the second edition of DFGC.|61|82.0|2022|2.0|8000|8000|61.0|https://codalab.lisn.upsaclay.fr/competitions/2692|||||multimedia|computer vision|Deepfake creation|deepfake|||AUC score||False|False|True|False||[1.0]
ICPR2022-ODOR|The ODeuropa Competition on Olfactory Object Recognition to detect olfactory objects in the visual arts|38|48.0|2022|3.0|None|None|74.0|https://codalab.lisn.upsaclay.fr/competitions/1939|https://odor-challenge.github.io/2022/||||multimedia|computer vision|detect a diverse range of smell-related objects in historical artworks|object recognition|||Prediction score|mAP, IoU|True|False|True|False||[nan]
SHARP 2022 - Challenge 1 / Track 1 @ CVPR 2022|SHARP 2022 - Challenge 1 / Track 1 - In conjunction with CVPR 2022Recovering textured human body scans from partial acquisitions.|23|19.0|2022|1.0|2000|2000|125.0|https://codalab.lisn.upsaclay.fr/competitions/1777|||||multimedia|computer vision|recover a complete 3D scan from its partial acquisition|image transformation|||final score (%)|shape score, area score, colour score|True|False|True|False||[1.1728279226871832]
1st ACRE Online Competition|Multi-class segmentation of RGB images to distinguish crop, weeds, and the background.|22|50.0|2022|3.0|None|None|87.0|https://codalab.lisn.upsaclay.fr/competitions/2197|||||agriculture|computer vision|segment RGB images to distinguish between crop, weeds, and background|multi-class classification|||Global IoU|IoU|False|False|True|False||[1.25]
SHARP 2022 - Challenge 2 / Track 2 @ CVPR 2022 - Evaluation Phase|Evaluation Phase of SHARP 2022 - Challenge 2 / Track 2 - In conjunction with CVPR 2022Recovery of Parametric Sharp Edges in 3D Object Scans.|8|14.0|2022|1.0|2000|2000|22.0|https://codalab.lisn.upsaclay.fr/competitions/4609|||||multimedia|computer vision|recover fine details from raw scans. Given a 3D object scan with smooth edges, the goal of this challenge is to reconstruct the corresponding CAD model as a triangular mesh with sharp edges approximating the ground-truth sharp edges.|image transformation|||final score (%)|area score,  shape score, length score, edge score |True|False|True|False||[1.1806901899961226]
SHARP 2022 - Challenge 1 / Track 2 @ CVPR 2022 - Evaluation Phase|Evaluation Phase of SHARP 2022 - Challenge 1 / Track 2 - In conjunction with CVPR 2022Recovering textured object scans from partial acquisitions.|13|121.0|2022|1.0|2000|2000|22.0|https://codalab.lisn.upsaclay.fr/competitions/4603|||||multimedia|computer vision|recover a complete 3D scan from its partial acquisition. This track (Challenge 1 - Track 2) focuses on recovering textured object scans from partial acquisitions|image transformation|||final score (%)|area score,  shape score, length score, edge score |True|False|True|False||[1.0513905683192262]
SHARP 2022 - Challenge 1 / Track 1 @ CVPR 2022 - Evaluation Phase|Evaluation Phase of SHARP 2022 - Challenge 1 / Track 1 - In conjunction with CVPR 2022Recovering textured human body scans from partial acquisitions. Evaluation Phase.|17|110.0|2022|1.0|2000|2000|22.0|https://codalab.lisn.upsaclay.fr/competitions/4604|||||multimedia|computer vision|recover a complete 3D scan from its partial acquisition. This track (Challenge 1 - Track 1) focuses on recovering textured human body scans from partial acquisitions|image transformation|||final score (%)|shape score, area score, colour score|True|False|True|False||[1.2902886714643809]
The 4th Large-scale Video Object Segmentation - Track 3: Referring Video Object Segmentation|YouTube-VOS is the first large-scale dataset for video object segmentation. Our dataset contains 3900+ YouTube videos, densely-sampled high-quality pixel-level annotations, and 15k language expressions.|64|1340.0|2022|2.0|None|None|66.0|https://codalab.lisn.upsaclay.fr/competitions/3282|||||multimedia|computer vision||object segmentation|||Overall |Region Jaccard (J), Boundary F measure (F)|True|False|True|False||[0.0]
The 4th Large-scale Video Object Segmentation Challenge - Track 2: Video Instance Segmentation|YouTube-VOS is the first large-scale dataset for video object segmentation. Our dataset contains 4000+ YouTube videos, 70+ common objects and densely-sampled high-quality pixel-level annotations.|185|2154.0|2022|2.0|None|None|85.0|https://codalab.lisn.upsaclay.fr/competitions/3410|||||multimedia|computer vision||instance segmentation|||mAP|AP, AR|True|False|True|False||[1.2682077264091196]
The 4th Large-scale Video Object Segmentation Challenge - Track 1: Video Object Segmentation|YouTube-VOS is the first large-scale dataset for video object segmentation. Our dataset contains 4000+ YouTube videos, 70+ common objects and densely-sampled high-quality pixel-level annotations.|140|509.0|2022|2.0|None|None|86.0|https://codalab.lisn.upsaclay.fr/competitions/3295|||||multimedia|computer vision||object segmentation|||Overall |Region Jaccard (J), Boundary F measure (F)|True|False|True|False||[1.05201072386059]
TinyAction Challenge [CVPR 2022] [Recognition task]|A challenge focusing on recognizing tiny activities in low-resolution videos.|76|9862.0|2022|1.0|None|None|118.0|https://codalab.lisn.upsaclay.fr/competitions/1832|||||multimedia|computer vision||action recognition|||F1|F1, precision, recall|True|False|True|False||[2.20018113625233]
ROSE Challenge [CVPR 2022]|This challenge will focus on developing robust action recognition models.|42|481.0|2022|6.0|None|None|119.0|https://codalab.lisn.upsaclay.fr/competitions/2618|||||multimedia        |computer vision|develop robust activity recognition models which will be tested for robustness against various perturbations. The robustness will be evaluated based on the model's performance on the test set with natural corruptions and perturbations.|action recognition|||Accuracy |accuracy|False|False|True|False||[1.6119129718348542]
EPIC-KITCHENS-100 Action Recognition|Classify trimmed action segments from the EPIC-KITCHENS-100 dataset.|38|69.0|2022|1.0|None|None|137.0|https://codalab.lisn.upsaclay.fr/competitions/776|||||multimedia|computer vision|unscripted egocentric action dataset collected from 45 kitchens from 4 cities across the world|action recognition|||Top-1 Accuracy (%)|SLS, accuracy|True|False|True|False||[0.0]
EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge|Perform video-to-text and text-to-video retrieval on the EPIC-KITCHENS-100 dataset.|16|32.0|2022|1.0|None|None|137.0|https://codalab.lisn.upsaclay.fr/competitions/617|||||multimedia|computer vision, NLP|Perform cross-modal retrieval by searching between vision and text modalities||||Mean Average Precision (%)|SLS, mAP|True|False|True|False||[0.0]
EPIC-KITCHENS-100 Action Anticipation|Anticipate future actions from video observations of the EPIC-KITCHENS-100 dataset.|25|126.0|2021|1.0|None|None|321.0|https://codalab.lisn.upsaclay.fr/competitions/702|||||multimedia|computer vision|anticipation of a future action from the observation of a preceding video segment|multi-class classification, action recognition|||Overall (%)|SLS, recall|True|False|True|False||[0.0]
EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition|Classify trimmed action segments from the EPIC-KITCHENS-100 UDA target set.|21|100.0|2022|1.0|None|None|137.0|https://codalab.lisn.upsaclay.fr/competitions/1241|||||multimedia|computer vision|Given labelled videos from the source domain and unlabelled videos from the target domain, the goal classify actions in the target domain. An action is defined as a verb and noun depicted in a trimmed video clip.|multi-class classification, action recognition|||Target Top-1 Accuracy (%)|SLS, accuracy|True|False|True|False||[0.0]
EPIC-KITCHENS-100 Action Detection|Detect all actions in the egocentric videos of EPIC-KITCHENS-100 dataset.|27|79.0|2022|1.0|None|None|137.0|https://codalab.lisn.upsaclay.fr/competitions/707|||||multimedia|computer vision|Given a video, we aim to predict the set of all actions instances |multi-class classification, action recognition|||Avg. mAP (%)|SLS, mAP|True|False|True|False||[0.0]
IJCB 2022 Mobile Behavioral Biometrics Competition|Organized by Biometrics and Data Pattern Analytics - BiDA Lab (UAM)|24|178.0|2022|2.0|None|None|65.0|https://codalab.lisn.upsaclay.fr/competitions/3564|||||biometrics|computer vision, NLP|benchmarking mobile authentication systems based on behavioral biometric traits transparently acquired by mobile devices during ordinary Human-Computer Interaction (HCI), using a novel public database and a standard experimental protocol|touch gestures||||AUC|False|True|True|False||[0.0]
Woodscape Fisheye Object Detection Challenge for Autonomous Driving - CVPR 2022 OmniCV Workshop|Woodscape is a multi-task, multi-camera fisheye dataset for autonomous driving. The objective of this challenge is to generate 2D bounding boxes for fisheye images, and in particular to advance the state of the art and to benchmark techniques for Object Detection on fisheye images.|93|1199.0|2022|2.0|1000|1000|51.0|https://codalab.lisn.upsaclay.fr/competitions/4074|||||autonomous driving|computer vision|generate 2D bounding boxes for fisheye images, and in particular to advance the state of the art and to benchmark techniques for 2D Object Detection on fisheye images|object recognition|||Score (mAP)|mAP|False|False|True|False||[1.2552415679124889]
PRINCE Out-of-distribution Generalization Challenge @ ECML-PKDD|PRINCE Out-of-distribution Generalization Challenge @ ECML-PKDD|60|887.0|2022|2.0|None|None|63.0|https://codalab.lisn.upsaclay.fr/competitions/3353||||||NLP|learn a mapping from a set of features (X) to a label (y) that generalizes well accross environments that have not been seen in training|binary classification|||NCE (never-seen environment)|Normalized Cross Entropy (NCE)|False|False|True|False||[0.0]
DAVINCIS@IberLEF 2022|Detection of Aggressive and Violent Incidents from Social Media in Spanish|43|189.0|2022|4.0|None|None|87.0|https://codalab.lisn.upsaclay.fr/competitions/2638|||||linguistics, tweets|NLP|detection of violent incidents on Twitter: Subtask 1: Violent event identification. Determine whether a given tweet is associated with a violent incident or not (binary classification). Subtask 2: Violent event category recognition. Recognize the crime category (see above) to which a given tweet belongs (multi-class multi-label classification). The considered categories are: Accident, Homicide, Non-Violent-incident, Robbery, Kidnapping.|binary classification, multi-class classification|||f1|Precision, recall, F1|True|True|True|False||[1.1769202186952599]
DAC4AutoML Competition - DAC4SGD Track|Dynamic Algorithm Configuration for Learning Rate Control in Deep Learning|20|64.0|2022|1.0|None|None|75.0|https://codalab.lisn.upsaclay.fr/competitions/3672||||||AutoML|||||Average Rank||||True|True||[0.0]
Video distortion detection and classification in the context of video surveillance|ICIP2022 Challenge for Video distortion detection and classification in the context of video surveillance|26|131.0|2022|2.0|None|None|97.0|https://codalab.lisn.upsaclay.fr/competitions/3093|||||multimedia, video-surveillance|computer vision|detection and classification of various distortions that are little or not at all studied in the field of image quality assessment in this context. Video distortion detection and classification in the context of video surveillance|object recognition|||F1 (Single Class+Level) |speed, accuracy, F1|True|False|True|False||[1.4869739478957915]
Fincausal 2022|This is a competition for the FinCausal 2022 Shared Task|14|248.0|2022|2.0|None|None|115.0|https://codalab.lisn.upsaclay.fr/competitions/3802|||||finance|NLP|experiment causality detection, and focuses on determining causality associated to an event: evaluate which events or which chain of events can cause a financial object to be modified or an event to occur, regarding a given external context|binary classification, relation extraction|||Exact match|Precision, Recall, F1|True|True|True|False||[1.1049250535331905]
ECCV'22 ChaLearn Seasons in Drift Challenge (track 3: month level)|Detection at month level. Train on a predefined and single month data and evaluate concept drift across time.|13|35.0|2022|2.0|None|None|60.0|https://codalab.lisn.upsaclay.fr/competitions/4276|https://chalearnlap.cvc.uab.cat/challenge/51/description/||||multimedia, video-surveillance|computer vision|thermal object detection: spotlight the problem of concept drift in a surveillance context and highlight the challenges and limitations of existing methods. Each track aims at evaluating how robust a given detection method is to concept drift, by training on limited data from a specific time period (day, week, month in February) and evaluation performance across time, by validating and testing performance on months of unseen data (Jan., May., Apr., May., Jun., Jul., Aug. and Sep.)|object recognition|||mAP weighted|mAP|False|False|True|False||[1.2403975527210438]
ECCV'22 ChaLearn Seasons in Drift Challenge (track 1: day level)|Detection at day level. Train on a predefined and single day data and evaluate concept drift across time.|34|73.0|2022|2.0|None|None|60.0|https://codalab.lisn.upsaclay.fr/competitions/4272|https://chalearnlap.cvc.uab.cat/challenge/51/description/||||multimedia, video-surveillance|computer vision|thermal object detection: spotlight the problem of concept drift in a surveillance context and highlight the challenges and limitations of existing methods. Each track aims at evaluating how robust a given detection method is to concept drift, by training on limited data from a specific time period (day, week, month in February) and evaluation performance across time, by validating and testing performance on months of unseen data (Jan., May., Apr., May., Jun., Jul., Aug. and Sep.)|object recognition|||mAP weighted|mAP|False|False|True|False||[1.679508582993289]
ECCV'22 ChaLearn Seasons in Drift Challenge (track 2: week level)|Detection at week level. Train on a predefined and single week data and evaluate concept drift across time.|13|23.0|2022|2.0|None|None|60.0|https://codalab.lisn.upsaclay.fr/competitions/4273|https://chalearnlap.cvc.uab.cat/challenge/51/description/||||multimedia, video-surveillance|computer vision|thermal object detection: spotlight the problem of concept drift in a surveillance context and highlight the challenges and limitations of existing methods. Each track aims at evaluating how robust a given detection method is to concept drift, by training on limited data from a specific time period (day, week, month in February) and evaluation performance across time, by validating and testing performance on months of unseen data (Jan., May., Apr., May., Jun., Jul., Aug. and Sep.)|object recognition|||mAP weighted|mAP|False|False|True|False||[1.275879709708384]
ECCV'22 ChaLearn Sign Spotting Challenge Challenge (track 2: OSLWL)|OSLWL (one shot learning and weak labels).|16|100.0|2022|2.0|None|None|64.0|https://codalab.lisn.upsaclay.fr/competitions/4199|https://chalearnlap.cvc.uab.cat/challenge/49/description/||||multimedia, health|computer vision|fine-grain sign spotting for continuous SLR|action recognition|||avg F1|F1|False|False|True|False||[2.4023702661912485]
ECCV'22 ChaLearn Sign Spotting Challenge Challenge (track 1: MMSL)|MSSL (multiple shot supervised learning).|26|19.0|2022|2.0|None|None|64.0|https://codalab.lisn.upsaclay.fr/competitions/4198|https://chalearnlap.cvc.uab.cat/challenge/49/description/||||multimedia, health|computer vision|fine-grain sign spotting for continuous SLR|action recognition|||avg F1|F1|False|False|True|False||[1.6709407536883456]
Invoice Analysis and Identification Challenge|Invoice Analysis and Recognition Challenge, which aims to extract predefined content (such as amount, date, invoice code, invoice number, etc.) from scanned tickets. In this challenge, we have released six types of invoices, including Taxi Invoice, Quota Invoice, Toll Invoice, Passenger Invoice, Aircraft Itinerary Invoice, and Train Invoice, for algorithm verification, to promote the implementation of OCR in real production scenarios.|160|531.0|2022|2.0|None|None|69.0|https://codalab.lisn.upsaclay.fr/competitions/3823|https://davar-lab.github.io/competition/CSIG2022-invoice.html||||invoicing, accounting|computer vision, NLP|invoice recognition algorithms|text recognition|||F_score|F1, precision, recall|True|False|True|False||[0.0]
MAFAT Challenge - WiFi Sensing: Non Invasive Human Presence Detection - Track 2|In this challenge, MAFAT’s DDR&D (Directorate of Defense Research & Development) would like to tackle the challenge of inferring room occupancy based on machine learning methods applied to WiFi Received Signal Strength Indicator (RSSI) data|130|262.0|2022|2.0|25000|25000|76.0|https://codalab.lisn.upsaclay.fr/competitions/3798|||||multimedia|computer vision, signal processing|inferring room occupancy based on machine learning methods applied to WiFi Received Signal Strength Indicator (RSSI) data: Track 2—People Counting: Participant’s goal is to estimate how many people are in the room.|regression|||MAE score|ROC AUC|False|False|True|True||[0.0]
MAFAT Challenge - WiFi Sensing: Non Invasive Human Presence Detection - Track 1|In this challenge, MAFAT’s DDR&D (Directorate of Defense Research & Development) would like to tackle the challenge of inferring room occupancy based on machine learning methods applied to WiFi Received Signal Strength Indicator (RSSI) data|448|1068.0|2022|2.0|25000|25000|76.0|https://codalab.lisn.upsaclay.fr/competitions/3498|||||multimedia|computer vision, signal processing|inferring room occupancy based on machine learning methods applied to WiFi Received Signal Strength Indicator (RSSI) data: Track 1—Occupancy Prediction: Participant’s goal is to infer whether a room is empty (0) or occupied (1).|binary classification|||ROC AUC score|ROC AUC|False|False|True|True||[1.1314229598350192]
DAC4AutoML Competition - DAC4RL Track|Dynamic Algorithm Configuration for Reinforcement Learning|12|29.0|2022|2.0|500|500|111.0|https://codalab.lisn.upsaclay.fr/competitions/3727||||||RL|dynamically adapt hyperparameters for Deep Learning (DL) and Reinforcement Learning (RL)|reinforcement learning|||||||True|True||[0.0]
3rd CLVision Challenge - Instance Classification|3rd CLVision Workshop Challenge - Instance Classification Track|63|397.0|2022|2.0|None|None|92.0|https://codalab.lisn.upsaclay.fr/competitions/3568|https://sites.google.com/view/clvision2022/challenge/instance-classification-track-1||||multimedia|computer vision, continual learning|instance classification|instance classification|||AvgAccuracy |accuracy|False|False|True|False||[1.2851419237525807]
3rd CLVision Challenge - Category Detection|3rd CLVision Workshop Challenge - Category Detection Track|47|122.0|2022|2.0|None|None|92.0|https://codalab.lisn.upsaclay.fr/competitions/3569|||||multimedia|computer vision, continual learning||multi-class classification|||AvgAP |AP, AR|True|False|True|False||[0.0]
3rd CLVision Challenge - Instance Detection|3rd CLVision Workshop Challenge - Instance Detection Track|36|62.0|2022|2.0|None|None|92.0|https://codalab.lisn.upsaclay.fr/competitions/3570|||||multimedia|computer vision, continual learning||instance detection|||AvgAP |AP, AR|True|False|True|False||[0.0]
AutoML2022: Multiobjective Hyperparameter Optimization for Transformers|Design hyperparameter optimization methods to discover Transformer models that are both fast and accurate. (AutoML 2022 Conference Competition)|17|40.0|2022|2.0|None|None|91.0|https://codalab.lisn.upsaclay.fr/competitions/3647||||||AutoML|||||Set1 NumPareto (avg)||||True|True||[1.693548387096774]
Zero Cost NAS Competition|Zero Cost NAS Competition @AutoML-Conf-2022|18|25.0|2022|1.0|None|None|80.0|https://codalab.lisn.upsaclay.fr/competitions/3932||||||AutoML|||||Score ||||True|True||[1.1271477663230238]
Image Restoration for Under-display Camera @MIPI-challenge|challenge|119|56.0|2022|2.0|None|None|48.0|https://codalab.lisn.upsaclay.fr/competitions/4874|||||multimedia|computer vision|restore UDC images with complicated degradations|image transformation|||PSNR|PSNR, SSIM, LPIPS|True|False|True|False||[1.3539260655863936]
RGBW Joint Fusion and Denoise @MIPI-challenge|challenge|53|14.0|2022|2.0|None|None|48.0|https://codalab.lisn.upsaclay.fr/competitions/4953|||||multimedia|computer vision|efficient and high-performance fusion algorithm to get bayer from RGBW DbinB and DbinC|image transformation|||not on the lb|PSNR, SSIM, LPIPS, KLD|True|False|True|False||[1.2617143790108498]
Quad Joint Remosaic and Denoise @MIPI-challenge|challenge|62|12.0|2022|2.0|None|None|48.0|https://codalab.lisn.upsaclay.fr/competitions/4955|||||multimedia|computer vision|remosaic and denoise challenge|image transformation|||not on the lb|PSNR, SSIM, LPIPS, KLD|True|False|True|False||[1.1617614577014064]
RGB+TOF Depth Completion @MIPI-challenge|challenge|81|17.0|2022|2.0|None|None|48.0|https://codalab.lisn.upsaclay.fr/competitions/4956|||||multimedia|computer vision|fuse the pre-aligned RGB and sparse ToF depth measurement to obtain a complete depth map|image transformation, regression|||SCORE |RMAE, EWMAE, RDS, RTSD|True|False|True|False||[1.1050594227504245]
2022 NICO Common Context Generalization Challenge (ECCV 2022 Workshop)|NICO Common Context Generalization Challenge|74|712.0|2022|1.0|None|None|87.0|https://codalab.lisn.upsaclay.fr/competitions/4084|||||multimedia|computer vision, NLP|facilitate the OOD generalization in visual recognition through promoting the research on the intrinsic learning mechanisms with native invariance and generalization ability|visual recognition, domain generalisation|||Acc |accuracy|False|False|True|False||[1.3160368963145013]
2022 NICO Hybrid Context Generalization Challenge (ECCV 2022 Workshop)|NICO Hybrid Context Generalization Challenge|47|228.0|2022|1.0|None|None|87.0|https://codalab.lisn.upsaclay.fr/competitions/4083|||||multimedia|computer vision|developing reliable algorithms across different contexts (domains) to improve the generalization ability of models|visual recognition, domain generalisation|||Acc |accuracy|False|False|True|False||[1.3210471747062755]
Meta-learning from Learning Curves - 2ND ROUND|AutoML-Conf 2022 Competition on Meta-learning from Learning Curves|28|62.0|2022|2.0|1000|1000|49.0|https://codalab.lisn.upsaclay.fr/competitions/4894||||||AutoML|||||Average Final Score||||True|True||[1.1421911421911422]
Apple detection data challenge|Multi scale apple detection|50|146.0|2021|2.0|None|None|219.0|https://codalab.lisn.upsaclay.fr/competitions/743|||||multimedia, agriculture|computer vision|detect fruits for counting from far-distance images and detect fruits in the close-distance views for later applications such as pathogen propagation estimation|image recognition, regression|||mAP|AP|False|False|True|False||[1.2809657342179055]
Causal News Corpus - Event Causality Shared Task 2022|Event Causality Identification Shared Task on Causal News Corpus (CNC): Subtask 1: Causal Event Classification -- Does an event sentence contain any cause-effect meaning?Subtask 2: Cause-Effect-Signal Span Detection -- Which consecutive spans correspond to cause, effect or signal per causal sentence?|11|14.0|2022|4.0|None|None|171.0|https://codalab.lisn.upsaclay.fr/competitions/2299|||||linguistics, news|NLP|Subtask 1: Causal Event Classification -- Does an event sentence contain any cause-effect meaning? Subtask 2: Cause-Effect-Signal Span Detection -- Which consecutive spans correspond to cause, effect or signal per causal sentence?|binary classificaiton, multi-class classification|||F1|precision, recall, F1, accuracy, MCC|True|False|True|False||[1.0531663057008611]
ECCV DeeperAction Challenge - Kinetics-TPS Track on Part-level Action Parsing and Action Recognition|The challenge is Track 4 at ECCV DeeperAction Challenge. This track is to recognize a human action by compositional learning of body part state in videos. The challenge will be carried out on the Kinetics-TPS dataset. More information on the dataset and downloads can be found at https://github.com/Hypnosx/Kinetics-TPS.|29|31.0|2022|2.0|None|None|122.0|https://codalab.lisn.upsaclay.fr/competitions/4392|||||multimedia|computer vision|part state parsing for boosting action recognition. Hence, the participants should predict human location, body part location, part state in the frame level, and then integrate these results together to predict human action in the video level|action recognition|||Score ||False|False|True|False||[1.2649572649572651]
ECCV DeeperAction Challenge - UrbanPipe Track on Fine-grained Video Anomaly Recognition|The challenge is Track 5 at ECCV DeeperAction Challenge. The challenge requires to predict the categories of pipe defects in a short QV video. The challenge will be carried out on the QV-Pipe dataset. More information on the dataset can be found at https://deeperaction.github.io/datasets/urbanpipe.|29|17.0|2022|2.0|None|None|122.0|https://codalab.lisn.upsaclay.fr/competitions/4439|||||multimedia, urbanisme|computer vision|Fine-grained Video Anomaly Recognition: predict the categories of pipe defects in a short QV video|multi-class classification|||mAP|mAP|False|False|True|False||[1.1026078556841057]
SMM4H'22 Task 2. Classification of stance and premise in tweets about health mandates (COVID-19)|This is a competition for COLING'22 Shared Task (SMM4H'22 Task 2)|9|10.0|2022|6.0|None|None|230.0|https://codalab.lisn.upsaclay.fr/competitions/5067|||||tweets, health, covid|NLP|argument mining (or argumentation mining) for extracting arguments from COVID-related tweets|argument mining, binary classification|||F1𝑝𝑟𝑒𝑚𝑖𝑠𝑒 ,  F1𝑠𝑡𝑎𝑛𝑐𝑒 |F1|False|False|True|False||[0.0]
DynamicEarthNet:Daily Multi-Spectral Satellite Dataset for Semantic Change Segmentation|The evaluation page for DynamicEarthNet:Daily Multi-Spectral Satellite Dataset for Semantic Change Segmentation (CVPR 2022)|32|10.0|2022|2.0|None|None|453.0|https://codalab.lisn.upsaclay.fr/competitions/2882|http://www.classic.grss-ieee.org/earthvision2021/challenge.html|https://arxiv.org/pdf/2203.12560.pdf||||computer vision|perform monthly multi-class change detection with a limited amount of multi-class change labels|multi-class classification|||SCS |SCS, BC, SC, mIoU|True|False|True|False||[1.2357723577235773]
M2MeT Challenge 2022:Task2(Multi-Speaker ASR)-Sub-trackI(Fixed Training Condition)|Multi-channel Multi-party Meeting Transcription Challenge (M2MeT) has been accepted as a Signal Processing Grand Challenge (SPGC) of ICASSP 2022！Please refer to https://www.alibabacloud.com/m2met-alimeeting for more details.|26|154.0|2021|1.0|None|None|3652.0|https://codalab.lisn.upsaclay.fr/competitions/837||https://arxiv.org/abs/2110.07393|||speech|signal processing|speech recognition and speaker diarization in conference scenario as a reference. The goal is to simplify the training and evaluation procedures, so that participants can easily and flexibly experiment and verify the neural network-based method|speech recognition|||CER|CER|False|False|True|True||[0.0]
M2MeT Challenge 2022:Task1(Speaker Diarization)-Sub-trackII(Open Training Condition)|Multi-channel Multi-party Meeting Transcription Challenge (M2MeT) has been accepted as a Signal Processing Grand Challenge (SPGC) of ICASSP 2022！Please refer to https://www.alibabacloud.com/m2met-alimeeting for more details.|14|17.0|2021|1.0|None|None|3652.0|https://codalab.lisn.upsaclay.fr/competitions/840||https://arxiv.org/abs/2110.07393|||speech|signal processing|speech recognition and speaker diarization in conference scenario as a reference. The goal is to simplify the training and evaluation procedures, so that participants can easily and flexibly experiment and verify the neural network-based method|speech recognition|||DER|DER|False|False|True|True||[0.0]
M2MeT Challenge 2022:Task1(Speaker Diarization)-Sub-trackI(Fixed Training Condition)|Multi-channel Multi-party Meeting Transcription Challenge (M2MeT) has been accepted as a Signal Processing Grand Challenge (SPGC) of ICASSP 2022！Please refer to https://www.alibabacloud.com/m2met-alimeeting for more details.|26|167.0|2021|1.0|None|None|3652.0|https://codalab.lisn.upsaclay.fr/competitions/839||https://arxiv.org/abs/2110.07393|||speech|signal processing|speech recognition and speaker diarization in conference scenario as a reference. The goal is to simplify the training and evaluation procedures, so that participants can easily and flexibly experiment and verify the neural network-based method|speech recognition|||DER|DER|False|False|True|True||[0.0]
WebFace260M Benchmark|The WebFace260M Benchmark|31|83.0|2021|3.0|None|None|3652.0|https://codalab.lisn.upsaclay.fr/competitions/859|https://ibug.doc.ic.ac.uk/resources/masked-face-recognition-challenge-workshop-iccv-21/||https://www.face-benchmark.org/index.html||multimedia, covid|computer vision|Face Bio-metrics under COVID: Masked Face Recognition Challenge||||All-Masked (MFR)|MFR, SFR|True|False|True|False||[0.0]
MSLS Place recognition challenge|Welcome to the Mapillary Street-level Sequences place recognition challenge|24|104.0|2021|1.0|None|None|10325.0|https://codalab.lisn.upsaclay.fr/competitions/865|https://github.com/mapillary/mapillary_sls||||multimedia|computer vision|place recognition|image recognition||1.6M street-level images|recall@5 |recall|False|False|True|False||[1.1762740183792817]
FinQA Competition - Public Test Set|"This competition is held for the <a href=""https://finqasite.github.io/"">FinQA dataset</a>. The leaderboard is for the submission of results on the public test set. We will release the private test set later."|29|46.0|2022|1.0|None|None|28489.0|https://codalab.lisn.upsaclay.fr/competitions/1846|https://finqasite.github.io/|https://arxiv.org/abs/2109.00122|||liguistics, finance|NLP||QA|||Program Accuracy, Execution Accuracy|accuracy|False|False|True|False||[1.1590548223523587]
LOVEU@CVPR'22 Track 3: Affordance-Centric Question-driven Task Completion (AQTC)|https://sites.google.com/view/loveucvpr22/track-3?authuser=0|11|54.0|2022|1.0|None|None|0.0|https://codalab.lisn.upsaclay.fr/competitions/4642|https://sites.google.com/view/loveucvpr22/track-3?authuser=0||||multimedia|computer vision|Affordance-Centric Question-driven Task Completion (AQTC)|Affordance-Centric Question-driven Task Completion (AQTC)|||Recall@1, Recall@3, MR, MRR|Recall@1, Recall@3, MR, MRR|True|False|True|True||[0.0]
BBO (GINA+MLP+SGD)|Black Box Optimization (BBO) challenge (GINA+MLP+SGD)|29|401.0|2022|1.0|None|None|0.0|https://codalab.lisn.upsaclay.fr/competitions/33||||||AutoML|hyper-parameter (hp) selection/tuning||||Score ||||True|True||[]
LoveDA Unsupervised Domain Adaptation|This LoveDA Unsupervised Domain Adaptation Benchmark for the Test Set|32|1404.0|2021|2.0|None|None|0.0|https://codalab.lisn.upsaclay.fr/competitions/424|||||multimedia, aerial|computer vision|large-scale land-cover mapping and deep learning applied to high spatial resolution satellite images|semantic segmentation|||mIoU |mIoU|False|False|True|False||[1.2423109640358807]
MIND News Recommendation Competition|This is a competition for news recommendation on the Microsoft News Dataset (MIND).|44|275.0|2020|2.0|None|None|32.0|https://codalab.lisn.upsaclay.fr/competitions/420|||||news|NLP|news recommendation|multi-class classification|||AUC |AUC, MRR, nDCG@K|True|False|True|False||[1.0900197693772946]
MAVEN Event Detection Challenge|A massive general domain textual event detection dataset.|41|290.0|2020|1.0|None|None|0.0|https://codalab.lisn.upsaclay.fr/competitions/395||https://arxiv.org/abs/2004.13590||https://github.com/THU-KEG/MAVEN-dataset|linguistics|NLP|Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text.|event detection|||Macro_F1 |precision, recall, and F1|True|False|True|False||[1.1528164856738834]
DocRED|Document-level relation extration|56|551.0|2019|1.0|None|None|0.0|https://codalab.lisn.upsaclay.fr/competitions/365|||||linguistics|NLP|relation extration|relation extration|||RE_F1, Evi_F1|F1|False|False|True|False||[24.721311475409834]
Gene network inference challenge 2021-2022|The goal of this challenge is to reverse enginner gene regulation networks from simulated steady-state observational data. Participants are challenged to infer five network structures of 20 or 100 nodes.|12|187.0|2022|2.0|None|None|35.0|https://codalab.lisn.upsaclay.fr/competitions/1505|||||biology|computer vision|reverse enginner gene regulation networks from simulated steady-state observational data: Each prediction score corresponds to the confidence of the participant in the existence of the directed edge. If a link is missing in the prediction file, it will be assumed that the prediction score is equal to 0|regression|||score |AUPR|False|False|True|False||[]
2022 IEEE GRSS Data Fusion Contest Track SLM|2022 IEEE GRSS Data Fusion Contest Track SLM|239|5100.0|2022|3.0|None|None|86.0|https://codalab.lisn.upsaclay.fr/competitions/880|||||multimedia, urbanisme, geoscience|computer vision|create land cover maps using a few annotated images while leveraging the large amount of unlabeled images for training||||Prediction |accuracy|False|False|True|False||[1.350632911392405]
ROD2021 Challenge|Radar Object Detection (ROD) on Radio Frequency Images for Autonomous Driving.|55|990.0|2022|2.0|None|None|0.0|https://codalab.lisn.upsaclay.fr/competitions/1063|||||autonomous driving|computer vision|radar object detection|object detection|||AP (total)|AP|False|False|True|False||[1.3374004910348933]
Statistical NLP - Assignment 3 - Taxonomy Enrichment for the Russian Language|Taxonomy Enrichment for the Russian Language|20|174.0|2021|6.0|None|None|21.0|https://codalab.lisn.upsaclay.fr/competitions/539|||||linguistics|NLP|Taxonomy Enrichment: extend an existing taxonomy with relations of previously unseen words. Given words that are not yet included in the taxonomy, we need to associate each word with the appropriate hypernyms from an existing taxonomy|text generation, ordinal regression|||MAP|mAP, MRR|True|False|True|False||[1.904436860068259]
LoveDA Semantic Segmentation|This LoveDA Semantic Segmentation Benchmark for the Test Set|55|405.0|2021|1.0|None|None|0.0|https://codalab.lisn.upsaclay.fr/competitions/421|||||multimedia, aerial|computer vision|large-scale land-cover mapping and deep learning applied to high spatial resolution satellite images|semantic segmentation|||mIoU |mIoU|False|False|True|False||[1.1232694235459744]
QuALES@IberLEF2022: Question Answering Learning from Examples in Spanish|Answering questions from Spanish texts.|21|121.0|2022|3.0|None|None|67.0|https://codalab.lisn.upsaclay.fr/competitions/2619|||||linguistics, news|NLP|automatically find answers to questions in Spanish from news text|QA|||Score 1, Score 2|exact match, F1|True|False|True|False||[0.0]
RUSSE 2022 Russian Text Detoxification Based on Parallel Corpora|Dialogue shared task 2022|59|228.0|2021|3.0|None|None|61.0|https://codalab.lisn.upsaclay.fr/competitions/642|||||linguistics|NLP|presenting a neutral version of a user message which preserves meaningful content. We denote this task as detoxification|text generation, text style transfer|||Joint score|Style transfer accuracy, Meaning preservation, Fluency, Joint score|True|False|True|True||[1.4768384879725087]
2022 GeoAI Martian Challenge|Result evaluation server for 2022 GeoAI Martian Challenge (http://cici.lab.asu.edu/martian/)|33|32.0|2022|2.0|None|None|0.0|https://codalab.lisn.upsaclay.fr/competitions/1934|http://cici.lab.asu.edu/martian/#home||||aerial, space science|computer vision|Mars crater detection|object detection|||AP|AP, AR|True|False|True|False||[1.1842427208221191]
Customized Chat Grounding Persona and Knowledge|Customized Chat Grounding Persona and Knowledge|9|17.0|2022|2.0|None|None|41.0|https://codalab.lisn.upsaclay.fr/competitions/3754|||||linguistics|NLP|build a customized and intelligent conversational agent|chat-bot|||Average Score,  Average Grounding, Average Generation||True|False|True|False||[0.0]
Argument Mining Competition (RuArg-2022)|Dialogue Evaluation Shared Task 2022|108|1205.0|2021|3.0|None|None|63.0|https://codalab.lisn.upsaclay.fr/competitions/786|||||linguistics|NLP|stance detection and premise classification: In the first task, it is required to determine the point of view (stance) of the text’s author in relation to the given claim. In the second task, you need to recognize whether the text contains premises “for” or “against” to a given claim.|argument mining, multi-class classification, binary classification|||F1 Stance Detection, F1 Premise Classification|F1|False|False|True|True||[1.3930697721874064]
Meta-learning from Learning Curves|IEEE WCCI 2022 Competition on Meta-learning from Learning Curves|55|763.0|2022|2.0|1000|1000|56.0|https://codalab.lisn.upsaclay.fr/competitions/753||||||RL|design meta-learning agents that leverage learning curve information of partially trained algorithms, hence reducing the cost of training them to convergence|meta-learning, reinforcement learning|||Average Final Score|ALC, mALC|True|False|True|True||[1.334811529933481]
HO3D (version 2)|This is competition to evaluate different methods for hand pose estimation when interacting with objects under severe occlusions using HO3D datasett.|13|144.0|2020|1.0|None|None|0.0|https://codalab.lisn.upsaclay.fr/competitions/4318|||||multimedia|computer vision|hand pose estimation|object detection||3D annotations for more than 65 sequences captured with 10 different subjects and 10 objects. approximately 80K images|Mean joint error (procrustes alignment), Mean joint error|Mesh error, Mean joint error, F1, AUC|True|False|True|False||[0.0]
WASSA 2022 Shared Task|Shared Task on Empathy Detection and Emotion Classification, organized as part of WASSA 2022|67|141.0|2022|3.0|None|None|72.0|https://codalab.lisn.upsaclay.fr/competitions/834|||||linguistics|NLP|Empathy Detection and Emotion Classification|multi-class classification, binary classification|||Averaged Pearson Correlations, Macro F1-Score, PER Pearson Correlations, IRI Pearson Correlations|Pearson correlations, F1|True|False|True|False||[0.0]
VoiceMOS Challenge|The first VoiceMOS Challenge. To participate, contact voicemos2022@nii.ac.jp before registration.|103|387.0|2021|4.0|None|None|92.0|https://codalab.lisn.upsaclay.fr/competitions/695||https://arxiv.org/abs/2105.02373|||speech|signal processing|predicting the MOS (mean opinion score) score of synthetic speech|regression|||MAIN_SYS_SRCC|mean squared error (MSE), Linear Correlation Coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC), and Kendall Tau Rank Correlation (KTAU)|True|False|True|False||[1.0182146077547338]
LOVEU@CVPR'22 Track 2: Generic Event Boundary Captioning|This competition is for generic event boundary captioning with no constraint of training data and pre-trained models.|14|45.0|2021|2.0|None|None|372.0|https://codalab.lisn.upsaclay.fr/competitions/4157|||||multimedia|computer vision|Boundary Captioning|boundary detection|||AVG |CIDEr, SPICE and ROUGE_L|True|False|True|True||[1.7128074385122973]
Far-field Speaker Verification Challenge (FFSVC2022) - Task1|FFSVC2022 is the satellite events of Interspeech2022. This is the official submission platform of FFSVC2022 task 1.|49|11.0|2022|2.0|None|None|87.0|https://codalab.lisn.upsaclay.fr/competitions/2167|||||speech|signal processing|speaker verification research with special focus on far-field scenario under noisy conditions in real scenarios|binary classification|||mDCF |Minimum Detection Cost (mDCF), Equal Error Rate (EER)|True|False|True|False||[1.0]
MedMTEval|Machine translation competition|31|27.0|2022|1.0|None|None|0.0|https://codalab.lisn.upsaclay.fr/competitions/1856|||||linguistics, medicine|NLP|translate medical texts from Russian to English and to get the highest possible results in COMET and NER accuracy metrics by breaking the baseline|machine translation|||COMET |BLEU, ROUGE, accuracy|True|False|True|False||[1.164768503375063]
UIoU Dark Zurich at Vision for All Seasons Workshop, CVPR 2020|UIoU Dark Zurich benchmarks semantic segmentation performance with the Uncertainty-aware IoU metric on the Dark Zurich dataset, presented in the paper Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation, ICCV 2019.|12|53.0|2020|2.0|None|None|86.0|https://codalab.lisn.upsaclay.fr/competitions/3783|||||multimedia|computer vision|design of robust vision algorithms for adverse weather and illumination conditions|image recognition|||UIoU |UIoU , IoU|True|False|True|False||[6.602702566936254]
CodRED|Cross-document relation extration|9|40.0|2022|2.0|None|None|0.0|https://codalab.lisn.upsaclay.fr/competitions/3770||https://aclanthology.org/2021.emnlp-main.366/|||linguistics|NLP|relation extraction|relation extraction|||AUC, F1|F1, AUC|True|False|True|True||[1.0]
LOVEU@CVPR'22 Track 1: Generic Event Boundary Detection|This competition is for generic event boundary detection with no constraint of training data and pre-trained models.|43|314.0|2022|2.0|None|None|7.0|https://codalab.lisn.upsaclay.fr/competitions/4145|https://sites.google.com/view/loveucvpr22/home||||multimedia|computer vision|boundary detection|boundary detection|||F1 Score|F1, precision, recall|True|False|True|True||[1.6303317535545025]
HO3D (version 3)|This is competition to evaluate different methods for hand pose estimation when interacting with objects under severe occlusions using HO3D datasett.|10|33.0|2020|1.0|None|None|0.0|https://codalab.lisn.upsaclay.fr/competitions/4393|||||multimedia|computer vision|hand pose estimation|object detection||3D annotations for more than 65 sequences captured with 10 different subjects and 10 objects. approximately 80K images|Mean joint error (procrustes alignment), Mean joint error|Mesh error, Mean joint error, F1, AUC|True|False|True|False||[0.0]
IberLEF 2022 Task - PoliticEs. Spanish Author Profiling for Political Ideology|Political ideology is a psychographic trait that can be used to understand individual and social behaviour, including moral and ethical values as well as inherent attitudes, appraisals, biases, and prejudices (Verhulst et al., 2012). The relationship between personality traits and political ideology was demonstrated in Fatke (2017). The author gathered data from 21 countries and found a correlation between political ideology and the big five personality traits. For instance, he found that conscientiousness was strongly correlated with the right wing, whereas openness to experience and agreeability were notably more correlated to the left wing. Moreover, our political ideology has a great influence in our daily lives. For example, Baumgaertner et al. (2018) found a correlation between political ideology and the attitude of citizens to vaccination campaigns of infectious diseases. This shared task aims to extract political ideology information from texts. For this, two authorship analysis tasks are proposed. On the one hand, an author profiling task focused on the identification of the political spectrum from a binary and multi-class perspective, and the gender. On the other hand, an authorship attribution task, focused on revealing the identity of an author based on their writings. In recent years, several shared tasks have been organized on authorship analysis under the PAN workshop series, so there is an expected target community. The novelty of this task is that, to the best of our knowledge, none of these previous tasks have focused on political ideology.___|63|201.0|2022|3.0|None|None|84.0|https://codalab.lisn.upsaclay.fr/competitions/1948|||||linguistics, politics|NLP|extract political ideology information from texts. For this, an author profiling task is proposed. It is focused on the identification of the gender, the profession, and the political spectrum from a binary and multi-class perspective|multi-class classification, binary classification|||Average Macro F1|F1, precision, recall|True|False|True|False||[1.1210667583477039]
TG-CSR (Vacation dataset, Multi-set format)|Theoretically-Grounded Commonsense Reasoning (TG-CSR)|10|45.0|2022|1.0|None|None|0.0|https://codalab.lisn.upsaclay.fr/competitions/3080|||||linguistics|NLP|assess whether a single machine commonsense model, such as a language representation model, is able to answer questions across all these different categories without too much training data|QA|||score |F1|False|False|True|True||[1.4013714489268858]
AutoCV2 Challenge|Create a fully Automatic Deep Learning solution for Computer Vision|70|336.0|2019|2.0|4000 USD|4000|49.0|https://autodl.lri.fr/competitions/3|||||multimedia|AutoML||multi-label classificaiton|||<Rank>|||||||[]
AutoNLP|Create a fully Automated Deep Learning solution for Text Categorization|99|420.0|2019|2.0|7500 USD|7500|29.0|https://autodl.lri.fr/competitions/35|||||linguistics|AutoML||multi-label classificaiton|||<Rank>|||||||[]
AutoSpeech Challenge|Create a fully Automated Deep Learning solution for Speech Categorization|60|234.0|2019|2.0|4000 USD|4000|90.0|https://autodl.lri.fr/competitions/48; https://autodl.lri.fr/competitions/153; https://autodl.lri.fr/competitions/106|||||speech|AutoML|||||<Rank>|||||||[]
AutoWSL Challenge|Automatic Weakly Supervised Learning|49|439.0|2019|2.0|4000 USD|4000|30.0|https://autodl.lri.fr/competitions/64||||||AutoML|||||<Rank>|||||||[]
AutoSeries Challenge|Create a fully Automatic Machine Learning solution for Speech Recognition|82|716.0|2019|2.0|5000 USD|5000|44.0|https://autodl.lri.fr/competitions/149; https://autodl.lri.fr/competitions/163||||||AutoML||multi-label classificaiton|||<Rank>|||||||[]
AutoDL Challenge|Create a fully Automated Deep Learning Solution|87|247.0|2020|2.0|4000 USD|4000|91.0|https://autodl.lri.fr/competitions/162||||||AutoML||multi-label classificaiton|||<Rank>|||||||[]
MetaDL - NeurIPS 2021|Meta-learning with Deep Learning|56|188.0|2021|2.0|1000 USD|1000|60.0|https://autodl.lri.fr/competitions/210||||||AutoML|||||<Rank>|||||||[]
KDD Cup 2019: AutoML for Temporal Relational Data||117||2019|2.0|||110.0|https://codalab.sunai.uoc.edu/competitions/559||||||AutoML|temporal relational data|binary classification||||AUC|False|False|False|True||[]
Microscopy Challenge - Data Science Africa 2019||254||2019|2.0||||https://codalab.sunai.uoc.edu/competitions/522|http://air.ug/microscopy/||||medicine|computer vision|train machine learning methods to recognise different pathogen objects, and to make this accessible in the form of an Android application usable at the point of care: Step 1: Binary classification problem. You need to implement a machine learning model in order to classify whether a patch is postive or negative. Step 2: Regression problem. You need to predict the number of parasites on each image (using model trained on Step 1).|binary classification, regression|||Classification score|AUC, MSE|True|True|True|True||[]
Fashion IQ Challenge 2019||47||2019|2.0||||https://codalab.sunai.uoc.edu/competitions/573|||||fashion|computer vision, NLP|retrieving images by natural language based feedback|conversational image search retrieval systems|||Avg |Recall@10, Recall@50|True|False|True|False||[]
Lexical Semantic Change Detection in German||26||2019|2.0||||https://codalab.sunai.uoc.edu/competitions/560|||||linguistics|NLP|unsupervised detection of lexical semantic change, i.e., word sense changes over time, in a German text corpus: rank all target words according to their degree of lexical semantic change between Ca and Cb as annotated by human judges.|ordianl regression|||Spearman |Spearman rank correlation|False|False|True|False||[]
2nd 3D Face Alignment in the Wild Challenge: Dense Reconstruction from Video||31||2019|2.0|||47.0|https://codalab.sunai.uoc.edu/competitions/572|||||multimedia|computer vision|2nd 3D Face Alignment in the Wild Challenge: Dense Reconstruction from Video: 3D face reconstruction methods on a new large corpora of profile-to-profile face videos annotated with corresponding high-resolution 3D ground truth meshes|image reconstruction|||mean_ARMSE |ARMSE|False|False|True|False||[]
IROS 2019 Lifelong Robotic Vision Challenge: Lifelong Object Recognition||37||2019|2.0||||https://codalab.sunai.uoc.edu/competitions/581|||||multimedia|computer vision|object recognition|object recognition|||result ||False|False|True|False||[]
AutoRL Challenge||35|16.0|2021|2.0|9000 USD|9000|79.0|https://www.automl.ai/competitions/16||||||RL|design computer programs capable of automatically generating policies(usually in the form of agents) for a collection of DJSSPs ( dynamic job shop scheduling problem):  automatically train agent(s) for each given DJSSP environment with the purpose to maximize the long-term return, and the evaluation is based on the performance of agents on a collection of different environments|reinforcement learning|||<Rank>|makespan, pending time violation|True|False|False|True||[]
Auto-KWS 2021 Challenge||18|14.0|2021|2.0|4000 USD|4000|53.0|https://www.automl.ai/competitions/12|||||speech|AutoML, signal processing|Personalized Keyword Spotting (Auto-KWS): automated machine learning for Personalized Keyword Spotting (Auto-KWS) which aims at proposing automated solutions for personalized keyword spotting tasks|keyword spotting|||<Rank>|miss rate (MR), false alarm rate (FAR)|True|True|False|True||[]
KDD Cup 2020 AutoGraph Challenge||237|2269.0|2020|3.0|33500 USD|33500|75.0|https://www.automl.ai/competitions/3|||||graphs|AutoML|Machine learning on graph-structured data: learn low-dimensional representation of each node in the graph, which are used for downstream tasks, such as friend recommendation in a social network, or classifying academic papers into different subjects in a citation network: design a computer program capable of providing solutions to graph representation learning problems autonomously|node classification|||<Rank>|accuracy|False|False|False|True||[]
AutoSpeech 2020||46|263.0|2020|3.0|4000 USD|4000|65.0|https://www.automl.ai/competitions/2|||||speech|AutoML, signal processing|multi-label classification problems, which come from different speech classification domains|multi-class classification|||Score|ALC|False|True|False|True||[]
